{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982e7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "from time import sleep\n",
    "import os\n",
    "import zipfile\n",
    "# import re\n",
    "# import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbadcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd99f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup directories\n",
    "\n",
    "# List of directories to be created\n",
    "directories = [\n",
    "#     './data/SR-game-logs',\n",
    "#     './data/SR-opponent-game-logs',\n",
    "#     './data/rosters',\n",
    "    './data/play_by_play/'\n",
    "]\n",
    "\n",
    "# Loop through the directories and create each one if it doesn't already exist\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc3d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Copy ./data/nfl.db here from Scraping\n",
    "\n",
    "!cp ../Scraping/nfl.db ./data\n",
    "# !cp ../Scraping/data/games.csv ./data/Games.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96561a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/td/Code/nfl-ai/Streamlit\n",
      "Saved ./data/Teams.csv\n",
      "Saved ./data/Games.csv\n",
      "Saved ./data/PlayerStats.csv\n"
     ]
    }
   ],
   "source": [
    "# !python exportToCSV.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Check current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Connect to SQLite Database\n",
    "conn = sqlite3.connect('./data/nfl.db')  # Ensure this path is correct\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Attempt to save each table to a CSV file\n",
    "try:\n",
    "    for table_info in cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\"):\n",
    "        table_name = table_info[0]\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {table_name};\", conn)\n",
    "        file_path = f'./data/{table_name}.csv'\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b663aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make UpcomingGames.csv from ./data/nfl.db\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Scraping file path\n",
    "db_file_path = './data/nfl.db'\n",
    "\n",
    "# Connect to the SQLite Scraping\n",
    "conn = sqlite3.connect(db_file_path)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query to select upcoming games for the 2023 season where both team scores are null\n",
    "query_upcoming_games = \"\"\"\n",
    "SELECT * FROM Games \n",
    "WHERE season = 2023 AND away_score IS NULL AND home_score IS NULL;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "cursor.execute(query_upcoming_games)\n",
    "upcoming_games = cursor.fetchall()\n",
    "\n",
    "# Column names for the Games table\n",
    "cursor.execute(\"PRAGMA table_info(Games);\")\n",
    "columns = [column[1] for column in cursor.fetchall()]\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df_upcoming_games = pd.DataFrame(upcoming_games, columns=columns)\n",
    "\n",
    "# Save to a CSV file in the './data' directory\n",
    "csv_file_path = './data/UpcomingGames.csv'\n",
    "df_upcoming_games.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Close the Scraping connection\n",
    "conn.close()\n",
    "\n",
    "# After running this code, the CSV file will be saved to the specified path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f299702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the NFL data from the CSV file\n",
    "nfl_data = pd.read_csv('./data/Games.csv')\n",
    "\n",
    "# Identify unplayed games in the 2023 season (games with missing scores)\n",
    "# unplayed_games = nfl_data[(nfl_data['season'] == 2023) & (nfl_data['away_score'].isna() | nfl_data['home_score'].isna())]\n",
    "unplayed_games = nfl_data[(nfl_data['season'] == 2023) & (nfl_data['away_score'].isna() & nfl_data['home_score'].isna())]\n",
    "\n",
    "# Remove unplayed games from the dataset\n",
    "nfl_data_cleaned = nfl_data.drop(unplayed_games.index)\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file in the same directory\n",
    "nfl_data_cleaned.to_csv('./data/Games.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21c19d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved play_by_play_2000.csv\n",
      "Downloaded and saved play_by_play_2001.csv\n",
      "Downloaded and saved play_by_play_2002.csv\n",
      "Downloaded and saved play_by_play_2003.csv\n",
      "Downloaded and saved play_by_play_2004.csv\n",
      "Downloaded and saved play_by_play_2005.csv\n",
      "Downloaded and saved play_by_play_2006.csv\n",
      "Downloaded and saved play_by_play_2007.csv\n",
      "Downloaded and saved play_by_play_2008.csv\n",
      "Downloaded and saved play_by_play_2009.csv\n",
      "Downloaded and saved play_by_play_2010.csv\n",
      "Downloaded and saved play_by_play_2011.csv\n",
      "Downloaded and saved play_by_play_2012.csv\n",
      "Downloaded and saved play_by_play_2013.csv\n",
      "Downloaded and saved play_by_play_2014.csv\n",
      "Downloaded and saved play_by_play_2015.csv\n",
      "Downloaded and saved play_by_play_2016.csv\n",
      "Downloaded and saved play_by_play_2017.csv\n",
      "Downloaded and saved play_by_play_2018.csv\n",
      "Downloaded and saved play_by_play_2019.csv\n",
      "Downloaded and saved play_by_play_2020.csv\n",
      "Downloaded and saved play_by_play_2021.csv\n",
      "Downloaded and saved play_by_play_2022.csv\n",
      "Downloaded and saved play_by_play_2023.csv\n"
     ]
    }
   ],
   "source": [
    "### Download pbp \n",
    "### https://github.com/nflverse/nflverse-data/releases/tag/pbp\n",
    "\n",
    "import requests\n",
    "\n",
    "# Iterate through the years 2000 to 2023\n",
    "for year in range(2000, 2024):\n",
    "    # Construct the URL for the CSV file of the specific year\n",
    "    url = f\"https://github.com/nflverse/nflverse-data/releases/download/pbp/play_by_play_{year}.csv\"\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Open the file in write-binary mode and save the CSV\n",
    "        with open(f\"./data/play_by_play/play_by_play_{year}.csv\", 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded and saved play_by_play_{year}.csv\")\n",
    "    else:\n",
    "        print(f\"Failed to download data for the year {year}\")\n",
    "\n",
    "# Zip the downloaded files\n",
    "zip_filename = \"./data/play_by_play.zip\"\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for root, dirs, files in os.walk(\"./data/play_by_play/\"):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), \"./data/play_by_play/\"))\n",
    "\n",
    "print(f\"Zipped all files into {zip_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb59c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d114a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a540d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69252fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
