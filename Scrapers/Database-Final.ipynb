{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4132418-0dcc-4060-8c33-6e22a2384e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "from time import sleep\n",
    "import random\n",
    "from requests.exceptions import Timeout, RequestException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186623a-4234-4d97-a327-cd3c1aa050d0",
   "metadata": {},
   "source": [
    "Data from https://github.com/nflverse/nflverse-data/releases/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fbf2bf-447a-4322-9582-06ee68303eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data\n",
    "!mkdir data\n",
    "!mkdir data/rosters\n",
    "!mkdir data/player-stats\n",
    "!mkdir data/scoring-tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c511f87-b89c-4fd6-a588-37f5e75c7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm nfl.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a5aa85-cd25-47d5-b922-8d6f71a72786",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create 'Teams' in nfl.db\n",
    "\n",
    "teams = [\n",
    "    ['ARI', 'Arizona Cardinals', 'NFC West'],\n",
    "    ['ATL', 'Atlanta Falcons', 'NFC South'],\n",
    "    ['BAL', 'Baltimore Ravens', 'AFC North'],\n",
    "    ['BUF', 'Buffalo Bills', 'AFC East'],\n",
    "    ['CAR', 'Carolina Panthers', 'NFC South'],\n",
    "    ['CHI', 'Chicago Bears', 'NFC North'],\n",
    "    ['CIN', 'Cincinnati Bengals', 'AFC North'],\n",
    "    ['CLE', 'Cleveland Browns', 'AFC North'],\n",
    "    ['DAL', 'Dallas Cowboys', 'NFC East'],\n",
    "    ['DEN', 'Denver Broncos', 'AFC West'],\n",
    "    ['DET', 'Detroit Lions', 'NFC North'],\n",
    "    ['GB', 'Green Bay Packers', 'NFC North'],\n",
    "    ['HOU', 'Houston Texans', 'AFC South'],\n",
    "    ['IND', 'Indianapolis Colts', 'AFC South'],\n",
    "    ['JAX', 'Jacksonville Jaguars', 'AFC South'],\n",
    "    ['KC', 'Kansas City Chiefs', 'AFC West'],\n",
    "    ['LAC', 'Los Angeles Chargers', 'AFC West'],\n",
    "    ['LAR', 'Los Angeles Rams', 'NFC West'],\n",
    "    ['LVR', 'Las Vegas Raiders', 'AFC West'],\n",
    "    ['MIA', 'Miami Dolphins', 'AFC East'],\n",
    "    ['MIN', 'Minnesota Vikings', 'NFC North'],\n",
    "    ['NE', 'New England Patriots', 'AFC East'],\n",
    "    ['NO', 'New Orleans Saints', 'NFC South'],\n",
    "    ['NYG', 'New York Giants', 'NFC East'],\n",
    "    ['NYJ', 'New York Jets', 'AFC East'],\n",
    "    ['PHI', 'Philadelphia Eagles', 'NFC East'],\n",
    "    ['PIT', 'Pittsburgh Steelers', 'AFC North'],\n",
    "    ['SEA', 'Seattle Seahawks', 'NFC West'],\n",
    "    ['SF', 'San Francisco 49ers', 'NFC West'],\n",
    "    ['TB', 'Tampa Bay Buccaneers', 'NFC South'],\n",
    "    ['TEN', 'Tennessee Titans', 'AFC South'],\n",
    "    ['WAS', 'Washington Commanders', 'NFC East']\n",
    "]\n",
    "\n",
    "df_teams = pd.DataFrame(teams, columns=['TeamID', 'Team', 'Division'])\n",
    "with sqlite3.connect('nfl.db') as conn:\n",
    "    df_teams.to_sql('Teams', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7943a3fa-648b-4b8e-8112-d3d6867da6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Games' in nfl.db\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/nflverse/nfldata/master/data/games.csv'\n",
    "response = requests.get(url)\n",
    "if response.ok:\n",
    "    with open('./data/games.csv', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "else:\n",
    "    raise Exception(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "df = pd.read_csv('./data/games.csv')\n",
    "df = df[df['season'] >= 2010]\n",
    "\n",
    "# Standardize team names\n",
    "standardize_mapping = {\n",
    "    'OAK': 'LVR',  # Oakland Raiders to Las Vegas Raiders\n",
    "    'SD': 'LAC',   # San Diego Chargers to Los Angeles Chargers\n",
    "    'STL': 'LAR',  # St. Louis Rams to Los Angeles Rams\n",
    "    'LA': 'LAR',   # Los Angeles Rams\n",
    "    'LV': 'LVR'    # Las Vegas Raiders\n",
    "}\n",
    "df['away_team'] = df['away_team'].replace(standardize_mapping)\n",
    "df['home_team'] = df['home_team'].replace(standardize_mapping)\n",
    "df.rename(columns={'gameday': 'date'}, inplace=True)\n",
    "df = df[df['season'] != 1999]\n",
    "\n",
    "# Standardize the 'game_id' column\n",
    "df['game_id'] = df['game_id'].apply(lambda x: f\"{x.split('_')[0]}_{x.split('_')[1]}_{standardize_mapping.get(x.split('_')[2], x.split('_')[2])}_{standardize_mapping.get(x.split('_')[3], x.split('_')[3])}\")\n",
    "\n",
    "# Convert datetime and create new columns\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['week'] = df['week'].apply(lambda x: f'{x:02d}')\n",
    "df['game_id_simple'] = df['season'].astype(str) + \"_\" + df['week']\n",
    "df['game_id_team1'] = df['game_id_simple'] + \"_\" + df['home_team']\n",
    "df['game_id_team2'] = df['game_id_simple'] + \"_\" + df['away_team']\n",
    "\n",
    "# Select columns \n",
    "selected_columns = [\n",
    "    'game_id', 'season', 'week', 'game_type', 'date', 'weekday', 'gametime', \n",
    "    'away_team', 'away_score', 'home_team', 'home_score', 'location', 'result',\t'total', 'overtime', \n",
    "    'spread_line', 'total_line', 'away_rest', 'home_rest', 'roof', 'surface', 'temp', 'wind', \n",
    "    'away_qb_id', 'home_qb_id', 'away_qb_name', 'home_qb_name', 'away_coach', 'home_coach', 'referee',\n",
    "    'stadium_id', 'stadium', 'game_id_simple', 'game_id_team1', 'game_id_team2', 'pfr'\n",
    "]\n",
    "df_selected = df[selected_columns]\n",
    "\n",
    "# Save\n",
    "db_path = 'nfl.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "df_selected.to_sql('Games', conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "df_selected.to_csv('./data/games_modified.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3943552f-f7bd-45c8-829f-11049c0274c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete all games before 2015 season\n",
    "\n",
    "# # Path to your SQLite database\n",
    "# db_path = 'nfl.db'  # Replace this with the correct path\n",
    "\n",
    "# # Connect to the database\n",
    "# conn = sqlite3.connect(db_path)\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # SQL query to delete all games before 2015\n",
    "# delete_query = \"\"\"\n",
    "# DELETE FROM Games\n",
    "# WHERE season < 2015;\n",
    "# \"\"\"\n",
    "\n",
    "# # Execute the query\n",
    "# cursor.execute(delete_query)\n",
    "\n",
    "# # Commit the changes\n",
    "# conn.commit()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d0cf5-b52a-4af0-ae8d-b13e75aeec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Old interceptions code\n",
    "### Old sacks code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e7051c-55cc-4cb5-ab1f-f96bdd71ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved player_stats_2023.csv\n",
      "Downloaded and saved player_stats_2024.csv\n",
      "Merged and cleaned player stats saved to './data/player_stats.csv'\n",
      "Final cleaned player stats saved to './data/player_stats.csv'\n",
      "Player stats saved to 'PlayerStats' table in nfl.db\n"
     ]
    }
   ],
   "source": [
    "# Create 'Players' in nfl.db\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# for year in range(2015, 2025):\n",
    "for year in range(2023, 2025):\n",
    "    url = f\"https://github.com/nflverse/nflverse-data/releases/download/player_stats/player_stats_{year}.csv\"\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        file_path = os.path.join('./data/player-stats/', f\"player_stats_{year}.csv\")\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded and saved player_stats_{year}.csv\")\n",
    "        \n",
    "        # Load and append the downloaded file to the list of DataFrames\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'opponent_team' in df.columns:\n",
    "            df = df.drop(columns=['opponent_team'])\n",
    "        dataframes.append(df)\n",
    "    else:\n",
    "        print(f\"Failed to download data for the year {year}\")\n",
    "\n",
    "# Merge all player stats DataFrames\n",
    "merged_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "# Standardize team abbreviations and format weeks\n",
    "# standardize_mapping = {'LA': 'LAR', 'LV': 'LVR'}\n",
    "standardize_mapping = {\n",
    "    'OAK': 'LVR',  # Oakland Raiders to Las Vegas Raiders\n",
    "    'SD': 'LAC',   # San Diego Chargers to Los Angeles Chargers\n",
    "    'STL': 'LAR',  # St. Louis Rams to Los Angeles Rams\n",
    "    'LA': 'LAR',   # Los Angeles Rams\n",
    "    'LV': 'LVR'    # Las Vegas Raiders\n",
    "}\n",
    "merged_df['recent_team'] = merged_df['recent_team'].replace(standardize_mapping)\n",
    "merged_df['week'] = merged_df['week'].apply(lambda x: f'{x:02d}')\n",
    "\n",
    "# Create game IDs\n",
    "merged_df['game_id_team'] = merged_df['season'].astype(str) + '_' + merged_df['week'].astype(str) + '_' + merged_df['recent_team']\n",
    "merged_df['game_id_simple'] = merged_df['season'].astype(str) + '_' + merged_df['week'].astype(str)\n",
    "\n",
    "# Save\n",
    "merged_df.to_csv('./data/player_stats.csv', index=False)\n",
    "print(\"Merged and cleaned player stats saved to './data/player_stats.csv'\")\n",
    "\n",
    "# Merge with game data from games.csv\n",
    "# games_df = pd.read_csv('./data/games.csv')\n",
    "# game_id_map = pd.concat([\n",
    "#     games_df[['game_id_team1', 'game_id', 'home_team', 'away_team']].rename(columns={'game_id_team1': 'game_id_team'}),\n",
    "#     games_df[['game_id_team2', 'game_id', 'home_team', 'away_team']].rename(columns={'game_id_team2': 'game_id_team'})\n",
    "# ]).drop_duplicates(subset=['game_id_team'])\n",
    "# merged_df = merged_df.merge(game_id_map, on='game_id_team', how='left')\n",
    "games_df = pd.read_csv('./data/games_modified.csv')\n",
    "game_id_map = pd.concat([\n",
    "    games_df[['game_id_team1', 'game_id', 'home_team', 'away_team']].rename(columns={'game_id_team1': 'game_id_team'}),\n",
    "    games_df[['game_id_team2', 'game_id', 'home_team', 'away_team']].rename(columns={'game_id_team2': 'game_id_team'})\n",
    "]).drop_duplicates(subset=['game_id_team'])\n",
    "merged_df = merged_df.merge(game_id_map, on='game_id_team', how='left')\n",
    "\n",
    "\n",
    "# Clean the DataFrame\n",
    "position_groups_to_remove = ['SPEC', 'LB', 'DB', 'OL', 'DL']\n",
    "df_cleaned = merged_df[~merged_df['position_group'].isin(position_groups_to_remove)].dropna(subset=['position_group'])\n",
    "\n",
    "df_cleaned.to_csv('./data/player_stats.csv', index=False)\n",
    "print(\"Final cleaned player stats saved to './data/player_stats.csv'\")\n",
    "\n",
    "# Save the cleaned player stats to SQLite database\n",
    "conn = sqlite3.connect('nfl.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL command to create the 'PlayerStats' table\n",
    "# player_current_team TEXT,  -- Renamed column\n",
    "create_table_sql = '''\n",
    "CREATE TABLE IF NOT EXISTS PlayerStats (\n",
    "    player_display_name TEXT,\n",
    "    game_id TEXT,\n",
    "    season INTEGER,\n",
    "    week INTEGER,\n",
    "    position TEXT,\n",
    "    headshot_url TEXT,\n",
    "    completions INTEGER,\n",
    "    attempts INTEGER,\n",
    "    passing_yards INTEGER,\n",
    "    passing_tds INTEGER,\n",
    "    interceptions INTEGER,\n",
    "    sacks INTEGER,\n",
    "    carries INTEGER,\n",
    "    rushing_yards INTEGER,\n",
    "    rushing_tds INTEGER,\n",
    "    rushing_fumbles INTEGER,\n",
    "    receptions INTEGER,\n",
    "    targets INTEGER,\n",
    "    receiving_yards INTEGER,\n",
    "    receiving_tds INTEGER,\n",
    "    receiving_fumbles INTEGER,\n",
    "    fantasy_points_ppr REAL,\n",
    "    home_team TEXT,\n",
    "    away_team TEXT,\n",
    "    player_current_team TEXT\n",
    ");\n",
    "'''\n",
    "\n",
    "# Execute the SQL command\n",
    "cursor.execute(create_table_sql)\n",
    "\n",
    "# Load the cleaned data into a DataFrame\n",
    "df = pd.read_csv('./data/player_stats.csv')\n",
    "\n",
    "# Rename the 'recent_team' column to 'player_current_team'\n",
    "df.rename(columns={'recent_team': 'player_current_team'}, inplace=True)\n",
    "\n",
    "# Select only the relevant columns to import\n",
    "columns_to_import = ['player_display_name', 'player_current_team', 'game_id', 'season', 'week', \n",
    "                     'position', 'headshot_url', 'completions', 'attempts', 'passing_yards', \n",
    "                     'passing_tds', 'interceptions', 'sacks', 'carries', 'rushing_yards', \n",
    "                     'rushing_tds', 'rushing_fumbles', 'receptions', 'targets', 'receiving_yards', \n",
    "                     'receiving_tds', 'receiving_fumbles', 'fantasy_points_ppr', 'home_team', 'away_team']\n",
    "\n",
    "df_to_import = df[columns_to_import]\n",
    "\n",
    "# Import data into the 'PlayerStats' table\n",
    "df_to_import.to_sql('PlayerStats', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "print(\"Player stats saved to 'PlayerStats' table in nfl.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ce83e2-12df-4fc9-97fa-819cecdd95ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved roster_2023.csv\n",
      "Downloaded and saved roster_2024.csv\n",
      "Final file saved to ./data/rosters.csv\n",
      "Rosters table created and data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create 'Rosters' in nfl.db (2015-2025)\n",
    "\n",
    "# Iterate through the years 2015 to 2025\n",
    "# for year in range(2015, 2025):\n",
    "for year in range(2023, 2025):\n",
    "    # Construct the URL for the CSV file of the specific year\n",
    "    url = f\"https://github.com/nflverse/nflverse-data/releases/download/rosters/roster_{year}.csv\"\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Open the file in write-binary mode and save the CSV\n",
    "        with open(f\"./data/rosters/roster_{year}.csv\", 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded and saved roster_{year}.csv\")\n",
    "    else:\n",
    "        print(f\"Failed to download data for the year {year}\")\n",
    "\n",
    "        \n",
    "# Combine roster data years\n",
    "dataframes = []\n",
    "\n",
    "for year in range(2023, 2025):\n",
    "    file_path = f'./data/rosters/roster_{year}.csv'\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "merged_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Make pfr_id's\n",
    "base_url = \"https://www.pro-football-reference.com/players/\"\n",
    "merged_data['url'] = merged_data['pfr_id'].apply(lambda x: f\"{base_url}{x[0]}/{x}.htm\" if pd.notna(x) else None)\n",
    "\n",
    "merged_data.to_csv('./data/rosters.csv', index=False)\n",
    "print(\"Final file saved to ./data/rosters.csv\")\n",
    "\n",
    "conn = sqlite3.connect('nfl.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"DROP TABLE IF EXISTS rosters\")\n",
    "\n",
    "# SQL command to create the 'Rosters' table without any primary key\n",
    "create_table_sql = '''\n",
    "CREATE TABLE IF NOT EXISTS Rosters (\n",
    "    season INTEGER,\n",
    "    team TEXT,\n",
    "    position TEXT,\n",
    "    depth_chart_position TEXT,\n",
    "    status TEXT,\n",
    "    full_name TEXT,\n",
    "    first_name TEXT,\n",
    "    last_name TEXT,\n",
    "    birth_date TEXT,\n",
    "    height REAL,\n",
    "    weight REAL,\n",
    "    college TEXT,\n",
    "    pfr_id TEXT,\n",
    "    years_exp REAL,\n",
    "    headshot_url TEXT,\n",
    "    week INTEGER,\n",
    "    game_type TEXT,\n",
    "    entry_year REAL,\n",
    "    rookie_year REAL,\n",
    "    draft_club TEXT,\n",
    "    draft_number REAL,\n",
    "    url TEXT\n",
    ");\n",
    "'''\n",
    "\n",
    "# Execute the SQL command to create the table\n",
    "cursor.execute(create_table_sql)\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df = pd.read_csv('data/rosters.csv')\n",
    "\n",
    "# Insert the DataFrame data into the 'rosters' table\n",
    "df.to_sql('Rosters', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Rosters table created and data inserted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c113d2-d80a-45db-86d5-2ffe925c836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rosters table standardized and updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Standardize Team Names in Rosters table\n",
    "\n",
    "standardize_mapping = {\n",
    "    'ARZ': 'ARI',  # Arizona Cardinals\n",
    "    'BLT': 'BAL',  # Baltimore Ravens\n",
    "    'CLV': 'CLE',  # Cleveland Browns\n",
    "    'HST': 'HOU',  # Houston Texans\n",
    "    'LA': 'LAR',   # Los Angeles Rams\n",
    "    'LV': 'LVR',   # Las Vegas Raiders\n",
    "    'OAK': 'LVR',  # Oakland Raiders to Las Vegas Raiders\n",
    "    'SD': 'LAC',   # San Diego Chargers to Los Angeles Chargers\n",
    "    'SL': 'LAR'    # St. Louis Rams to Los Angeles Rams\n",
    "}\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('nfl.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load the Rosters data into a DataFrame\n",
    "df = pd.read_sql_query(\"SELECT * FROM Rosters\", conn)\n",
    "\n",
    "# Standardize team abbreviations in the 'team' and 'draft_club' columns\n",
    "df['team'] = df['team'].replace(standardize_mapping)\n",
    "df['draft_club'] = df['draft_club'].replace(standardize_mapping)\n",
    "\n",
    "# Save the updated DataFrame back to the database\n",
    "df.to_sql('Rosters', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "print(\"Rosters table standardized and updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f14453d-7cf8-4923-a018-2f648e6f96e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ARI\n",
      "2. ATL\n",
      "3. BAL\n",
      "4. BUF\n",
      "5. CAR\n",
      "6. CHI\n",
      "7. CIN\n",
      "8. CLE\n",
      "9. DAL\n",
      "10. DEN\n",
      "11. DET\n",
      "12. GB\n",
      "13. HOU\n",
      "14. IND\n",
      "15. JAX\n",
      "16. KC\n",
      "17. LAC\n",
      "18. LAR\n",
      "19. LVR\n",
      "20. MIA\n",
      "21. MIN\n",
      "22. NE\n",
      "23. NO\n",
      "24. NYG\n",
      "25. NYJ\n",
      "26. PHI\n",
      "27. PIT\n",
      "28. SEA\n",
      "29. SF\n",
      "30. TB\n",
      "31. TEN\n",
      "32. WAS\n"
     ]
    }
   ],
   "source": [
    "# Standardize Team Names in rosters.csv\n",
    "\n",
    "file_path = 'data/rosters.csv'\n",
    "rosters_df = pd.read_csv(file_path)\n",
    "\n",
    "# Standardize mapping for team names\n",
    "standardize_mapping = {\n",
    "    'ARZ': 'ARI',  # Arizona Cardinals\n",
    "    'BLT': 'BAL',  # Baltimore Ravens\n",
    "    'CLV': 'CLE',  # Cleveland Browns\n",
    "    'HST': 'HOU',  # Houston Texans\n",
    "    'LA': 'LAR',   # Los Angeles Rams\n",
    "    'LV': 'LVR',   # Las Vegas Raiders\n",
    "    'OAK': 'LVR',  # Oakland Raiders to Las Vegas Raiders\n",
    "    'SD': 'LAC',   # San Diego Chargers to Los Angeles Chargers\n",
    "    'SL': 'LAR'    # St. Louis Rams to Los Angeles Rams\n",
    "}\n",
    "\n",
    "# Apply the standardization mapping to the 'team' column\n",
    "rosters_df['team'] = rosters_df['team'].replace(standardize_mapping)\n",
    "\n",
    "# Extract and print the unique standardized team names in a numbered list\n",
    "standardized_teams = rosters_df['team'].unique()\n",
    "standardized_team_list_sorted = sorted(list(standardized_teams))\n",
    "\n",
    "# Print the standardized team names\n",
    "for idx, team in enumerate(standardized_team_list_sorted, 1):\n",
    "    print(f\"{idx}. {team}\")\n",
    "\n",
    "# If you want to save the standardized data back to a CSV file\n",
    "rosters_df.to_csv('data/rosters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679eac3d-3ead-4128-9ddf-42a7a6e32a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Scores 2000-2025\n",
    "# Not in nfl.db currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef027ab-0d72-4152-beb2-f78d88171ca0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209080ram.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110atl.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110car.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110chi.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110cin.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110det.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110htx.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110mia.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110nyj.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110was.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110crd.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110sdg.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110min.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110oti.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209110dal.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209120sea.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209150kan.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180rav.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180cle.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180det.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180jax.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180nor.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180nyg.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180pit.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180ram.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180sfo.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180dal.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180den.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180rai.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209180gnb.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209190buf.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209190phi.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209220cle.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250car.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250chi.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250clt.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250mia.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250min.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250nwe.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250nyj.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250oti.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250was.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250sdg.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250crd.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250sea.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250tam.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209250den.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209260nyg.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202209290cin.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020nor.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020atl.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020rav.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020dal.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020det.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020htx.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020clt.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020nyg.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020phi.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020pit.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020car.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020gnb.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020rai.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210020tam.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210030sfo.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210060den.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090gnb.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090buf.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090cle.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090jax.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090min.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090nwe.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090nor.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090nyj.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090tam.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090was.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090car.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090crd.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090ram.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210090rav.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210100kan.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210130chi.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210160atl.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210160cle.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210160gnb.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210160clt.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210160mia.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210160nor.htm\n",
      "Scraping game: https://www.pro-football-reference.com/boxscores/202210160nyg.htm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 212\u001b[39m\n\u001b[32m    210\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    211\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError scraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m             \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScraping complete for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear_to_scrape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. The data has been saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    214\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAll box scores scraping complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # # Box Scores 2015-2025\n",
    "# # # Not in nfl.db currently\n",
    "\n",
    "# # df = pd.read_csv('./data/games.csv')\n",
    "\n",
    "# # # Create the 'pfr_url' column\n",
    "# # df['pfr_url'] = 'https://www.pro-football-reference.com/boxscores/' + df['pfr'] + '.htm'\n",
    "\n",
    "# # df.to_csv('./data/games.csv', index=False)\n",
    "\n",
    "# # csv_file_path = 'data/box_scores.csv'\n",
    "# # games_csv_path = 'data/games.csv'\n",
    "\n",
    "# # # Define the headers (for reference)\n",
    "# # headers = ['URL', 'Team', '1', '2', '3', '4', 'OT1', 'OT2', 'OT3', 'OT4', 'Final']\n",
    "\n",
    "# # # Open the CSV file for writing\n",
    "# # with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "# #     score_writer = csv.writer(csvfile)\n",
    "# #     score_writer.writerow(headers)  # Write the headers to the CSV file\n",
    "\n",
    "# #     # Loop through each year from 2022 to 2025\n",
    "# #     for year_to_scrape in range(2015, 2025):\n",
    "# #         # Read the URLs for the current season from 'games.csv'\n",
    "# #         game_urls = []\n",
    "# #         with open(games_csv_path, 'r') as csvfile:\n",
    "# #             reader = csv.DictReader(csvfile)\n",
    "# #             for row in reader:\n",
    "# #                 if row['season'] == str(year_to_scrape):  # Filter for the current season\n",
    "# #                     game_urls.append(row['pfr_url'])\n",
    "\n",
    "# #         # Iterate over each URL and scrape data\n",
    "# #         for url in game_urls:\n",
    "# #             try:    \n",
    "# #                 # Print the current game being scraped\n",
    "# #                 print(f\"Scraping game: {url}\")\n",
    "                \n",
    "# #                 # Send a GET request to the URL\n",
    "# #                 response = requests.get(url)\n",
    "# #                 response.raise_for_status()\n",
    "\n",
    "# #                 # Parse the content with BeautifulSoup\n",
    "# #                 soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# #                 # Find the linescore table by its class\n",
    "# #                 linescore_table = soup.find('table', class_='linescore')\n",
    "\n",
    "# #                 if linescore_table:\n",
    "# #                     # Find all rows in the linescore table, skip the header row\n",
    "# #                     rows = linescore_table.find_all('tr')[1:]\n",
    "\n",
    "# #                     # Extract and write the data from each row\n",
    "# #                     for row in rows:\n",
    "# #                         cols = row.find_all('td')\n",
    "# #                         team_name = cols[1].text.strip()\n",
    "# #                         scores = [col.text.strip() for col in cols[2:]]\n",
    "\n",
    "# #                         # Pad the scores list to match the headers length\n",
    "# #                         scores += [''] * (len(headers) - 2 - len(scores))\n",
    "\n",
    "# #                         score_writer.writerow([url, team_name] + scores)\n",
    "\n",
    "# #                 # Sleep for 3 seconds before the next request\n",
    "# #                 time.sleep(2)\n",
    "\n",
    "# #             except Exception as e:\n",
    "# #                 print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "# #             time.sleep(1)\n",
    "\n",
    "# # print(f\"Scraping complete. The data has been saved to {csv_file_path}.\")\n",
    "\n",
    "# # Box Scores 2015-2025\n",
    "# df = pd.read_csv('./data/games.csv')\n",
    "\n",
    "# # Create the 'pfr_url' column\n",
    "# df['pfr_url'] = 'https://www.pro-football-reference.com/boxscores/' + df['pfr'] + '.htm'\n",
    "\n",
    "# df.to_csv('./data/games.csv', index=False)\n",
    "\n",
    "# csv_file_path = 'data/box_scores.csv'\n",
    "# games_csv_path = 'data/games.csv'\n",
    "\n",
    "# # Define the headers (for reference)\n",
    "# headers = ['URL', 'Team', '1', '2', '3', '4', 'OT1', 'OT2', 'OT3', 'OT4', 'Final']\n",
    "\n",
    "# # Load existing box scores to avoid duplicates\n",
    "# existing_urls = set()\n",
    "# if os.path.exists(csv_file_path):\n",
    "#     with open(csv_file_path, 'r') as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             existing_urls.add(row['URL'])\n",
    "\n",
    "# # Open the CSV file for writing (append mode if file exists)\n",
    "# with open(csv_file_path, 'a', newline='') as csvfile:\n",
    "#     score_writer = csv.writer(csvfile)\n",
    "\n",
    "#     # Write headers only if the file is newly created\n",
    "#     if os.path.getsize(csv_file_path) == 0:\n",
    "#         score_writer.writerow(headers)  # Write the headers to the CSV file\n",
    "\n",
    "#     # Loop through each year from 2015 to 2025\n",
    "#     # for year_to_scrape in range(2015, 2025):\n",
    "#     for year_to_scrape in range(2018, 2025):\n",
    "#         # Read the URLs for the current season from 'games.csv'\n",
    "#         game_urls = []\n",
    "#         with open(games_csv_path, 'r') as csvfile:\n",
    "#             reader = csv.DictReader(csvfile)\n",
    "#             for row in reader:\n",
    "#                 if row['season'] == str(year_to_scrape):  # Filter for the current season\n",
    "#                     game_urls.append(row['pfr_url'])\n",
    "\n",
    "#         # Iterate over each URL and scrape data\n",
    "#         for url in game_urls:\n",
    "#             if url in existing_urls:\n",
    "#                 print(f\"Skipping already scraped game: {url}\")\n",
    "#                 continue\n",
    "\n",
    "#             try:    \n",
    "#                 # Print the current game being scraped\n",
    "#                 print(f\"Scraping game: {url}\")\n",
    "                \n",
    "#                 # Send a GET request to the URL\n",
    "#                 response = requests.get(url)\n",
    "#                 response.raise_for_status()\n",
    "\n",
    "#                 # Parse the content with BeautifulSoup\n",
    "#                 soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#                 # Find the linescore table by its class\n",
    "#                 linescore_table = soup.find('table', class_='linescore')\n",
    "\n",
    "#                 if linescore_table:\n",
    "#                     # Find all rows in the linescore table, skip the header row\n",
    "#                     rows = linescore_table.find_all('tr')[1:]\n",
    "\n",
    "#                     # Extract and write the data from each row\n",
    "#                     for row in rows:\n",
    "#                         cols = row.find_all('td')\n",
    "#                         team_name = cols[1].text.strip()\n",
    "#                         scores = [col.text.strip() for col in cols[2:]]\n",
    "\n",
    "#                         # Pad the scores list to match the headers length\n",
    "#                         scores += [''] * (len(headers) - 2 - len(scores))\n",
    "\n",
    "#                         score_writer.writerow([url, team_name] + scores)\n",
    "\n",
    "#                 # Sleep for 3 seconds before the next request\n",
    "#                 time.sleep(1.3)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "#             time.sleep(1)\n",
    "\n",
    "# print(f\"Scraping complete. The data has been saved to {csv_file_path}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Box Scores 2015-2025 - Organized by Year (UPDATED VERSION)\n",
    "df = pd.read_csv('./data/games.csv')\n",
    "df['pfr_url'] = 'https://www.pro-football-reference.com/boxscores/' + df['pfr'] + '.htm'\n",
    "df.to_csv('./data/games.csv', index=False)\n",
    "games_csv_path = 'data/games.csv'\n",
    "headers = ['URL', 'Team', '1', '2', '3', '4', 'OT1', 'OT2', 'OT3', 'OT4', 'Final']\n",
    "box_scores_dir = 'data/SR-box-scores'\n",
    "os.makedirs(box_scores_dir, exist_ok=True)\n",
    "for year_to_scrape in range(2022, 2025):\n",
    "    csv_file_path = f'{box_scores_dir}/all_box_scores_{year_to_scrape}.csv'\n",
    "    existing_urls = set()\n",
    "    if os.path.exists(csv_file_path):\n",
    "        with open(csv_file_path, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                existing_urls.add(row['URL'])\n",
    "    game_urls = []\n",
    "    with open(games_csv_path, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['season'] == str(year_to_scrape):\n",
    "                game_urls.append(row['pfr_url'])\n",
    "    with open(csv_file_path, 'a', newline='') as csvfile:\n",
    "        score_writer = csv.writer(csvfile)\n",
    "        if os.path.getsize(csv_file_path) == 0:\n",
    "            score_writer.writerow(headers)\n",
    "        for url in game_urls:\n",
    "            if url in existing_urls:\n",
    "                print(f\"Skipping already scraped game: {url}\")\n",
    "                continue\n",
    "            try:\n",
    "                print(f\"Scraping game: {url}\")\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                linescore_table = soup.find('table', class_='linescore')\n",
    "                if linescore_table:\n",
    "                    rows = linescore_table.find_all('tr')[1:]\n",
    "                    for row in rows:\n",
    "                        cols = row.find_all('td')\n",
    "                        team_name = cols[1].text.strip()\n",
    "                        scores = [col.text.strip() for col in cols[2:]]\n",
    "                        scores += [''] * (len(headers) - 2 - len(scores))\n",
    "                        score_writer.writerow([url, team_name] + scores)\n",
    "                time.sleep(1.3)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {url}: {e}\")\n",
    "            time.sleep(1)\n",
    "    print(f\"Scraping complete for {year_to_scrape}. The data has been saved to {csv_file_path}.\")\n",
    "print(\"All box scores scraping complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac195cd-594d-4365-bfa7-d0fe5e855204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix OT columns in box scores\n",
    "# Not in nfl.db currently\n",
    "\n",
    "df = pd.read_csv('data/box_scores.csv')\n",
    "\n",
    "# Function to shift the furthest right value to the 'Final' column\n",
    "# def shift_to_final(row):\n",
    "#     for col in reversed(row.index[:-1]):\n",
    "#         if pd.notna(row[col]):\n",
    "#             row['Final'] = row[col]\n",
    "#             row[col] = None\n",
    "#             break\n",
    "#     return row\n",
    "def shift_to_final(row):\n",
    "    if pd.isna(row['Final']):  # Only shift if the 'Final' column is empty\n",
    "        for col in reversed(row.index[:-1]):\n",
    "            if pd.notna(row[col]):\n",
    "                row['Final'] = row[col]\n",
    "                row[col] = None\n",
    "                break\n",
    "    return row\n",
    "\n",
    "\n",
    "# Apply the function to each row\n",
    "df = df.apply(shift_to_final, axis=1)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('data/box_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f7226-8192-4b46-ae88-2cd4fe264ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add box scores to nfl.db\n",
    "# Not in nfl.db currently\n",
    "# Haven't tested\n",
    "\n",
    "# box_scores_path = '/mnt/data/box_scores.csv'\n",
    "# box_scores_df = pd.read_csv(box_scores_path)\n",
    "\n",
    "# # Mapping of full team names to their abbreviations\n",
    "# team_name_mapping = {\n",
    "#     'Buffalo Bills': 'BUF',\n",
    "#     'Denver Broncos': 'DEN',\n",
    "#     'Houston Texans': 'HOU',\n",
    "#     'New York Jets': 'NYJ',\n",
    "#     'Washington Redskins': 'WAS',\n",
    "#     'Pittsburgh Steelers': 'PIT',\n",
    "#     'Washington Football Team': 'WAS',\n",
    "#     'Minnesota Vikings': 'MIN',\n",
    "#     'Seattle Seahawks': 'SEA',\n",
    "#     'Los Angeles Chargers': 'LAC',\n",
    "#     'Dallas Cowboys': 'DAL',\n",
    "#     'St. Louis Rams': 'LAR',\n",
    "#     'Cincinnati Bengals': 'CIN',\n",
    "#     'Washington Commanders': 'WAS',\n",
    "#     'Baltimore Ravens': 'BAL',\n",
    "#     'Jacksonville Jaguars': 'JAX',\n",
    "#     'Green Bay Packers': 'GB',\n",
    "#     'Detroit Lions': 'DET',z\n",
    "#     'Atlanta Falcons': 'ATL',\n",
    "#     'Tampa Bay Buccaneers': 'TB',\n",
    "#     'Arizona Cardinals': 'ARI',\n",
    "#     'Las Vegas Raiders': 'LVR',\n",
    "#     'San Francisco 49ers': 'SF',\n",
    "#     'Cleveland Browns': 'CLE',\n",
    "#     'San Diego Chargers': 'LAC',\n",
    "#     'Kansas City Chiefs': 'KC',\n",
    "#     'Tennessee Titans': 'TEN',\n",
    "#     'Carolina Panthers': 'CAR',\n",
    "#     'Chicago Bears': 'CHI',\n",
    "#     'New England Patriots': 'NE',\n",
    "#     'Philadelphia Eagles': 'PHI',\n",
    "#     'Los Angeles Rams': 'LAR',\n",
    "#     'New Orleans Saints': 'NO',\n",
    "#     'Oakland Raiders': 'LVR',\n",
    "#     'Miami Dolphins': 'MIA',\n",
    "#     'New York Giants': 'NYG',\n",
    "#     'Indianapolis Colts': 'IND'\n",
    "# }\n",
    "\n",
    "# # Apply the mapping to the 'Team' column in box_scores_df\n",
    "# box_scores_df['Team'] = box_scores_df['Team'].map(team_name_mapping)\n",
    "\n",
    "# # Verify that the mapping has been applied correctly\n",
    "# cleaned_team_names = box_scores_df['Team'].unique().tolist()\n",
    "# print(\"Cleaned team names:\", cleaned_team_names)\n",
    "\n",
    "# # Now, let's merge the cleaned box scores data into your SQLite database\n",
    "# db_path = '/mnt/data/nfl_updated.db'\n",
    "\n",
    "# # Connect to the SQLite database\n",
    "# conn = sqlite3.connect(db_path)\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Create a new table for BoxScores if it doesn't exist\n",
    "# cursor.execute('''\n",
    "#     CREATE TABLE IF NOT EXISTS BoxScores (\n",
    "#         URL TEXT,\n",
    "#         Team TEXT,\n",
    "#         Q1 INTEGER,\n",
    "#         Q2 INTEGER,\n",
    "#         Q3 INTEGER,\n",
    "#         Q4 INTEGER,\n",
    "#         OT1 INTEGER,\n",
    "#         OT2 INTEGER,\n",
    "#         OT3 INTEGER,\n",
    "#         OT4 INTEGER,\n",
    "#         Final INTEGER\n",
    "#     );\n",
    "# ''')\n",
    "\n",
    "# # Insert cleaned data into the BoxScores table\n",
    "# box_scores_df.to_sql('BoxScores', conn, if_exists='replace', index=False)\n",
    "\n",
    "# # Commit the changes and close the connection\n",
    "# conn.commit()\n",
    "# conn.close()\n",
    "\n",
    "# print(\"Box scores data has been successfully merged into the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e96f399-2a51-442a-add0-e888e647ba15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped scoring data for game ID: 2023_01_DET_KC, PFR: 202309070kan\n",
      "Successfully scraped scoring data for game ID: 2023_01_CAR_ATL, PFR: 202309100atl\n",
      "Successfully scraped scoring data for game ID: 2023_01_HOU_BAL, PFR: 202309100rav\n",
      "Successfully scraped scoring data for game ID: 2023_01_CIN_CLE, PFR: 202309100cle\n",
      "Successfully scraped scoring data for game ID: 2023_01_JAX_IND, PFR: 202309100clt\n",
      "Successfully scraped scoring data for game ID: 2023_01_TB_MIN, PFR: 202309100min\n",
      "Successfully scraped scoring data for game ID: 2023_01_TEN_NO, PFR: 202309100nor\n",
      "Successfully scraped scoring data for game ID: 2023_01_SF_PIT, PFR: 202309100pit\n",
      "Successfully scraped scoring data for game ID: 2023_01_ARI_WAS, PFR: 202309100was\n",
      "Successfully scraped scoring data for game ID: 2023_01_GB_CHI, PFR: 202309100chi\n",
      "Successfully scraped scoring data for game ID: 2023_01_LV_DEN, PFR: 202309100den\n",
      "Successfully scraped scoring data for game ID: 2023_01_MIA_LAC, PFR: 202309100sdg\n",
      "Successfully scraped scoring data for game ID: 2023_01_PHI_NE, PFR: 202309100nwe\n",
      "Successfully scraped scoring data for game ID: 2023_01_LA_SEA, PFR: 202309100sea\n",
      "Successfully scraped scoring data for game ID: 2023_01_DAL_NYG, PFR: 202309100nyg\n",
      "Successfully scraped scoring data for game ID: 2023_01_BUF_NYJ, PFR: 202309110nyj\n",
      "Successfully scraped scoring data for game ID: 2023_02_MIN_PHI, PFR: 202309140phi\n",
      "Successfully scraped scoring data for game ID: 2023_02_GB_ATL, PFR: 202309170atl\n",
      "Successfully scraped scoring data for game ID: 2023_02_LV_BUF, PFR: 202309170buf\n",
      "Successfully scraped scoring data for game ID: 2023_02_BAL_CIN, PFR: 202309170cin\n",
      "Successfully scraped scoring data for game ID: 2023_02_SEA_DET, PFR: 202309170det\n",
      "Successfully scraped scoring data for game ID: 2023_02_IND_HOU, PFR: 202309170htx\n",
      "Successfully scraped scoring data for game ID: 2023_02_KC_JAX, PFR: 202309170jax\n",
      "Successfully scraped scoring data for game ID: 2023_02_CHI_TB, PFR: 202309170tam\n",
      "Successfully scraped scoring data for game ID: 2023_02_LAC_TEN, PFR: 202309170oti\n",
      "Successfully scraped scoring data for game ID: 2023_02_NYG_ARI, PFR: 202309170crd\n",
      "Successfully scraped scoring data for game ID: 2023_02_SF_LA, PFR: 202309170ram\n",
      "Successfully scraped scoring data for game ID: 2023_02_NYJ_DAL, PFR: 202309170dal\n",
      "Successfully scraped scoring data for game ID: 2023_02_WAS_DEN, PFR: 202309170den\n",
      "Successfully scraped scoring data for game ID: 2023_02_MIA_NE, PFR: 202309170nwe\n",
      "Successfully scraped scoring data for game ID: 2023_02_NO_CAR, PFR: 202309180car\n",
      "Successfully scraped scoring data for game ID: 2023_02_CLE_PIT, PFR: 202309180pit\n",
      "Successfully scraped scoring data for game ID: 2023_03_NYG_SF, PFR: 202309210sfo\n",
      "Successfully scraped scoring data for game ID: 2023_03_IND_BAL, PFR: 202309240rav\n",
      "Successfully scraped scoring data for game ID: 2023_03_TEN_CLE, PFR: 202309240cle\n",
      "Successfully scraped scoring data for game ID: 2023_03_ATL_DET, PFR: 202309240det\n",
      "Successfully scraped scoring data for game ID: 2023_03_NO_GB, PFR: 202309240gnb\n",
      "Successfully scraped scoring data for game ID: 2023_03_HOU_JAX, PFR: 202309240jax\n",
      "Successfully scraped scoring data for game ID: 2023_03_DEN_MIA, PFR: 202309240mia\n",
      "Successfully scraped scoring data for game ID: 2023_03_LAC_MIN, PFR: 202309240min\n",
      "Successfully scraped scoring data for game ID: 2023_03_NE_NYJ, PFR: 202309240nyj\n",
      "Successfully scraped scoring data for game ID: 2023_03_BUF_WAS, PFR: 202309240was\n",
      "Successfully scraped scoring data for game ID: 2023_03_CAR_SEA, PFR: 202309240sea\n",
      "Successfully scraped scoring data for game ID: 2023_03_DAL_ARI, PFR: 202309240crd\n",
      "Successfully scraped scoring data for game ID: 2023_03_CHI_KC, PFR: 202309240kan\n",
      "Successfully scraped scoring data for game ID: 2023_03_PIT_LV, PFR: 202309240rai\n",
      "Successfully scraped scoring data for game ID: 2023_03_PHI_TB, PFR: 202309250tam\n",
      "Successfully scraped scoring data for game ID: 2023_03_LA_CIN, PFR: 202309250cin\n",
      "Successfully scraped scoring data for game ID: 2023_04_DET_GB, PFR: 202309280gnb\n",
      "Successfully scraped scoring data for game ID: 2023_04_ATL_JAX, PFR: 202310010jax\n",
      "Successfully scraped scoring data for game ID: 2023_04_MIA_BUF, PFR: 202310010buf\n",
      "Successfully scraped scoring data for game ID: 2023_04_MIN_CAR, PFR: 202310010car\n",
      "Successfully scraped scoring data for game ID: 2023_04_DEN_CHI, PFR: 202310010chi\n",
      "Successfully scraped scoring data for game ID: 2023_04_BAL_CLE, PFR: 202310010cle\n",
      "Successfully scraped scoring data for game ID: 2023_04_PIT_HOU, PFR: 202310010htx\n",
      "Successfully scraped scoring data for game ID: 2023_04_LA_IND, PFR: 202310010clt\n",
      "Successfully scraped scoring data for game ID: 2023_04_TB_NO, PFR: 202310010nor\n",
      "Successfully scraped scoring data for game ID: 2023_04_WAS_PHI, PFR: 202310010phi\n",
      "Successfully scraped scoring data for game ID: 2023_04_CIN_TEN, PFR: 202310010oti\n",
      "Successfully scraped scoring data for game ID: 2023_04_LV_LAC, PFR: 202310010sdg\n",
      "Successfully scraped scoring data for game ID: 2023_04_NE_DAL, PFR: 202310010dal\n",
      "Successfully scraped scoring data for game ID: 2023_04_ARI_SF, PFR: 202310010sfo\n",
      "Successfully scraped scoring data for game ID: 2023_04_KC_NYJ, PFR: 202310010nyj\n",
      "Successfully scraped scoring data for game ID: 2023_04_SEA_NYG, PFR: 202310020nyg\n",
      "Successfully scraped scoring data for game ID: 2023_05_CHI_WAS, PFR: 202310050was\n",
      "Successfully scraped scoring data for game ID: 2023_05_JAX_BUF, PFR: 202310080buf\n",
      "Successfully scraped scoring data for game ID: 2023_05_HOU_ATL, PFR: 202310080atl\n",
      "Successfully scraped scoring data for game ID: 2023_05_CAR_DET, PFR: 202310080det\n",
      "Successfully scraped scoring data for game ID: 2023_05_TEN_IND, PFR: 202310080clt\n",
      "Successfully scraped scoring data for game ID: 2023_05_NYG_MIA, PFR: 202310080mia\n",
      "Successfully scraped scoring data for game ID: 2023_05_NO_NE, PFR: 202310080nwe\n",
      "Successfully scraped scoring data for game ID: 2023_05_BAL_PIT, PFR: 202310080pit\n",
      "Successfully scraped scoring data for game ID: 2023_05_CIN_ARI, PFR: 202310080crd\n",
      "Successfully scraped scoring data for game ID: 2023_05_PHI_LA, PFR: 202310080ram\n",
      "Successfully scraped scoring data for game ID: 2023_05_NYJ_DEN, PFR: 202310080den\n",
      "Successfully scraped scoring data for game ID: 2023_05_KC_MIN, PFR: 202310080min\n",
      "Successfully scraped scoring data for game ID: 2023_05_DAL_SF, PFR: 202310080sfo\n",
      "Successfully scraped scoring data for game ID: 2023_05_GB_LV, PFR: 202310090rai\n",
      "Successfully scraped scoring data for game ID: 2023_06_DEN_KC, PFR: 202310120kan\n",
      "Successfully scraped scoring data for game ID: 2023_06_BAL_TEN, PFR: 202310150oti\n",
      "Successfully scraped scoring data for game ID: 2023_06_WAS_ATL, PFR: 202310150atl\n",
      "Successfully scraped scoring data for game ID: 2023_06_MIN_CHI, PFR: 202310150chi\n",
      "Successfully scraped scoring data for game ID: 2023_06_SEA_CIN, PFR: 202310150cin\n",
      "Successfully scraped scoring data for game ID: 2023_06_SF_CLE, PFR: 202310150cle\n",
      "Successfully scraped scoring data for game ID: 2023_06_NO_HOU, PFR: 202310150htx\n",
      "Successfully scraped scoring data for game ID: 2023_06_IND_JAX, PFR: 202310150jax\n",
      "Successfully scraped scoring data for game ID: 2023_06_CAR_MIA, PFR: 202310150mia\n",
      "Successfully scraped scoring data for game ID: 2023_06_NE_LV, PFR: 202310150rai\n",
      "Successfully scraped scoring data for game ID: 2023_06_ARI_LA, PFR: 202310150ram\n",
      "Successfully scraped scoring data for game ID: 2023_06_PHI_NYJ, PFR: 202310150nyj\n",
      "Successfully scraped scoring data for game ID: 2023_06_DET_TB, PFR: 202310150tam\n",
      "Successfully scraped scoring data for game ID: 2023_06_NYG_BUF, PFR: 202310150buf\n",
      "Successfully scraped scoring data for game ID: 2023_06_DAL_LAC, PFR: 202310160sdg\n",
      "Successfully scraped scoring data for game ID: 2023_07_JAX_NO, PFR: 202310190nor\n",
      "Successfully scraped scoring data for game ID: 2023_07_DET_BAL, PFR: 202310220rav\n",
      "Successfully scraped scoring data for game ID: 2023_07_LV_CHI, PFR: 202310220chi\n",
      "Successfully scraped scoring data for game ID: 2023_07_CLE_IND, PFR: 202310220clt\n",
      "Successfully scraped scoring data for game ID: 2023_07_BUF_NE, PFR: 202310220nwe\n",
      "Successfully scraped scoring data for game ID: 2023_07_WAS_NYG, PFR: 202310220nyg\n",
      "Successfully scraped scoring data for game ID: 2023_07_ATL_TB, PFR: 202310220tam\n",
      "Successfully scraped scoring data for game ID: 2023_07_PIT_LA, PFR: 202310220ram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped scoring data for game ID: 2023_07_ARI_SEA, PFR: 202310220sea\n",
      "Successfully scraped scoring data for game ID: 2023_07_GB_DEN, PFR: 202310220den\n",
      "Successfully scraped scoring data for game ID: 2023_07_LAC_KC, PFR: 202310220kan\n",
      "Successfully scraped scoring data for game ID: 2023_07_MIA_PHI, PFR: 202310220phi\n",
      "Successfully scraped scoring data for game ID: 2023_07_SF_MIN, PFR: 202310230min\n",
      "Successfully scraped scoring data for game ID: 2023_08_TB_BUF, PFR: 202310260buf\n",
      "Successfully scraped scoring data for game ID: 2023_08_HOU_CAR, PFR: 202310290car\n",
      "Successfully scraped scoring data for game ID: 2023_08_LA_DAL, PFR: 202310290dal\n",
      "Successfully scraped scoring data for game ID: 2023_08_MIN_GB, PFR: 202310290gnb\n",
      "Successfully scraped scoring data for game ID: 2023_08_NO_IND, PFR: 202310290clt\n",
      "Successfully scraped scoring data for game ID: 2023_08_NE_MIA, PFR: 202310290mia\n",
      "Successfully scraped scoring data for game ID: 2023_08_NYJ_NYG, PFR: 202310290nyg\n",
      "Successfully scraped scoring data for game ID: 2023_08_JAX_PIT, PFR: 202310290pit\n",
      "Successfully scraped scoring data for game ID: 2023_08_ATL_TEN, PFR: 202310290oti\n",
      "Successfully scraped scoring data for game ID: 2023_08_PHI_WAS, PFR: 202310290was\n",
      "Successfully scraped scoring data for game ID: 2023_08_CLE_SEA, PFR: 202310290sea\n",
      "Successfully scraped scoring data for game ID: 2023_08_BAL_ARI, PFR: 202310290crd\n",
      "Successfully scraped scoring data for game ID: 2023_08_KC_DEN, PFR: 202310290den\n",
      "Successfully scraped scoring data for game ID: 2023_08_CIN_SF, PFR: 202310290sfo\n",
      "Successfully scraped scoring data for game ID: 2023_08_CHI_LAC, PFR: 202310290sdg\n",
      "Successfully scraped scoring data for game ID: 2023_08_LV_DET, PFR: 202310300det\n",
      "Successfully scraped scoring data for game ID: 2023_09_TEN_PIT, PFR: 202311020pit\n",
      "Successfully scraped scoring data for game ID: 2023_09_MIA_KC, PFR: 202311050kan\n",
      "Successfully scraped scoring data for game ID: 2023_09_MIN_ATL, PFR: 202311050atl\n",
      "Successfully scraped scoring data for game ID: 2023_09_SEA_BAL, PFR: 202311050rav\n",
      "Successfully scraped scoring data for game ID: 2023_09_ARI_CLE, PFR: 202311050cle\n",
      "Successfully scraped scoring data for game ID: 2023_09_LA_GB, PFR: 202311050gnb\n",
      "Successfully scraped scoring data for game ID: 2023_09_TB_HOU, PFR: 202311050htx\n",
      "Successfully scraped scoring data for game ID: 2023_09_WAS_NE, PFR: 202311050nwe\n",
      "Successfully scraped scoring data for game ID: 2023_09_CHI_NO, PFR: 202311050nor\n",
      "Successfully scraped scoring data for game ID: 2023_09_IND_CAR, PFR: 202311050car\n",
      "Successfully scraped scoring data for game ID: 2023_09_NYG_LV, PFR: 202311050rai\n",
      "Successfully scraped scoring data for game ID: 2023_09_DAL_PHI, PFR: 202311050phi\n",
      "Successfully scraped scoring data for game ID: 2023_09_BUF_CIN, PFR: 202311050cin\n",
      "Successfully scraped scoring data for game ID: 2023_09_LAC_NYJ, PFR: 202311060nyj\n",
      "Successfully scraped scoring data for game ID: 2023_10_CAR_CHI, PFR: 202311090chi\n",
      "Successfully scraped scoring data for game ID: 2023_10_IND_NE, PFR: 202311120nwe\n",
      "Successfully scraped scoring data for game ID: 2023_10_CLE_BAL, PFR: 202311120rav\n",
      "Successfully scraped scoring data for game ID: 2023_10_HOU_CIN, PFR: 202311120cin\n",
      "Successfully scraped scoring data for game ID: 2023_10_SF_JAX, PFR: 202311120jax\n",
      "Successfully scraped scoring data for game ID: 2023_10_NO_MIN, PFR: 202311120min\n",
      "Successfully scraped scoring data for game ID: 2023_10_GB_PIT, PFR: 202311120pit\n",
      "Successfully scraped scoring data for game ID: 2023_10_TEN_TB, PFR: 202311120tam\n",
      "Successfully scraped scoring data for game ID: 2023_10_ATL_ARI, PFR: 202311120crd\n",
      "Successfully scraped scoring data for game ID: 2023_10_DET_LAC, PFR: 202311120sdg\n",
      "Successfully scraped scoring data for game ID: 2023_10_NYG_DAL, PFR: 202311120dal\n",
      "Successfully scraped scoring data for game ID: 2023_10_WAS_SEA, PFR: 202311120sea\n",
      "Successfully scraped scoring data for game ID: 2023_10_NYJ_LV, PFR: 202311120rai\n",
      "Successfully scraped scoring data for game ID: 2023_10_DEN_BUF, PFR: 202311130buf\n",
      "Successfully scraped scoring data for game ID: 2023_11_CIN_BAL, PFR: 202311160rav\n",
      "Successfully scraped scoring data for game ID: 2023_11_DAL_CAR, PFR: 202311190car\n",
      "Successfully scraped scoring data for game ID: 2023_11_PIT_CLE, PFR: 202311190cle\n",
      "Successfully scraped scoring data for game ID: 2023_11_CHI_DET, PFR: 202311190det\n",
      "Successfully scraped scoring data for game ID: 2023_11_LAC_GB, PFR: 202311190gnb\n",
      "Successfully scraped scoring data for game ID: 2023_11_ARI_HOU, PFR: 202311190htx\n",
      "Successfully scraped scoring data for game ID: 2023_11_TEN_JAX, PFR: 202311190jax\n",
      "Successfully scraped scoring data for game ID: 2023_11_LV_MIA, PFR: 202311190mia\n",
      "Successfully scraped scoring data for game ID: 2023_11_NYG_WAS, PFR: 202311190was\n",
      "Successfully scraped scoring data for game ID: 2023_11_TB_SF, PFR: 202311190sfo\n",
      "Successfully scraped scoring data for game ID: 2023_11_NYJ_BUF, PFR: 202311190buf\n",
      "Successfully scraped scoring data for game ID: 2023_11_SEA_LA, PFR: 202311190ram\n",
      "Successfully scraped scoring data for game ID: 2023_11_MIN_DEN, PFR: 202311190den\n",
      "Successfully scraped scoring data for game ID: 2023_11_PHI_KC, PFR: 202311200kan\n",
      "Successfully scraped scoring data for game ID: 2023_12_GB_DET, PFR: 202311230det\n",
      "Successfully scraped scoring data for game ID: 2023_12_WAS_DAL, PFR: 202311230dal\n",
      "Successfully scraped scoring data for game ID: 2023_12_SF_SEA, PFR: 202311230sea\n",
      "Successfully scraped scoring data for game ID: 2023_12_MIA_NYJ, PFR: 202311240nyj\n",
      "Successfully scraped scoring data for game ID: 2023_12_NO_ATL, PFR: 202311260atl\n",
      "Successfully scraped scoring data for game ID: 2023_12_PIT_CIN, PFR: 202311260cin\n",
      "Successfully scraped scoring data for game ID: 2023_12_JAX_HOU, PFR: 202311260htx\n",
      "Successfully scraped scoring data for game ID: 2023_12_TB_IND, PFR: 202311260clt\n",
      "Successfully scraped scoring data for game ID: 2023_12_NE_NYG, PFR: 202311260nyg\n",
      "Successfully scraped scoring data for game ID: 2023_12_CAR_TEN, PFR: 202311260oti\n",
      "Successfully scraped scoring data for game ID: 2023_12_LA_ARI, PFR: 202311260crd\n",
      "Successfully scraped scoring data for game ID: 2023_12_CLE_DEN, PFR: 202311260den\n",
      "Successfully scraped scoring data for game ID: 2023_12_KC_LV, PFR: 202311260rai\n",
      "Successfully scraped scoring data for game ID: 2023_12_BUF_PHI, PFR: 202311260phi\n",
      "Successfully scraped scoring data for game ID: 2023_12_BAL_LAC, PFR: 202311260sdg\n",
      "Successfully scraped scoring data for game ID: 2023_12_CHI_MIN, PFR: 202311270min\n",
      "Successfully scraped scoring data for game ID: 2023_13_SEA_DAL, PFR: 202311300dal\n",
      "Successfully scraped scoring data for game ID: 2023_13_DEN_HOU, PFR: 202312030htx\n",
      "Successfully scraped scoring data for game ID: 2023_13_LAC_NE, PFR: 202312030nwe\n",
      "Successfully scraped scoring data for game ID: 2023_13_DET_NO, PFR: 202312030nor\n",
      "Successfully scraped scoring data for game ID: 2023_13_ATL_NYJ, PFR: 202312030nyj\n",
      "Successfully scraped scoring data for game ID: 2023_13_ARI_PIT, PFR: 202312030pit\n",
      "Successfully scraped scoring data for game ID: 2023_13_IND_TEN, PFR: 202312030oti\n",
      "Successfully scraped scoring data for game ID: 2023_13_MIA_WAS, PFR: 202312030was\n",
      "Successfully scraped scoring data for game ID: 2023_13_CAR_TB, PFR: 202312030tam\n",
      "Successfully scraped scoring data for game ID: 2023_13_CLE_LA, PFR: 202312030ram\n",
      "Successfully scraped scoring data for game ID: 2023_13_SF_PHI, PFR: 202312030phi\n",
      "Successfully scraped scoring data for game ID: 2023_13_KC_GB, PFR: 202312030gnb\n",
      "Successfully scraped scoring data for game ID: 2023_13_CIN_JAX, PFR: 202312040jax\n",
      "Successfully scraped scoring data for game ID: 2023_14_NE_PIT, PFR: 202312070pit\n",
      "Successfully scraped scoring data for game ID: 2023_14_TB_ATL, PFR: 202312100atl\n",
      "Successfully scraped scoring data for game ID: 2023_14_LA_BAL, PFR: 202312100rav\n",
      "Successfully scraped scoring data for game ID: 2023_14_DET_CHI, PFR: 202312100chi\n",
      "Successfully scraped scoring data for game ID: 2023_14_IND_CIN, PFR: 202312100cin\n",
      "Successfully scraped scoring data for game ID: 2023_14_JAX_CLE, PFR: 202312100cle\n",
      "Successfully scraped scoring data for game ID: 2023_14_CAR_NO, PFR: 202312100nor\n",
      "Successfully scraped scoring data for game ID: 2023_14_HOU_NYJ, PFR: 202312100nyj\n",
      "Successfully scraped scoring data for game ID: 2023_14_MIN_LV, PFR: 202312100rai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped scoring data for game ID: 2023_14_SEA_SF, PFR: 202312100sfo\n",
      "Successfully scraped scoring data for game ID: 2023_14_BUF_KC, PFR: 202312100kan\n",
      "Successfully scraped scoring data for game ID: 2023_14_DEN_LAC, PFR: 202312100sdg\n",
      "Successfully scraped scoring data for game ID: 2023_14_PHI_DAL, PFR: 202312100dal\n",
      "Successfully scraped scoring data for game ID: 2023_14_TEN_MIA, PFR: 202312110mia\n",
      "Successfully scraped scoring data for game ID: 2023_14_GB_NYG, PFR: 202312110nyg\n",
      "Successfully scraped scoring data for game ID: 2023_15_LAC_LV, PFR: 202312140rai\n",
      "Successfully scraped scoring data for game ID: 2023_15_MIN_CIN, PFR: 202312160cin\n",
      "Successfully scraped scoring data for game ID: 2023_15_PIT_IND, PFR: 202312160clt\n",
      "Successfully scraped scoring data for game ID: 2023_15_DEN_DET, PFR: 202312160det\n",
      "Successfully scraped scoring data for game ID: 2023_15_ATL_CAR, PFR: 202312170car\n",
      "Successfully scraped scoring data for game ID: 2023_15_CHI_CLE, PFR: 202312170cle\n",
      "Successfully scraped scoring data for game ID: 2023_15_TB_GB, PFR: 202312170gnb\n",
      "Successfully scraped scoring data for game ID: 2023_15_NYJ_MIA, PFR: 202312170mia\n",
      "Successfully scraped scoring data for game ID: 2023_15_KC_NE, PFR: 202312170nwe\n",
      "Successfully scraped scoring data for game ID: 2023_15_NYG_NO, PFR: 202312170nor\n",
      "Successfully scraped scoring data for game ID: 2023_15_HOU_TEN, PFR: 202312170oti\n",
      "Successfully scraped scoring data for game ID: 2023_15_SF_ARI, PFR: 202312170crd\n",
      "Successfully scraped scoring data for game ID: 2023_15_WAS_LA, PFR: 202312170ram\n",
      "Successfully scraped scoring data for game ID: 2023_15_DAL_BUF, PFR: 202312170buf\n",
      "Successfully scraped scoring data for game ID: 2023_15_BAL_JAX, PFR: 202312170jax\n",
      "Successfully scraped scoring data for game ID: 2023_15_PHI_SEA, PFR: 202312180sea\n",
      "Successfully scraped scoring data for game ID: 2023_16_NO_LA, PFR: 202312210ram\n",
      "Successfully scraped scoring data for game ID: 2023_16_CIN_PIT, PFR: 202312230pit\n",
      "Successfully scraped scoring data for game ID: 2023_16_BUF_LAC, PFR: 202312230sdg\n",
      "Successfully scraped scoring data for game ID: 2023_16_IND_ATL, PFR: 202312240atl\n",
      "Successfully scraped scoring data for game ID: 2023_16_GB_CAR, PFR: 202312240car\n",
      "Successfully scraped scoring data for game ID: 2023_16_CLE_HOU, PFR: 202312240htx\n",
      "Successfully scraped scoring data for game ID: 2023_16_DET_MIN, PFR: 202312240min\n",
      "Successfully scraped scoring data for game ID: 2023_16_WAS_NYJ, PFR: 202312240nyj\n",
      "Successfully scraped scoring data for game ID: 2023_16_SEA_TEN, PFR: 202312240oti\n",
      "Successfully scraped scoring data for game ID: 2023_16_JAX_TB, PFR: 202312240tam\n",
      "Successfully scraped scoring data for game ID: 2023_16_ARI_CHI, PFR: 202312240chi\n",
      "Successfully scraped scoring data for game ID: 2023_16_DAL_MIA, PFR: 202312240mia\n",
      "Successfully scraped scoring data for game ID: 2023_16_NE_DEN, PFR: 202312240den\n",
      "Successfully scraped scoring data for game ID: 2023_16_LV_KC, PFR: 202312250kan\n",
      "Successfully scraped scoring data for game ID: 2023_16_NYG_PHI, PFR: 202312250phi\n",
      "Successfully scraped scoring data for game ID: 2023_16_BAL_SF, PFR: 202312250sfo\n",
      "Successfully scraped scoring data for game ID: 2023_17_NYJ_CLE, PFR: 202312280cle\n",
      "Successfully scraped scoring data for game ID: 2023_17_DET_DAL, PFR: 202312300dal\n",
      "Successfully scraped scoring data for game ID: 2023_17_MIA_BAL, PFR: 202312310rav\n",
      "Successfully scraped scoring data for game ID: 2023_17_NE_BUF, PFR: 202312310buf\n",
      "Successfully scraped scoring data for game ID: 2023_17_ATL_CHI, PFR: 202312310chi\n",
      "Successfully scraped scoring data for game ID: 2023_17_TEN_HOU, PFR: 202312310htx\n",
      "Successfully scraped scoring data for game ID: 2023_17_LV_IND, PFR: 202312310clt\n",
      "Successfully scraped scoring data for game ID: 2023_17_CAR_JAX, PFR: 202312310jax\n",
      "Successfully scraped scoring data for game ID: 2023_17_LA_NYG, PFR: 202312310nyg\n",
      "Successfully scraped scoring data for game ID: 2023_17_ARI_PHI, PFR: 202312310phi\n",
      "Successfully scraped scoring data for game ID: 2023_17_NO_TB, PFR: 202312310tam\n",
      "Successfully scraped scoring data for game ID: 2023_17_SF_WAS, PFR: 202312310was\n",
      "Successfully scraped scoring data for game ID: 2023_17_PIT_SEA, PFR: 202312310sea\n",
      "Successfully scraped scoring data for game ID: 2023_17_LAC_DEN, PFR: 202312310den\n",
      "Successfully scraped scoring data for game ID: 2023_17_CIN_KC, PFR: 202312310kan\n",
      "Successfully scraped scoring data for game ID: 2023_17_GB_MIN, PFR: 202312310min\n",
      "Successfully scraped scoring data for game ID: 2023_18_PIT_BAL, PFR: 202401060rav\n",
      "Successfully scraped scoring data for game ID: 2023_18_HOU_IND, PFR: 202401060clt\n",
      "Successfully scraped scoring data for game ID: 2023_18_TB_CAR, PFR: 202401070car\n",
      "Successfully scraped scoring data for game ID: 2023_18_CLE_CIN, PFR: 202401070cin\n",
      "Successfully scraped scoring data for game ID: 2023_18_MIN_DET, PFR: 202401070det\n",
      "Successfully scraped scoring data for game ID: 2023_18_NYJ_NE, PFR: 202401070nwe\n",
      "Successfully scraped scoring data for game ID: 2023_18_ATL_NO, PFR: 202401070nor\n",
      "Successfully scraped scoring data for game ID: 2023_18_JAX_TEN, PFR: 202401070oti\n",
      "Successfully scraped scoring data for game ID: 2023_18_SEA_ARI, PFR: 202401070crd\n",
      "Successfully scraped scoring data for game ID: 2023_18_CHI_GB, PFR: 202401070gnb\n",
      "Successfully scraped scoring data for game ID: 2023_18_KC_LAC, PFR: 202401070sdg\n",
      "Successfully scraped scoring data for game ID: 2023_18_DEN_LV, PFR: 202401070rai\n",
      "Successfully scraped scoring data for game ID: 2023_18_PHI_NYG, PFR: 202401070nyg\n",
      "Successfully scraped scoring data for game ID: 2023_18_LA_SF, PFR: 202401070sfo\n",
      "Successfully scraped scoring data for game ID: 2023_18_DAL_WAS, PFR: 202401070was\n",
      "Successfully scraped scoring data for game ID: 2023_18_BUF_MIA, PFR: 202401070mia\n",
      "Successfully scraped scoring data for game ID: 2023_19_CLE_HOU, PFR: 202401130htx\n",
      "Successfully scraped scoring data for game ID: 2023_19_MIA_KC, PFR: 202401130kan\n",
      "Successfully scraped scoring data for game ID: 2023_19_GB_DAL, PFR: 202401140dal\n",
      "Successfully scraped scoring data for game ID: 2023_19_LA_DET, PFR: 202401140det\n",
      "Successfully scraped scoring data for game ID: 2023_19_PIT_BUF, PFR: 202401150buf\n",
      "Successfully scraped scoring data for game ID: 2023_19_PHI_TB, PFR: 202401150tam\n",
      "Successfully scraped scoring data for game ID: 2023_20_HOU_BAL, PFR: 202401200rav\n",
      "Successfully scraped scoring data for game ID: 2023_20_GB_SF, PFR: 202401200sfo\n",
      "Successfully scraped scoring data for game ID: 2023_20_TB_DET, PFR: 202401210det\n",
      "Successfully scraped scoring data for game ID: 2023_20_KC_BUF, PFR: 202401210buf\n",
      "Successfully scraped scoring data for game ID: 2023_21_KC_BAL, PFR: 202401280rav\n",
      "Successfully scraped scoring data for game ID: 2023_21_DET_SF, PFR: 202401280sfo\n",
      "Successfully scraped scoring data for game ID: 2023_22_SF_KC, PFR: 202402110kan\n",
      "Scraping completed for 2023. Scoring data saved to ./data/scoring-tables/all_nfl_scoring_tables_2023.csv.\n",
      "Successfully scraped scoring data for game ID: 2024_01_BAL_KC, PFR: 202409050kan\n",
      "Successfully scraped scoring data for game ID: 2024_01_GB_PHI, PFR: 202409060phi\n",
      "Successfully scraped scoring data for game ID: 2024_01_PIT_ATL, PFR: 202409080atl\n",
      "Successfully scraped scoring data for game ID: 2024_01_ARI_BUF, PFR: 202409080buf\n",
      "Successfully scraped scoring data for game ID: 2024_01_TEN_CHI, PFR: 202409080chi\n",
      "Successfully scraped scoring data for game ID: 2024_01_NE_CIN, PFR: 202409080cin\n",
      "Successfully scraped scoring data for game ID: 2024_01_HOU_IND, PFR: 202409080clt\n",
      "Successfully scraped scoring data for game ID: 2024_01_JAX_MIA, PFR: 202409080mia\n",
      "Successfully scraped scoring data for game ID: 2024_01_CAR_NO, PFR: 202409080nor\n",
      "Successfully scraped scoring data for game ID: 2024_01_MIN_NYG, PFR: 202409080nyg\n",
      "Successfully scraped scoring data for game ID: 2024_01_LV_LAC, PFR: 202409080sdg\n",
      "Successfully scraped scoring data for game ID: 2024_01_DEN_SEA, PFR: 202409080sea\n",
      "Successfully scraped scoring data for game ID: 2024_01_DAL_CLE, PFR: 202409080cle\n",
      "Successfully scraped scoring data for game ID: 2024_01_WAS_TB, PFR: 202409080tam\n",
      "Successfully scraped scoring data for game ID: 2024_01_LA_DET, PFR: 202409080det\n",
      "Successfully scraped scoring data for game ID: 2024_01_NYJ_SF, PFR: 202409090sfo\n",
      "Successfully scraped scoring data for game ID: 2024_02_BUF_MIA, PFR: 202409120mia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped scoring data for game ID: 2024_02_LV_BAL, PFR: 202409150rav\n",
      "Successfully scraped scoring data for game ID: 2024_02_LAC_CAR, PFR: 202409150car\n",
      "Successfully scraped scoring data for game ID: 2024_02_NO_DAL, PFR: 202409150dal\n",
      "Successfully scraped scoring data for game ID: 2024_02_TB_DET, PFR: 202409150det\n",
      "Successfully scraped scoring data for game ID: 2024_02_IND_GB, PFR: 202409150gnb\n",
      "Successfully scraped scoring data for game ID: 2024_02_CLE_JAX, PFR: 202409150jax\n",
      "Successfully scraped scoring data for game ID: 2024_02_SF_MIN, PFR: 202409150min\n",
      "Successfully scraped scoring data for game ID: 2024_02_SEA_NE, PFR: 202409150nwe\n",
      "Successfully scraped scoring data for game ID: 2024_02_NYJ_TEN, PFR: 202409150oti\n",
      "Successfully scraped scoring data for game ID: 2024_02_NYG_WAS, PFR: 202409150was\n",
      "Successfully scraped scoring data for game ID: 2024_02_LA_ARI, PFR: 202409150crd\n",
      "Successfully scraped scoring data for game ID: 2024_02_PIT_DEN, PFR: 202409150den\n",
      "Successfully scraped scoring data for game ID: 2024_02_CIN_KC, PFR: 202409150kan\n",
      "Successfully scraped scoring data for game ID: 2024_02_CHI_HOU, PFR: 202409150htx\n",
      "Successfully scraped scoring data for game ID: 2024_02_ATL_PHI, PFR: 202409160phi\n",
      "Successfully scraped scoring data for game ID: 2024_03_NE_NYJ, PFR: 202409190nyj\n",
      "Successfully scraped scoring data for game ID: 2024_03_NYG_CLE, PFR: 202409220cle\n",
      "Successfully scraped scoring data for game ID: 2024_03_CHI_IND, PFR: 202409220clt\n",
      "Successfully scraped scoring data for game ID: 2024_03_HOU_MIN, PFR: 202409220min\n",
      "Successfully scraped scoring data for game ID: 2024_03_PHI_NO, PFR: 202409220nor\n",
      "Successfully scraped scoring data for game ID: 2024_03_LAC_PIT, PFR: 202409220pit\n",
      "Successfully scraped scoring data for game ID: 2024_03_DEN_TB, PFR: 202409220tam\n",
      "Successfully scraped scoring data for game ID: 2024_03_GB_TEN, PFR: 202409220oti\n",
      "Successfully scraped scoring data for game ID: 2024_03_CAR_LV, PFR: 202409220rai\n",
      "Successfully scraped scoring data for game ID: 2024_03_MIA_SEA, PFR: 202409220sea\n",
      "Successfully scraped scoring data for game ID: 2024_03_DET_ARI, PFR: 202409220crd\n",
      "Successfully scraped scoring data for game ID: 2024_03_BAL_DAL, PFR: 202409220dal\n",
      "Successfully scraped scoring data for game ID: 2024_03_SF_LA, PFR: 202409220ram\n",
      "Successfully scraped scoring data for game ID: 2024_03_KC_ATL, PFR: 202409220atl\n",
      "Successfully scraped scoring data for game ID: 2024_03_JAX_BUF, PFR: 202409230buf\n",
      "Successfully scraped scoring data for game ID: 2024_03_WAS_CIN, PFR: 202409230cin\n",
      "Successfully scraped scoring data for game ID: 2024_04_DAL_NYG, PFR: 202409260nyg\n",
      "Successfully scraped scoring data for game ID: 2024_04_NO_ATL, PFR: 202409290atl\n",
      "Successfully scraped scoring data for game ID: 2024_04_CIN_CAR, PFR: 202409290car\n",
      "Successfully scraped scoring data for game ID: 2024_04_LA_CHI, PFR: 202409290chi\n",
      "Successfully scraped scoring data for game ID: 2024_04_MIN_GB, PFR: 202409290gnb\n",
      "Successfully scraped scoring data for game ID: 2024_04_JAX_HOU, PFR: 202409290htx\n",
      "Successfully scraped scoring data for game ID: 2024_04_PIT_IND, PFR: 202409290clt\n",
      "Successfully scraped scoring data for game ID: 2024_04_DEN_NYJ, PFR: 202409290nyj\n",
      "Successfully scraped scoring data for game ID: 2024_04_PHI_TB, PFR: 202409290tam\n",
      "Successfully scraped scoring data for game ID: 2024_04_WAS_ARI, PFR: 202409290crd\n",
      "Successfully scraped scoring data for game ID: 2024_04_NE_SF, PFR: 202409290sfo\n",
      "Successfully scraped scoring data for game ID: 2024_04_KC_LAC, PFR: 202409290sdg\n",
      "Successfully scraped scoring data for game ID: 2024_04_CLE_LV, PFR: 202409290rai\n",
      "Successfully scraped scoring data for game ID: 2024_04_BUF_BAL, PFR: 202409290rav\n",
      "Successfully scraped scoring data for game ID: 2024_04_TEN_MIA, PFR: 202409300mia\n",
      "Successfully scraped scoring data for game ID: 2024_04_SEA_DET, PFR: 202409300det\n",
      "Successfully scraped scoring data for game ID: 2024_05_TB_ATL, PFR: 202410030atl\n",
      "Successfully scraped scoring data for game ID: 2024_05_NYJ_MIN, PFR: 202410060min\n",
      "Successfully scraped scoring data for game ID: 2024_05_CAR_CHI, PFR: 202410060chi\n",
      "Successfully scraped scoring data for game ID: 2024_05_BAL_CIN, PFR: 202410060cin\n",
      "Successfully scraped scoring data for game ID: 2024_05_BUF_HOU, PFR: 202410060htx\n",
      "Successfully scraped scoring data for game ID: 2024_05_IND_JAX, PFR: 202410060jax\n",
      "Successfully scraped scoring data for game ID: 2024_05_MIA_NE, PFR: 202410060nwe\n",
      "Successfully scraped scoring data for game ID: 2024_05_CLE_WAS, PFR: 202410060was\n",
      "Successfully scraped scoring data for game ID: 2024_05_LV_DEN, PFR: 202410060den\n",
      "Successfully scraped scoring data for game ID: 2024_05_ARI_SF, PFR: 202410060sfo\n",
      "Successfully scraped scoring data for game ID: 2024_05_GB_LA, PFR: 202410060ram\n",
      "Successfully scraped scoring data for game ID: 2024_05_NYG_SEA, PFR: 202410060sea\n",
      "Successfully scraped scoring data for game ID: 2024_05_DAL_PIT, PFR: 202410060pit\n",
      "Successfully scraped scoring data for game ID: 2024_05_NO_KC, PFR: 202410070kan\n",
      "Successfully scraped scoring data for game ID: 2024_06_SF_SEA, PFR: 202410100sea\n",
      "Successfully scraped scoring data for game ID: 2024_06_JAX_CHI, PFR: 202410130chi\n",
      "Successfully scraped scoring data for game ID: 2024_06_WAS_BAL, PFR: 202410130rav\n",
      "Successfully scraped scoring data for game ID: 2024_06_ARI_GB, PFR: 202410130gnb\n",
      "Successfully scraped scoring data for game ID: 2024_06_HOU_NE, PFR: 202410130nwe\n",
      "Successfully scraped scoring data for game ID: 2024_06_TB_NO, PFR: 202410130nor\n",
      "Successfully scraped scoring data for game ID: 2024_06_CLE_PHI, PFR: 202410130phi\n",
      "Successfully scraped scoring data for game ID: 2024_06_IND_TEN, PFR: 202410130oti\n",
      "Successfully scraped scoring data for game ID: 2024_06_LAC_DEN, PFR: 202410130den\n",
      "Successfully scraped scoring data for game ID: 2024_06_PIT_LV, PFR: 202410130rai\n",
      "Successfully scraped scoring data for game ID: 2024_06_ATL_CAR, PFR: 202410130car\n",
      "Successfully scraped scoring data for game ID: 2024_06_DET_DAL, PFR: 202410130dal\n",
      "Successfully scraped scoring data for game ID: 2024_06_CIN_NYG, PFR: 202410130nyg\n",
      "Successfully scraped scoring data for game ID: 2024_06_BUF_NYJ, PFR: 202410140nyj\n",
      "Successfully scraped scoring data for game ID: 2024_07_DEN_NO, PFR: 202410170nor\n",
      "Successfully scraped scoring data for game ID: 2024_07_NE_JAX, PFR: 202410200jax\n",
      "Successfully scraped scoring data for game ID: 2024_07_SEA_ATL, PFR: 202410200atl\n",
      "Successfully scraped scoring data for game ID: 2024_07_TEN_BUF, PFR: 202410200buf\n",
      "Successfully scraped scoring data for game ID: 2024_07_CIN_CLE, PFR: 202410200cle\n",
      "Successfully scraped scoring data for game ID: 2024_07_HOU_GB, PFR: 202410200gnb\n",
      "Successfully scraped scoring data for game ID: 2024_07_MIA_IND, PFR: 202410200clt\n",
      "Successfully scraped scoring data for game ID: 2024_07_DET_MIN, PFR: 202410200min\n",
      "Successfully scraped scoring data for game ID: 2024_07_PHI_NYG, PFR: 202410200nyg\n",
      "Successfully scraped scoring data for game ID: 2024_07_LV_LA, PFR: 202410200ram\n",
      "Successfully scraped scoring data for game ID: 2024_07_CAR_WAS, PFR: 202410200was\n",
      "Successfully scraped scoring data for game ID: 2024_07_KC_SF, PFR: 202410200sfo\n",
      "Successfully scraped scoring data for game ID: 2024_07_NYJ_PIT, PFR: 202410200pit\n",
      "Successfully scraped scoring data for game ID: 2024_07_BAL_TB, PFR: 202410210tam\n",
      "Successfully scraped scoring data for game ID: 2024_07_LAC_ARI, PFR: 202410210crd\n",
      "Successfully scraped scoring data for game ID: 2024_08_MIN_LA, PFR: 202410240ram\n",
      "Successfully scraped scoring data for game ID: 2024_08_PHI_CIN, PFR: 202410270cin\n",
      "Successfully scraped scoring data for game ID: 2024_08_BAL_CLE, PFR: 202410270cle\n",
      "Successfully scraped scoring data for game ID: 2024_08_TEN_DET, PFR: 202410270det\n",
      "Successfully scraped scoring data for game ID: 2024_08_IND_HOU, PFR: 202410270htx\n",
      "Successfully scraped scoring data for game ID: 2024_08_GB_JAX, PFR: 202410270jax\n",
      "Successfully scraped scoring data for game ID: 2024_08_ARI_MIA, PFR: 202410270mia\n",
      "Successfully scraped scoring data for game ID: 2024_08_NYJ_NE, PFR: 202410270nwe\n",
      "Successfully scraped scoring data for game ID: 2024_08_ATL_TB, PFR: 202410270tam\n",
      "Successfully scraped scoring data for game ID: 2024_08_NO_LAC, PFR: 202410270sdg\n",
      "Successfully scraped scoring data for game ID: 2024_08_BUF_SEA, PFR: 202410270sea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped scoring data for game ID: 2024_08_CAR_DEN, PFR: 202410270den\n",
      "Successfully scraped scoring data for game ID: 2024_08_KC_LV, PFR: 202410270rai\n",
      "Successfully scraped scoring data for game ID: 2024_08_CHI_WAS, PFR: 202410270was\n",
      "Successfully scraped scoring data for game ID: 2024_08_DAL_SF, PFR: 202410270sfo\n",
      "Successfully scraped scoring data for game ID: 2024_08_NYG_PIT, PFR: 202410280pit\n",
      "Successfully scraped scoring data for game ID: 2024_09_HOU_NYJ, PFR: 202410310nyj\n",
      "Successfully scraped scoring data for game ID: 2024_09_DAL_ATL, PFR: 202411030atl\n",
      "Successfully scraped scoring data for game ID: 2024_09_DEN_BAL, PFR: 202411030rav\n",
      "Successfully scraped scoring data for game ID: 2024_09_MIA_BUF, PFR: 202411030buf\n",
      "Successfully scraped scoring data for game ID: 2024_09_NO_CAR, PFR: 202411030car\n",
      "Successfully scraped scoring data for game ID: 2024_09_LV_CIN, PFR: 202411030cin\n",
      "Successfully scraped scoring data for game ID: 2024_09_LAC_CLE, PFR: 202411030cle\n",
      "Successfully scraped scoring data for game ID: 2024_09_WAS_NYG, PFR: 202411030nyg\n",
      "Successfully scraped scoring data for game ID: 2024_09_NE_TEN, PFR: 202411030oti\n",
      "Successfully scraped scoring data for game ID: 2024_09_CHI_ARI, PFR: 202411030crd\n",
      "Successfully scraped scoring data for game ID: 2024_09_JAX_PHI, PFR: 202411030phi\n",
      "Successfully scraped scoring data for game ID: 2024_09_DET_GB, PFR: 202411030gnb\n",
      "Successfully scraped scoring data for game ID: 2024_09_LA_SEA, PFR: 202411030sea\n",
      "Successfully scraped scoring data for game ID: 2024_09_IND_MIN, PFR: 202411030min\n",
      "Successfully scraped scoring data for game ID: 2024_09_TB_KC, PFR: 202411040kan\n",
      "Successfully scraped scoring data for game ID: 2024_10_CIN_BAL, PFR: 202411070rav\n",
      "Successfully scraped scoring data for game ID: 2024_10_NYG_CAR, PFR: 202411100car\n",
      "Successfully scraped scoring data for game ID: 2024_10_NE_CHI, PFR: 202411100chi\n",
      "Successfully scraped scoring data for game ID: 2024_10_BUF_IND, PFR: 202411100clt\n",
      "Successfully scraped scoring data for game ID: 2024_10_MIN_JAX, PFR: 202411100jax\n",
      "Successfully scraped scoring data for game ID: 2024_10_DEN_KC, PFR: 202411100kan\n",
      "Successfully scraped scoring data for game ID: 2024_10_ATL_NO, PFR: 202411100nor\n",
      "Successfully scraped scoring data for game ID: 2024_10_SF_TB, PFR: 202411100tam\n",
      "Successfully scraped scoring data for game ID: 2024_10_PIT_WAS, PFR: 202411100was\n",
      "Successfully scraped scoring data for game ID: 2024_10_TEN_LAC, PFR: 202411100sdg\n",
      "Successfully scraped scoring data for game ID: 2024_10_NYJ_ARI, PFR: 202411100crd\n",
      "Successfully scraped scoring data for game ID: 2024_10_PHI_DAL, PFR: 202411100dal\n",
      "Successfully scraped scoring data for game ID: 2024_10_DET_HOU, PFR: 202411100htx\n",
      "Successfully scraped scoring data for game ID: 2024_10_MIA_LA, PFR: 202411110ram\n",
      "Successfully scraped scoring data for game ID: 2024_11_WAS_PHI, PFR: 202411140phi\n",
      "Successfully scraped scoring data for game ID: 2024_11_GB_CHI, PFR: 202411170chi\n",
      "Successfully scraped scoring data for game ID: 2024_11_JAX_DET, PFR: 202411170det\n",
      "Successfully scraped scoring data for game ID: 2024_11_LV_MIA, PFR: 202411170mia\n",
      "Successfully scraped scoring data for game ID: 2024_11_LA_NE, PFR: 202411170nwe\n",
      "Successfully scraped scoring data for game ID: 2024_11_CLE_NO, PFR: 202411170nor\n",
      "Successfully scraped scoring data for game ID: 2024_11_IND_NYJ, PFR: 202411170nyj\n",
      "Successfully scraped scoring data for game ID: 2024_11_BAL_PIT, PFR: 202411170pit\n",
      "Successfully scraped scoring data for game ID: 2024_11_MIN_TEN, PFR: 202411170oti\n",
      "Successfully scraped scoring data for game ID: 2024_11_ATL_DEN, PFR: 202411170den\n",
      "Successfully scraped scoring data for game ID: 2024_11_SEA_SF, PFR: 202411170sfo\n",
      "Successfully scraped scoring data for game ID: 2024_11_KC_BUF, PFR: 202411170buf\n",
      "Successfully scraped scoring data for game ID: 2024_11_CIN_LAC, PFR: 202411170sdg\n",
      "Successfully scraped scoring data for game ID: 2024_11_HOU_DAL, PFR: 202411180dal\n",
      "Successfully scraped scoring data for game ID: 2024_12_PIT_CLE, PFR: 202411210cle\n",
      "Successfully scraped scoring data for game ID: 2024_12_KC_CAR, PFR: 202411240car\n",
      "Successfully scraped scoring data for game ID: 2024_12_MIN_CHI, PFR: 202411240chi\n",
      "Successfully scraped scoring data for game ID: 2024_12_TEN_HOU, PFR: 202411240htx\n",
      "Successfully scraped scoring data for game ID: 2024_12_DET_IND, PFR: 202411240clt\n",
      "Successfully scraped scoring data for game ID: 2024_12_NE_MIA, PFR: 202411240mia\n",
      "Successfully scraped scoring data for game ID: 2024_12_TB_NYG, PFR: 202411240nyg\n",
      "Successfully scraped scoring data for game ID: 2024_12_DAL_WAS, PFR: 202411240was\n",
      "Successfully scraped scoring data for game ID: 2024_12_DEN_LV, PFR: 202411240rai\n",
      "Successfully scraped scoring data for game ID: 2024_12_SF_GB, PFR: 202411240gnb\n",
      "Successfully scraped scoring data for game ID: 2024_12_ARI_SEA, PFR: 202411240sea\n",
      "Successfully scraped scoring data for game ID: 2024_12_PHI_LA, PFR: 202411240ram\n",
      "Successfully scraped scoring data for game ID: 2024_12_BAL_LAC, PFR: 202411250sdg\n",
      "Successfully scraped scoring data for game ID: 2024_13_CHI_DET, PFR: 202411280det\n",
      "Successfully scraped scoring data for game ID: 2024_13_NYG_DAL, PFR: 202411280dal\n",
      "Successfully scraped scoring data for game ID: 2024_13_MIA_GB, PFR: 202411280gnb\n",
      "Successfully scraped scoring data for game ID: 2024_13_LV_KC, PFR: 202411290kan\n",
      "Successfully scraped scoring data for game ID: 2024_13_LAC_ATL, PFR: 202412010atl\n",
      "Successfully scraped scoring data for game ID: 2024_13_PIT_CIN, PFR: 202412010cin\n",
      "Successfully scraped scoring data for game ID: 2024_13_HOU_JAX, PFR: 202412010jax\n",
      "Successfully scraped scoring data for game ID: 2024_13_ARI_MIN, PFR: 202412010min\n",
      "Successfully scraped scoring data for game ID: 2024_13_IND_NE, PFR: 202412010nwe\n",
      "Successfully scraped scoring data for game ID: 2024_13_SEA_NYJ, PFR: 202412010nyj\n",
      "Successfully scraped scoring data for game ID: 2024_13_TEN_WAS, PFR: 202412010was\n",
      "Successfully scraped scoring data for game ID: 2024_13_TB_CAR, PFR: 202412010car\n",
      "Successfully scraped scoring data for game ID: 2024_13_LA_NO, PFR: 202412010nor\n",
      "Successfully scraped scoring data for game ID: 2024_13_PHI_BAL, PFR: 202412010rav\n",
      "Successfully scraped scoring data for game ID: 2024_13_SF_BUF, PFR: 202412010buf\n",
      "Successfully scraped scoring data for game ID: 2024_13_CLE_DEN, PFR: 202412020den\n",
      "Successfully scraped scoring data for game ID: 2024_14_GB_DET, PFR: 202412050det\n",
      "Successfully scraped scoring data for game ID: 2024_14_NYJ_MIA, PFR: 202412080mia\n",
      "Successfully scraped scoring data for game ID: 2024_14_ATL_MIN, PFR: 202412080min\n",
      "Successfully scraped scoring data for game ID: 2024_14_NO_NYG, PFR: 202412080nyg\n",
      "Successfully scraped scoring data for game ID: 2024_14_CAR_PHI, PFR: 202412080phi\n",
      "Successfully scraped scoring data for game ID: 2024_14_CLE_PIT, PFR: 202412080pit\n",
      "Successfully scraped scoring data for game ID: 2024_14_LV_TB, PFR: 202412080tam\n",
      "Successfully scraped scoring data for game ID: 2024_14_JAX_TEN, PFR: 202412080oti\n",
      "Successfully scraped scoring data for game ID: 2024_14_SEA_ARI, PFR: 202412080crd\n",
      "Successfully scraped scoring data for game ID: 2024_14_BUF_LA, PFR: 202412080ram\n",
      "Successfully scraped scoring data for game ID: 2024_14_CHI_SF, PFR: 202412080sfo\n",
      "Successfully scraped scoring data for game ID: 2024_14_LAC_KC, PFR: 202412080kan\n",
      "Successfully scraped scoring data for game ID: 2024_14_CIN_DAL, PFR: 202412090dal\n",
      "Successfully scraped scoring data for game ID: 2024_15_LA_SF, PFR: 202412120sfo\n",
      "Successfully scraped scoring data for game ID: 2024_15_DAL_CAR, PFR: 202412150car\n",
      "Successfully scraped scoring data for game ID: 2024_15_KC_CLE, PFR: 202412150cle\n",
      "Successfully scraped scoring data for game ID: 2024_15_MIA_HOU, PFR: 202412150htx\n",
      "Successfully scraped scoring data for game ID: 2024_15_NYJ_JAX, PFR: 202412150jax\n",
      "Successfully scraped scoring data for game ID: 2024_15_WAS_NO, PFR: 202412150nor\n",
      "Successfully scraped scoring data for game ID: 2024_15_BAL_NYG, PFR: 202412150nyg\n",
      "Successfully scraped scoring data for game ID: 2024_15_CIN_TEN, PFR: 202412150oti\n",
      "Successfully scraped scoring data for game ID: 2024_15_NE_ARI, PFR: 202412150crd\n",
      "Successfully scraped scoring data for game ID: 2024_15_IND_DEN, PFR: 202412150den\n",
      "Successfully scraped scoring data for game ID: 2024_15_BUF_DET, PFR: 202412150det\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped scoring data for game ID: 2024_15_TB_LAC, PFR: 202412150sdg\n",
      "Successfully scraped scoring data for game ID: 2024_15_PIT_PHI, PFR: 202412150phi\n",
      "Successfully scraped scoring data for game ID: 2024_15_GB_SEA, PFR: 202412150sea\n",
      "Successfully scraped scoring data for game ID: 2024_15_CHI_MIN, PFR: 202412160min\n",
      "Successfully scraped scoring data for game ID: 2024_15_ATL_LV, PFR: 202412160rai\n",
      "Successfully scraped scoring data for game ID: 2024_16_DEN_LAC, PFR: 202412190sdg\n",
      "Successfully scraped scoring data for game ID: 2024_16_HOU_KC, PFR: 202412210kan\n",
      "Successfully scraped scoring data for game ID: 2024_16_PIT_BAL, PFR: 202412210rav\n",
      "Successfully scraped scoring data for game ID: 2024_16_NYG_ATL, PFR: 202412220atl\n",
      "Successfully scraped scoring data for game ID: 2024_16_ARI_CAR, PFR: 202412220car\n",
      "Successfully scraped scoring data for game ID: 2024_16_DET_CHI, PFR: 202412220chi\n",
      "Successfully scraped scoring data for game ID: 2024_16_CLE_CIN, PFR: 202412220cin\n",
      "Successfully scraped scoring data for game ID: 2024_16_TEN_IND, PFR: 202412220clt\n",
      "Successfully scraped scoring data for game ID: 2024_16_LA_NYJ, PFR: 202412220nyj\n",
      "Successfully scraped scoring data for game ID: 2024_16_PHI_WAS, PFR: 202412220was\n",
      "Successfully scraped scoring data for game ID: 2024_16_MIN_SEA, PFR: 202412220sea\n",
      "Successfully scraped scoring data for game ID: 2024_16_NE_BUF, PFR: 202412220buf\n",
      "Successfully scraped scoring data for game ID: 2024_16_JAX_LV, PFR: 202412220rai\n",
      "Successfully scraped scoring data for game ID: 2024_16_SF_MIA, PFR: 202412220mia\n",
      "Successfully scraped scoring data for game ID: 2024_16_TB_DAL, PFR: 202412220dal\n",
      "Successfully scraped scoring data for game ID: 2024_16_NO_GB, PFR: 202412230gnb\n",
      "Successfully scraped scoring data for game ID: 2024_17_KC_PIT, PFR: 202412250pit\n",
      "Successfully scraped scoring data for game ID: 2024_17_BAL_HOU, PFR: 202412250htx\n",
      "Successfully scraped scoring data for game ID: 2024_17_SEA_CHI, PFR: 202412260chi\n",
      "Successfully scraped scoring data for game ID: 2024_17_LAC_NE, PFR: 202412280nwe\n",
      "Successfully scraped scoring data for game ID: 2024_17_DEN_CIN, PFR: 202412280cin\n",
      "Successfully scraped scoring data for game ID: 2024_17_ARI_LA, PFR: 202412280ram\n",
      "Successfully scraped scoring data for game ID: 2024_17_NYJ_BUF, PFR: 202412290buf\n",
      "Successfully scraped scoring data for game ID: 2024_17_TEN_JAX, PFR: 202412290jax\n",
      "Successfully scraped scoring data for game ID: 2024_17_LV_NO, PFR: 202412290nor\n",
      "Successfully scraped scoring data for game ID: 2024_17_IND_NYG, PFR: 202412290nyg\n",
      "Successfully scraped scoring data for game ID: 2024_17_DAL_PHI, PFR: 202412290phi\n",
      "Successfully scraped scoring data for game ID: 2024_17_CAR_TB, PFR: 202412290tam\n",
      "Successfully scraped scoring data for game ID: 2024_17_MIA_CLE, PFR: 202412290cle\n",
      "Successfully scraped scoring data for game ID: 2024_17_GB_MIN, PFR: 202412290min\n",
      "Successfully scraped scoring data for game ID: 2024_17_ATL_WAS, PFR: 202412290was\n",
      "Successfully scraped scoring data for game ID: 2024_17_DET_SF, PFR: 202412300sfo\n",
      "Successfully scraped scoring data for game ID: 2024_18_CLE_BAL, PFR: 202501040rav\n",
      "Successfully scraped scoring data for game ID: 2024_18_CIN_PIT, PFR: 202501040pit\n",
      "Successfully scraped scoring data for game ID: 2024_18_CAR_ATL, PFR: 202501050atl\n",
      "Successfully scraped scoring data for game ID: 2024_18_WAS_DAL, PFR: 202501050dal\n",
      "Successfully scraped scoring data for game ID: 2024_18_CHI_GB, PFR: 202501050gnb\n",
      "Successfully scraped scoring data for game ID: 2024_18_JAX_IND, PFR: 202501050clt\n",
      "Successfully scraped scoring data for game ID: 2024_18_BUF_NE, PFR: 202501050nwe\n",
      "Successfully scraped scoring data for game ID: 2024_18_NYG_PHI, PFR: 202501050phi\n",
      "Successfully scraped scoring data for game ID: 2024_18_NO_TB, PFR: 202501050tam\n",
      "Successfully scraped scoring data for game ID: 2024_18_HOU_TEN, PFR: 202501050oti\n",
      "Successfully scraped scoring data for game ID: 2024_18_SF_ARI, PFR: 202501050crd\n",
      "Successfully scraped scoring data for game ID: 2024_18_KC_DEN, PFR: 202501050den\n",
      "Successfully scraped scoring data for game ID: 2024_18_SEA_LA, PFR: 202501050ram\n",
      "Successfully scraped scoring data for game ID: 2024_18_LAC_LV, PFR: 202501050rai\n",
      "Successfully scraped scoring data for game ID: 2024_18_MIA_NYJ, PFR: 202501050nyj\n",
      "Successfully scraped scoring data for game ID: 2024_18_MIN_DET, PFR: 202501050det\n",
      "Successfully scraped scoring data for game ID: 2024_19_LAC_HOU, PFR: 202501110htx\n",
      "Successfully scraped scoring data for game ID: 2024_19_PIT_BAL, PFR: 202501110rav\n",
      "Successfully scraped scoring data for game ID: 2024_19_DEN_BUF, PFR: 202501120buf\n",
      "Successfully scraped scoring data for game ID: 2024_19_GB_PHI, PFR: 202501120phi\n",
      "Successfully scraped scoring data for game ID: 2024_19_WAS_TB, PFR: 202501120tam\n",
      "Successfully scraped scoring data for game ID: 2024_19_MIN_LA, PFR: 202501130ram\n",
      "Successfully scraped scoring data for game ID: 2024_20_HOU_KC, PFR: 202501180kan\n",
      "Successfully scraped scoring data for game ID: 2024_20_WAS_DET, PFR: 202501180det\n",
      "Successfully scraped scoring data for game ID: 2024_20_LA_PHI, PFR: 202501190phi\n",
      "Successfully scraped scoring data for game ID: 2024_20_BAL_BUF, PFR: 202501190buf\n",
      "Successfully scraped scoring data for game ID: 2024_21_WAS_PHI, PFR: 202501260phi\n",
      "Successfully scraped scoring data for game ID: 2024_21_BUF_KC, PFR: 202501260kan\n",
      "Successfully scraped scoring data for game ID: 2024_22_KC_PHI, PFR: 202502090phi\n",
      "Scraping completed for 2024. Scoring data saved to ./data/scoring-tables/all_nfl_scoring_tables_2024.csv.\n"
     ]
    }
   ],
   "source": [
    "# Scoring Tables (touchdown logs)\n",
    "# Not in nfl.db currently\n",
    "\n",
    "# for year_to_scrape in range(2015, 2025):\n",
    "for year_to_scrape in range(2023, 2025):\n",
    "    # Initialize output CSV file with the year and \"scoring_tables\" in its name\n",
    "    output_filename = f'./data/scoring-tables/all_nfl_scoring_tables_{year_to_scrape}.csv'\n",
    "    with open(output_filename, 'w', newline='') as output_csvfile:\n",
    "        csvwriter = csv.writer(output_csvfile)\n",
    "        csvwriter.writerow(['Quarter', 'Time', 'Team', 'Detail', 'Team_1', 'Team_2', 'Game_ID'])  # Added 'Game_ID'\n",
    "\n",
    "        # Read the CSV file containing the game data\n",
    "        with open('./data/games.csv', 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            rows = [row for row in reader if int(row['game_id'].split('_')[0]) == year_to_scrape]  # Filter rows for the year\n",
    "\n",
    "            for row in rows:\n",
    "                pfr_value = row['pfr']\n",
    "                game_id = row['game_id']\n",
    "\n",
    "                # Form the URL using the 'pfr' value\n",
    "                url = f\"https://www.pro-football-reference.com/boxscores/{pfr_value}.htm\"\n",
    "\n",
    "                try:\n",
    "                    # Fetch the webpage\n",
    "                    response = requests.get(url)\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    # Find the table containing box score data using its class\n",
    "                    table = soup.find('table', {'id': 'scoring'})\n",
    "\n",
    "                    last_quarter = None  # To keep track of the last available quarter value\n",
    "\n",
    "                    # Loop through the rows to get the scoring data\n",
    "                    for i, row in enumerate(table.find_all('tr')):\n",
    "                        if i == 0:  # Skip the first header row\n",
    "                            continue\n",
    "                        cells = row.find_all(['td', 'th'])\n",
    "                        if len(cells) > 0:\n",
    "                            csv_row = [cell.text for cell in cells]\n",
    "\n",
    "                            # Fill in missing quarter values\n",
    "                            if csv_row[0]:\n",
    "                                last_quarter = csv_row[0]\n",
    "                            else:\n",
    "                                csv_row[0] = last_quarter\n",
    "\n",
    "                            csv_row.append(game_id)  # Append 'game_id' to each row\n",
    "                            csvwriter.writerow(csv_row)\n",
    "\n",
    "                    print(f\"Successfully scraped scoring data for game ID: {game_id}, PFR: {pfr_value}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while scraping {url}. Error: {e}\")\n",
    "\n",
    "                # Sleep for 3 seconds to avoid overloading the server\n",
    "                time.sleep(2)\n",
    "\n",
    "    print(f\"Scraping completed for {year_to_scrape}. Scoring data saved to {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3862f-969d-48e9-8c69-d0e23cb6b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add scoring tables (touchdown logs) to nfl.db\n",
    "# # Not in nfl.db currently\n",
    "# # Haven't tested\n",
    "\n",
    "# touchdown_logs_path = '/mnt/data/touchdown_logs.csv'\n",
    "# touchdown_logs_df = pd.read_csv(touchdown_logs_path)\n",
    "\n",
    "# # Updated mapping of full team names to their abbreviations\n",
    "# updated_team_name_mapping = {\n",
    "#     'Patriots': 'NE',\n",
    "#     'Steelers': 'PIT',\n",
    "#     'Bills': 'BUF',\n",
    "#     'Colts': 'IND',\n",
    "#     'Bears': 'CHI',\n",
    "#     'Packers': 'GB',\n",
    "#     'Chiefs': 'KC',\n",
    "#     'Texans': 'HOU',\n",
    "#     'Panthers': 'CAR',\n",
    "#     'Jaguars': 'JAX',\n",
    "#     'Browns': 'CLE',\n",
    "#     'Jets': 'NYJ',\n",
    "#     'Seahawks': 'SEA',\n",
    "#     'Rams': 'LAR',\n",
    "#     'Redskins': 'WAS',\n",
    "#     'Dolphins': 'MIA',\n",
    "#     'Cardinals': 'ARI',\n",
    "#     'Saints': 'NO',\n",
    "#     'Lions': 'DET',\n",
    "#     'Chargers': 'LAC',\n",
    "#     'Broncos': 'DEN',\n",
    "#     'Ravens': 'BAL',\n",
    "#     'Bengals': 'CIN',\n",
    "#     'Raiders': 'LVR',\n",
    "#     'Titans': 'TEN',\n",
    "#     'Buccaneers': 'TB',\n",
    "#     'Cowboys': 'DAL',\n",
    "#     'Giants': 'NYG',\n",
    "#     'Falcons': 'ATL',\n",
    "#     'Eagles': 'PHI',\n",
    "#     '49ers': 'SF',\n",
    "#     'Vikings': 'MIN',\n",
    "#     'Washington': 'WAS',\n",
    "#     'Football Team': 'WAS',\n",
    "#     'Commanders': 'WAS'\n",
    "# }\n",
    "\n",
    "# # Apply the updated mapping to the 'Team' column in touchdown_logs_df\n",
    "# touchdown_logs_df['Team'] = touchdown_logs_df['Team'].map(updated_team_name_mapping)\n",
    "\n",
    "# # Connect to the SQLite database\n",
    "# db_path = '/mnt/data/nfl_updated.db'\n",
    "# conn = sqlite3.connect(db_path)\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Create a new table for TouchdownLogs if it doesn't exist\n",
    "# cursor.execute('''\n",
    "#     CREATE TABLE IF NOT EXISTS TouchdownLogs (\n",
    "#         Quarter INTEGER,\n",
    "#         Time TEXT,\n",
    "#         Team TEXT,\n",
    "#         Detail TEXT,\n",
    "#         Team_1 INTEGER,\n",
    "#         Team_2 INTEGER,\n",
    "#         Game_ID TEXT\n",
    "#     );\n",
    "# ''')\n",
    "\n",
    "# # Insert cleaned data into the TouchdownLogs table, replacing any existing data\n",
    "# touchdown_logs_df.to_sql('TouchdownLogs', conn, if_exists='replace', index=False)\n",
    "\n",
    "# # Commit the changes and close the connection\n",
    "# conn.commit()\n",
    "# conn.close()\n",
    "\n",
    "# print(\"Touchdown logs data has been successfully cleaned and merged into the database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d0c66-6f5f-4a7e-88b8-39ff816e0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creating home_spread and away_spread columns in nfl.db --- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15efa9-8f8e-4311-91c4-0099a58d3125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # QB game tables\n",
    "# # Not in nfl.db currently\n",
    "\n",
    "# df = pd.read_csv('./data/rosters.csv')\n",
    "\n",
    "# # Drop rows where 'pfr_id' is missing\n",
    "# df = df.dropna(subset=['pfr_id'])\n",
    "\n",
    "# # Filter for Quarterbacks\n",
    "# qbs = df[df['position'] == 'QB']\n",
    "\n",
    "# # Open a CSV file to write the data\n",
    "# with open('./data/game_logs_qb.csv', 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     headers_written = False  # To track if headers have been written to the file\n",
    "\n",
    "#     # Initialize a counter for progress tracking\n",
    "#     total_qbs = len(qbs)\n",
    "#     qb_counter = 0\n",
    "\n",
    "#     # Iterate over each quarterback and scrape data\n",
    "#     for index, qb in qbs.iterrows():\n",
    "#         qb_counter += 1\n",
    "#         url = qb['url']\n",
    "#         print(f\"Processing QB {qb_counter}/{total_qbs}: {qb['first_name']} {qb['last_name']}\")\n",
    "#         first_name = qb['first_name']  # Get the player's first name\n",
    "#         last_name = qb['last_name']    # Get the player's last name\n",
    "#         position = 'QB'  # Assuming position is always QB as per your filter\n",
    "\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 200:\n",
    "#             soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#             game_logs_table = soup.find('table', {'id': 'passing'})  # Adjust this ID if needed\n",
    "\n",
    "#             if game_logs_table:  # Check if the table is found\n",
    "#                 header_row = game_logs_table.find('thead').find_all('tr')[-1]\n",
    "#                 data_rows = game_logs_table.find('tbody').find_all('tr')\n",
    "\n",
    "#                 if not headers_written:  # Write headers only once\n",
    "#                     headers = [header.text.strip() for header in header_row.find_all('th')]\n",
    "#                     # headers.extend(['Player URL', 'Position', 'First Name', 'Last Name'])  # Add new headers\n",
    "#                     # headers.extend(['Awards', 'Player URL', 'Position', 'First Name', 'Last Name'])  # Add new headers\n",
    "#                     headers.append('Player URL')  # Add only the Player URL column at the very end\n",
    "#                     writer.writerow(headers)\n",
    "#                     headers_written = True\n",
    "\n",
    "#                 for row in data_rows:\n",
    "#                     cells = row.find_all(['th', 'td'])\n",
    "#                     data = [cell.text.strip() for cell in cells]\n",
    "\n",
    "#                     # # Extract special characters for the 'Awards' column\n",
    "#                     # awards = ''\n",
    "#                     # if '*' in data[0]:\n",
    "#                     #     awards += 'Pro Bowl '\n",
    "#                     # if '+' in data[0]:\n",
    "#                     #     awards += 'All-Pro '\n",
    "#                     # awards = awards.strip()\n",
    "#                     # # Clean special characters from the 'Year' column (first column)\n",
    "#                     # data[0] = data[0].replace('*', '').replace('+', '')\n",
    "#                     # # Add the awards to the data\n",
    "#                     # data.append(awards)\n",
    "\n",
    "#                     # data.extend([url, position, first_name, last_name])  # Append additional data\n",
    "#                     data.append(url)  # Append only the Player URL to the end of the data\n",
    "#                     writer.writerow(data)\n",
    "#                 print(f\"Data written for {first_name} {last_name}\")\n",
    "\n",
    "#             else:\n",
    "#                 print(f\"No game logs table found for URL: {url}\")\n",
    "#         else:\n",
    "#             print(f\"Failed to retrieve URL: {url} with status code: {response.status_code}\")\n",
    "\n",
    "#         print(f'Processed URL: {url}')  # Print the URL being processed\n",
    "#         time.sleep(2)  # Add a 3-second delay after processing each URL\n",
    "\n",
    "# print('Data saved to game_logs_qb.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caf80ac5-9dc8-4256-929f-13d49398e23c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing QB 1/208: Aaron Rodgers\n",
      "Data written for Aaron Rodgers\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RodgAa00.htm\n",
      "Processing QB 2/208: Joseph Flacco\n",
      "Data written for Joseph Flacco\n",
      "Processed URL: https://www.pro-football-reference.com/players/F/FlacJo00.htm\n",
      "Processing QB 3/208: Joshua Johnson\n",
      "Data for Joshua Johnson in year 2021 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2021 already exists. Skipping...\n",
      "Data for Joshua Johnson in year  already exists. Skipping...\n",
      "Data written for Joshua Johnson\n",
      "Processed URL: https://www.pro-football-reference.com/players/J/JohnJo05.htm\n",
      "Processing QB 4/208: John Stafford\n",
      "Data written for John Stafford\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/StafMa00.htm\n",
      "Processing QB 5/208: Daniel McCoy\n",
      "Data written for Daniel McCoy\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/McCoCo00.htm\n",
      "Processing QB 6/208: Blaine Gabbert\n",
      "Data written for Blaine Gabbert\n",
      "Processed URL: https://www.pro-football-reference.com/players/G/GabbBl00.htm\n",
      "Processing QB 7/208: Andrew Dalton\n",
      "Data written for Andrew Dalton\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DaltAn00.htm\n",
      "Processing QB 8/208: Tyrod Taylor\n",
      "Data written for Tyrod Taylor\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TaylTy00.htm\n",
      "Processing QB 9/208: Russell Wilson\n",
      "Data written for Russell Wilson\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WilsRu00.htm\n",
      "Processing QB 10/208: Kirk Cousins\n",
      "Data written for Kirk Cousins\n",
      "Processed URL: https://www.pro-football-reference.com/players/C/CousKi00.htm\n",
      "Processing QB 11/208: Ryan Tannehill\n",
      "Data written for Ryan Tannehill\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TannRy00.htm\n",
      "Processing QB 12/208: Matthew Barkley\n",
      "Data for Matthew Barkley in year  already exists. Skipping...\n",
      "Data for Matthew Barkley in year  already exists. Skipping...\n",
      "Data written for Matthew Barkley\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BarkMa00.htm\n",
      "Processing QB 13/208: Eugene Smith\n",
      "Data written for Eugene Smith\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/SmitGe00.htm\n",
      "Processing QB 14/208: Theodore Bridgewater\n",
      "Data written for Theodore Bridgewater\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BridTe00.htm\n",
      "Processing QB 15/208: Derek Carr\n",
      "Data written for Derek Carr\n",
      "Processed URL: https://www.pro-football-reference.com/players/C/CarrDe02.htm\n",
      "Processing QB 16/208: Raymond McCarron\n",
      "Data written for Raymond McCarron\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/McCaA.00.htm\n",
      "Processing QB 17/208: James Garoppolo\n",
      "Data written for James Garoppolo\n",
      "Processed URL: https://www.pro-football-reference.com/players/G/GaroJi00.htm\n",
      "Processing QB 18/208: Jameis Winston\n",
      "Data written for Jameis Winston\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WinsJa00.htm\n",
      "Processing QB 19/208: Taylor Heinicke\n",
      "Data written for Taylor Heinicke\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HeinTa00.htm\n",
      "Processing QB 20/208: Trevor Siemian\n",
      "Data for Trevor Siemian in year  already exists. Skipping...\n",
      "Data written for Trevor Siemian\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/SiemTr00.htm\n",
      "Processing QB 21/208: Sean Mannion\n",
      "Data written for Sean Mannion\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MannSe00.htm\n",
      "Processing QB 22/208: Marcus Mariota\n",
      "Data written for Marcus Mariota\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MariMa01.htm\n",
      "Processing QB 23/208: Brandon Allen\n",
      "Data written for Brandon Allen\n",
      "Processed URL: https://www.pro-football-reference.com/players/A/AlleBr00.htm\n",
      "Processing QB 24/208: Jeffrey Driskel\n",
      "Data written for Jeffrey Driskel\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DrisJe00.htm\n",
      "Processing QB 25/208: Nathan Sudfeld\n",
      "Data for Nathan Sudfeld in year  already exists. Skipping...\n",
      "Data written for Nathan Sudfeld\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/SudfNa00.htm\n",
      "Processing QB 26/208: Carson Wentz\n",
      "Data written for Carson Wentz\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WentCa00.htm\n",
      "Processing QB 27/208: Rayne Prescott\n",
      "Data written for Rayne Prescott\n",
      "Processed URL: https://www.pro-football-reference.com/players/P/PresDa01.htm\n",
      "Processing QB 28/208: Jared Goff\n",
      "Data written for Jared Goff\n",
      "Processed URL: https://www.pro-football-reference.com/players/G/GoffJa00.htm\n",
      "Processing QB 29/208: Jacoby Brissett\n",
      "Data written for Jacoby Brissett\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BrisJa00.htm\n",
      "Processing QB 30/208: Phillip Walker\n",
      "Data written for Phillip Walker\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WalkPh00.htm\n",
      "Processing QB 31/208: Nicholas Mullens\n",
      "Data written for Nicholas Mullens\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MullNi00.htm\n",
      "Processing QB 32/208: Taysom Hill\n",
      "Data written for Taysom Hill\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HillTa00.htm\n",
      "Processing QB 33/208: Derrick Watson\n",
      "Data written for Derrick Watson\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WatsDe00.htm\n",
      "Processing QB 34/208: Cooper Rush\n",
      "Data written for Cooper Rush\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RushCo00.htm\n",
      "Processing QB 35/208: Mitchell Trubisky\n",
      "Data written for Mitchell Trubisky\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TrubMi00.htm\n",
      "Processing QB 36/208: Patrick Mahomes\n",
      "Data written for Patrick Mahomes\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MahoPa00.htm\n",
      "Processing QB 37/208: Casey Beathard\n",
      "Data written for Casey Beathard\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BeatC.00.htm\n",
      "Processing QB 38/208: Robert Dobbs\n",
      "Data for Robert Dobbs in year  already exists. Skipping...\n",
      "Data for Robert Dobbs in year 2023 already exists. Skipping...\n",
      "Data for Robert Dobbs in year 2023 already exists. Skipping...\n",
      "Data written for Robert Dobbs\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DobbJo00.htm\n",
      "Processing QB 39/208: Nathan Peterman\n",
      "Data written for Nathan Peterman\n",
      "Processed URL: https://www.pro-football-reference.com/players/P/PeteNa00.htm\n",
      "Processing QB 40/208: Michael White\n",
      "Data written for Michael White\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WhitMi01.htm\n",
      "Processing QB 41/208: Logan Woodside\n",
      "Data written for Logan Woodside\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WoodLo00.htm\n",
      "Processing QB 42/208: Kyle Allen\n",
      "Data written for Kyle Allen\n",
      "Processed URL: https://www.pro-football-reference.com/players/A/AlleKy00.htm\n",
      "Processing QB 43/208: Alexander McGough\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/McGoAl00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/McGoAl00.htm\n",
      "Processing QB 44/208: Mason Rudolph\n",
      "Data written for Mason Rudolph\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RudoMa00.htm\n",
      "Processing QB 45/208: Lamar Jackson\n",
      "Data written for Lamar Jackson\n",
      "Processed URL: https://www.pro-football-reference.com/players/J/JackLa00.htm\n",
      "Processing QB 46/208: Baker Mayfield\n",
      "Data for Baker Mayfield in year 2022 already exists. Skipping...\n",
      "Data for Baker Mayfield in year 2022 already exists. Skipping...\n",
      "Data written for Baker Mayfield\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MayfBa00.htm\n",
      "Processing QB 47/208: Joshua Allen\n",
      "Data written for Joshua Allen\n",
      "Processed URL: https://www.pro-football-reference.com/players/A/AlleJo02.htm\n",
      "Processing QB 48/208: Sam Darnold\n",
      "Data written for Sam Darnold\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DarnSa00.htm\n",
      "Processing QB 49/208: David Blough\n",
      "Data written for David Blough\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BlouDa00.htm\n",
      "Processing QB 50/208: Trace McSorley\n",
      "Data written for Trace McSorley\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/McSoTr00.htm\n",
      "Processing QB 51/208: Kyler Murray\n",
      "Data written for Kyler Murray\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MurrKy00.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing QB 52/208: Will Grier\n",
      "Data written for Will Grier\n",
      "Processed URL: https://www.pro-football-reference.com/players/G/GrieWi00.htm\n",
      "Processing QB 53/208: Jarrett Stidham\n",
      "Data written for Jarrett Stidham\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/StidJa00.htm\n",
      "Processing QB 54/208: Easton Stick\n",
      "Data written for Easton Stick\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/SticEa00.htm\n",
      "Processing QB 55/208: Gardner Minshew\n",
      "Data written for Gardner Minshew\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MinsGa00.htm\n",
      "Processing QB 56/208: Drew Lock\n",
      "Data written for Drew Lock\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LockDr00.htm\n",
      "Processing QB 57/208: Daniel Jones\n",
      "Data written for Daniel Jones\n",
      "Processed URL: https://www.pro-football-reference.com/players/J/JoneDa05.htm\n",
      "Processing QB 58/208: Tyler Huntley\n",
      "Data written for Tyler Huntley\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HuntTy01.htm\n",
      "Processing QB 59/208: William Fromm\n",
      "Data written for William Fromm\n",
      "Processed URL: https://www.pro-football-reference.com/players/F/FromJa00.htm\n",
      "Processing QB 60/208: Tua Tagovailoa\n",
      "Data written for Tua Tagovailoa\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TagoTu00.htm\n",
      "Processing QB 61/208: Jacob Eason\n",
      "Data written for Jacob Eason\n",
      "Processed URL: https://www.pro-football-reference.com/players/E/EasoJa00.htm\n",
      "Processing QB 62/208: Jordan Love\n",
      "Data written for Jordan Love\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LoveJo03.htm\n",
      "Processing QB 63/208: Jake Luton\n",
      "Data written for Jake Luton\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LutoJa00.htm\n",
      "Processing QB 64/208: Justin Herbert\n",
      "Data written for Justin Herbert\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HerbJu00.htm\n",
      "Processing QB 65/208: Benjamin DiNucci\n",
      "Data written for Benjamin DiNucci\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DiNuBe00.htm\n",
      "Processing QB 66/208: Jalen Hurts\n",
      "Data written for Jalen Hurts\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HurtJa00.htm\n",
      "Processing QB 67/208: Joe Burrow\n",
      "Data written for Joe Burrow\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BurrJo01.htm\n",
      "Processing QB 68/208: Samuel Ehlinger\n",
      "Data written for Samuel Ehlinger\n",
      "Processed URL: https://www.pro-football-reference.com/players/E/EhliSa00.htm\n",
      "Processing QB 69/208: Davis Mills\n",
      "Data written for Davis Mills\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MillDa02.htm\n",
      "Processing QB 70/208: Kyle Trask\n",
      "Data written for Kyle Trask\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TrasKy00.htm\n",
      "Processing QB 71/208: Ian Book\n",
      "Data written for Ian Book\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BookIa00.htm\n",
      "Processing QB 72/208: Justin Fields\n",
      "Data written for Justin Fields\n",
      "Processed URL: https://www.pro-football-reference.com/players/F/FielJu00.htm\n",
      "Processing QB 73/208: Kellen Mond\n",
      "Data written for Kellen Mond\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MondKe00.htm\n",
      "Processing QB 74/208: Trevor Lawrence\n",
      "Data written for Trevor Lawrence\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LawrTr00.htm\n",
      "Processing QB 75/208: Michael Jones\n",
      "Data written for Michael Jones\n",
      "Processed URL: https://www.pro-football-reference.com/players/J/JoneMa05.htm\n",
      "Processing QB 76/208: Trey Lance\n",
      "Data written for Trey Lance\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LancTr00.htm\n",
      "Processing QB 77/208: Zachary Wilson\n",
      "Data written for Zachary Wilson\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WilsZa00.htm\n",
      "Processing QB 78/208: Sam Howell\n",
      "Data written for Sam Howell\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HoweSa00.htm\n",
      "Processing QB 79/208: Anthony Brown\n",
      "Data written for Anthony Brown\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BrowAn06.htm\n",
      "Processing QB 80/208: Chris Oladokun\n",
      "Data written for Chris Oladokun\n",
      "Processed URL: https://www.pro-football-reference.com/players/O/OladCh00.htm\n",
      "Processing QB 81/208: Skylar Thompson\n",
      "Data written for Skylar Thompson\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/ThomSk00.htm\n",
      "Processing QB 82/208: Brock Purdy\n",
      "Data written for Brock Purdy\n",
      "Processed URL: https://www.pro-football-reference.com/players/P/PurdBr00.htm\n",
      "Processing QB 83/208: Kenny Pickett\n",
      "Data written for Kenny Pickett\n",
      "Processed URL: https://www.pro-football-reference.com/players/P/PickKe00.htm\n",
      "Processing QB 84/208: Bailey Zappe\n",
      "Data written for Bailey Zappe\n",
      "Processed URL: https://www.pro-football-reference.com/players/Z/ZappBa00.htm\n",
      "Processing QB 85/208: Desmond Ridder\n",
      "Data written for Desmond Ridder\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RiddDe00.htm\n",
      "Processing QB 86/208: Malik Willis\n",
      "Data written for Malik Willis\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WillMa12.htm\n",
      "Processing QB 87/208: Matt Corral\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/C/CorrMa00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/C/CorrMa00.htm\n",
      "Processing QB 88/208: Sean Clifford\n",
      "Data written for Sean Clifford\n",
      "Processed URL: https://www.pro-football-reference.com/players/C/ClifSe00.htm\n",
      "Processing QB 89/208: Tanner McKee\n",
      "Data written for Tanner McKee\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/McKeTa01.htm\n",
      "Processing QB 90/208: Tyson Bagent\n",
      "Data written for Tyson Bagent\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BageTy00.htm\n",
      "Processing QB 91/208: Tommy DeVito\n",
      "Data written for Tommy DeVito\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DeViTo00.htm\n",
      "Processing QB 92/208: Hendon Hooker\n",
      "Data written for Hendon Hooker\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HookHe00.htm\n",
      "Processing QB 93/208: Aidan O'Connell\n",
      "Data written for Aidan O'Connell\n",
      "Processed URL: https://www.pro-football-reference.com/players/O/OConAi00.htm\n",
      "Processing QB 94/208: Clayton Tune\n",
      "Data written for Clayton Tune\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TuneCl00.htm\n",
      "Processing QB 95/208: Dorian Thompson-Robinson\n",
      "Data written for Dorian Thompson-Robinson\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/ThomDo02.htm\n",
      "Processing QB 96/208: Jaren Hall\n",
      "Data written for Jaren Hall\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HallJa00.htm\n",
      "Processing QB 97/208: Max Duggan\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/D/DuggMa00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DuggMa00.htm\n",
      "Processing QB 98/208: Malik Cunningham\n",
      "Data for Malik Cunningham in year 2023 already exists. Skipping...\n",
      "Data for Malik Cunningham in year 2023 already exists. Skipping...\n",
      "Data written for Malik Cunningham\n",
      "Processed URL: https://www.pro-football-reference.com/players/C/CunnMa00.htm\n",
      "Processing QB 99/208: Jake Haener\n",
      "Data written for Jake Haener\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HaenJa00.htm\n",
      "Processing QB 100/208: Stetson Bennett\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/BennSt00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BennSt00.htm\n",
      "Processing QB 101/208: Bryce Young\n",
      "Data written for Bryce Young\n",
      "Processed URL: https://www.pro-football-reference.com/players/Y/YounBr01.htm\n",
      "Processing QB 102/208: Will Levis\n",
      "Data written for Will Levis\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LeviWi00.htm\n",
      "Processing QB 103/208: Coleridge Stroud\n",
      "Data written for Coleridge Stroud\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/StroCJ00.htm\n",
      "Processing QB 104/208: Anthony Richardson\n",
      "Data written for Anthony Richardson\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RichAn03.htm\n",
      "Processing QB 105/208: Aaron Rodgers\n",
      "Data for Aaron Rodgers in year 2005 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2006 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2007 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2008 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2009 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2010 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2011 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2012 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2013 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2014 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2015 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2016 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2017 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2018 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2019 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2020 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2021 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2022 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2023 already exists. Skipping...\n",
      "Data for Aaron Rodgers in year 2024 already exists. Skipping...\n",
      "Data written for Aaron Rodgers\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RodgAa00.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing QB 106/208: Matthew Ryan\n",
      "Data written for Matthew Ryan\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RyanMa00.htm\n",
      "Processing QB 107/208: Joseph Flacco\n",
      "Data for Joseph Flacco in year 2008 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2009 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2010 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2011 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2012 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2013 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2014 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2015 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2016 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2017 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2018 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2019 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2020 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2021 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2022 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2023 already exists. Skipping...\n",
      "Data for Joseph Flacco in year 2024 already exists. Skipping...\n",
      "Data written for Joseph Flacco\n",
      "Processed URL: https://www.pro-football-reference.com/players/F/FlacJo00.htm\n",
      "Processing QB 108/208: Joshua Johnson\n",
      "Data for Joshua Johnson in year 2009 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2010 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2011 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2012 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2013 already exists. Skipping...\n",
      "Data for Joshua Johnson in year  already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2018 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2019 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2020 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2021 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2021 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2021 already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2022 already exists. Skipping...\n",
      "Data for Joshua Johnson in year  already exists. Skipping...\n",
      "Data for Joshua Johnson in year 2024 already exists. Skipping...\n",
      "Data written for Joshua Johnson\n",
      "Processed URL: https://www.pro-football-reference.com/players/J/JohnJo05.htm\n",
      "Processing QB 109/208: John Stafford\n",
      "Data for John Stafford in year 2009 already exists. Skipping...\n",
      "Data for John Stafford in year 2010 already exists. Skipping...\n",
      "Data for John Stafford in year 2011 already exists. Skipping...\n",
      "Data for John Stafford in year 2012 already exists. Skipping...\n",
      "Data for John Stafford in year 2013 already exists. Skipping...\n",
      "Data for John Stafford in year 2014 already exists. Skipping...\n",
      "Data for John Stafford in year 2015 already exists. Skipping...\n",
      "Data for John Stafford in year 2016 already exists. Skipping...\n",
      "Data for John Stafford in year 2017 already exists. Skipping...\n",
      "Data for John Stafford in year 2018 already exists. Skipping...\n",
      "Data for John Stafford in year 2019 already exists. Skipping...\n",
      "Data for John Stafford in year 2020 already exists. Skipping...\n",
      "Data for John Stafford in year 2021 already exists. Skipping...\n",
      "Data for John Stafford in year 2022 already exists. Skipping...\n",
      "Data for John Stafford in year 2023 already exists. Skipping...\n",
      "Data for John Stafford in year 2024 already exists. Skipping...\n",
      "Data written for John Stafford\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/StafMa00.htm\n",
      "Processing QB 110/208: Andrew Dalton\n",
      "Data for Andrew Dalton in year 2011 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2012 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2013 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2014 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2015 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2016 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2017 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2018 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2019 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2020 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2021 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2022 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2023 already exists. Skipping...\n",
      "Data for Andrew Dalton in year 2024 already exists. Skipping...\n",
      "Data written for Andrew Dalton\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DaltAn00.htm\n",
      "Processing QB 111/208: Tyrod Taylor\n",
      "Data for Tyrod Taylor in year 2011 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2012 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2013 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2014 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2015 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2016 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2017 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2018 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2019 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2020 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2021 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2022 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2023 already exists. Skipping...\n",
      "Data for Tyrod Taylor in year 2024 already exists. Skipping...\n",
      "Data written for Tyrod Taylor\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TaylTy00.htm\n",
      "Processing QB 112/208: Russell Wilson\n",
      "Data for Russell Wilson in year 2012 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2013 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2014 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2015 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2016 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2017 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2018 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2019 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2020 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2021 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2022 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2023 already exists. Skipping...\n",
      "Data for Russell Wilson in year 2024 already exists. Skipping...\n",
      "Data written for Russell Wilson\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WilsRu00.htm\n",
      "Processing QB 113/208: Kirk Cousins\n",
      "Data for Kirk Cousins in year 2012 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2013 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2014 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2015 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2016 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2017 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2018 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2019 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2020 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2021 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2022 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2023 already exists. Skipping...\n",
      "Data for Kirk Cousins in year 2024 already exists. Skipping...\n",
      "Data written for Kirk Cousins\n",
      "Processed URL: https://www.pro-football-reference.com/players/C/CousKi00.htm\n",
      "Processing QB 114/208: Eugene Smith\n",
      "Data for Eugene Smith in year 2013 already exists. Skipping...\n",
      "Data for Eugene Smith in year 2014 already exists. Skipping...\n",
      "Data for Eugene Smith in year 2015 already exists. Skipping...\n",
      "Data for Eugene Smith in year 2016 already exists. Skipping...\n",
      "Data for Eugene Smith in year 2017 already exists. Skipping...\n",
      "Data for Eugene Smith in year 2018 already exists. Skipping...\n",
      "Data for Eugene Smith in year  already exists. Skipping...\n",
      "Data for Eugene Smith in year 2020 already exists. Skipping...\n",
      "Data for Eugene Smith in year 2021 already exists. Skipping...\n",
      "Data for Eugene Smith in year 2022 already exists. Skipping...\n",
      "Data for Eugene Smith in year 2023 already exists. Skipping...\n",
      "Data for Eugene Smith in year 2024 already exists. Skipping...\n",
      "Data written for Eugene Smith\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/SmitGe00.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing QB 115/208: Theodore Bridgewater\n",
      "Data for Theodore Bridgewater in year 2014 already exists. Skipping...\n",
      "Data for Theodore Bridgewater in year 2015 already exists. Skipping...\n",
      "Data for Theodore Bridgewater in year  already exists. Skipping...\n",
      "Data for Theodore Bridgewater in year 2017 already exists. Skipping...\n",
      "Data for Theodore Bridgewater in year 2018 already exists. Skipping...\n",
      "Data for Theodore Bridgewater in year 2019 already exists. Skipping...\n",
      "Data for Theodore Bridgewater in year 2020 already exists. Skipping...\n",
      "Data for Theodore Bridgewater in year 2021 already exists. Skipping...\n",
      "Data for Theodore Bridgewater in year 2022 already exists. Skipping...\n",
      "Data for Theodore Bridgewater in year 2023 already exists. Skipping...\n",
      "Data for Theodore Bridgewater in year 2024 already exists. Skipping...\n",
      "Data written for Theodore Bridgewater\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BridTe00.htm\n",
      "Processing QB 116/208: Derek Carr\n",
      "Data for Derek Carr in year 2014 already exists. Skipping...\n",
      "Data for Derek Carr in year 2015 already exists. Skipping...\n",
      "Data for Derek Carr in year 2016 already exists. Skipping...\n",
      "Data for Derek Carr in year 2017 already exists. Skipping...\n",
      "Data for Derek Carr in year 2018 already exists. Skipping...\n",
      "Data for Derek Carr in year 2019 already exists. Skipping...\n",
      "Data for Derek Carr in year 2020 already exists. Skipping...\n",
      "Data for Derek Carr in year 2021 already exists. Skipping...\n",
      "Data for Derek Carr in year 2022 already exists. Skipping...\n",
      "Data for Derek Carr in year 2023 already exists. Skipping...\n",
      "Data for Derek Carr in year 2024 already exists. Skipping...\n",
      "Data written for Derek Carr\n",
      "Processed URL: https://www.pro-football-reference.com/players/C/CarrDe02.htm\n",
      "Processing QB 117/208: James Garoppolo\n",
      "Data for James Garoppolo in year 2014 already exists. Skipping...\n",
      "Data for James Garoppolo in year 2015 already exists. Skipping...\n",
      "Data for James Garoppolo in year 2016 already exists. Skipping...\n",
      "Data for James Garoppolo in year 2017 already exists. Skipping...\n",
      "Data for James Garoppolo in year 2018 already exists. Skipping...\n",
      "Data for James Garoppolo in year 2019 already exists. Skipping...\n",
      "Data for James Garoppolo in year 2020 already exists. Skipping...\n",
      "Data for James Garoppolo in year 2021 already exists. Skipping...\n",
      "Data for James Garoppolo in year 2022 already exists. Skipping...\n",
      "Data for James Garoppolo in year 2023 already exists. Skipping...\n",
      "Data for James Garoppolo in year 2024 already exists. Skipping...\n",
      "Data written for James Garoppolo\n",
      "Processed URL: https://www.pro-football-reference.com/players/G/GaroJi00.htm\n",
      "Processing QB 118/208: Jameis Winston\n",
      "Data for Jameis Winston in year 2015 already exists. Skipping...\n",
      "Data for Jameis Winston in year 2016 already exists. Skipping...\n",
      "Data for Jameis Winston in year 2017 already exists. Skipping...\n",
      "Data for Jameis Winston in year 2018 already exists. Skipping...\n",
      "Data for Jameis Winston in year 2019 already exists. Skipping...\n",
      "Data for Jameis Winston in year 2020 already exists. Skipping...\n",
      "Data for Jameis Winston in year 2021 already exists. Skipping...\n",
      "Data for Jameis Winston in year 2022 already exists. Skipping...\n",
      "Data for Jameis Winston in year 2023 already exists. Skipping...\n",
      "Data for Jameis Winston in year 2024 already exists. Skipping...\n",
      "Data written for Jameis Winston\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WinsJa00.htm\n",
      "Processing QB 119/208: Taylor Heinicke\n",
      "Data for Taylor Heinicke in year 2017 already exists. Skipping...\n",
      "Data for Taylor Heinicke in year 2018 already exists. Skipping...\n",
      "Data for Taylor Heinicke in year  already exists. Skipping...\n",
      "Data for Taylor Heinicke in year 2020 already exists. Skipping...\n",
      "Data for Taylor Heinicke in year 2021 already exists. Skipping...\n",
      "Data for Taylor Heinicke in year 2022 already exists. Skipping...\n",
      "Data for Taylor Heinicke in year 2023 already exists. Skipping...\n",
      "Data for Taylor Heinicke in year 2024 already exists. Skipping...\n",
      "Data written for Taylor Heinicke\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HeinTa00.htm\n",
      "Processing QB 120/208: Trevor Siemian\n",
      "Data for Trevor Siemian in year 2015 already exists. Skipping...\n",
      "Data for Trevor Siemian in year 2016 already exists. Skipping...\n",
      "Data for Trevor Siemian in year 2017 already exists. Skipping...\n",
      "Data for Trevor Siemian in year  already exists. Skipping...\n",
      "Data for Trevor Siemian in year 2019 already exists. Skipping...\n",
      "Data for Trevor Siemian in year  already exists. Skipping...\n",
      "Data for Trevor Siemian in year 2021 already exists. Skipping...\n",
      "Data for Trevor Siemian in year 2022 already exists. Skipping...\n",
      "Data for Trevor Siemian in year 2023 already exists. Skipping...\n",
      "Data written for Trevor Siemian\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/SiemTr00.htm\n",
      "Processing QB 121/208: Marcus Mariota\n",
      "Data for Marcus Mariota in year 2015 already exists. Skipping...\n",
      "Data for Marcus Mariota in year 2016 already exists. Skipping...\n",
      "Data for Marcus Mariota in year 2017 already exists. Skipping...\n",
      "Data for Marcus Mariota in year 2018 already exists. Skipping...\n",
      "Data for Marcus Mariota in year 2019 already exists. Skipping...\n",
      "Data for Marcus Mariota in year 2020 already exists. Skipping...\n",
      "Data for Marcus Mariota in year 2021 already exists. Skipping...\n",
      "Data for Marcus Mariota in year 2022 already exists. Skipping...\n",
      "Data for Marcus Mariota in year 2023 already exists. Skipping...\n",
      "Data for Marcus Mariota in year 2024 already exists. Skipping...\n",
      "Data written for Marcus Mariota\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MariMa01.htm\n",
      "Processing QB 122/208: Brandon Allen\n",
      "Data for Brandon Allen in year 2019 already exists. Skipping...\n",
      "Data for Brandon Allen in year 2020 already exists. Skipping...\n",
      "Data for Brandon Allen in year 2021 already exists. Skipping...\n",
      "Data for Brandon Allen in year 2022 already exists. Skipping...\n",
      "Data for Brandon Allen in year  already exists. Skipping...\n",
      "Data for Brandon Allen in year 2024 already exists. Skipping...\n",
      "Data written for Brandon Allen\n",
      "Processed URL: https://www.pro-football-reference.com/players/A/AlleBr00.htm\n",
      "Processing QB 123/208: Jeffrey Driskel\n",
      "Data for Jeffrey Driskel in year 2018 already exists. Skipping...\n",
      "Data for Jeffrey Driskel in year 2019 already exists. Skipping...\n",
      "Data for Jeffrey Driskel in year 2020 already exists. Skipping...\n",
      "Data for Jeffrey Driskel in year 2021 already exists. Skipping...\n",
      "Data for Jeffrey Driskel in year 2022 already exists. Skipping...\n",
      "Data for Jeffrey Driskel in year 2023 already exists. Skipping...\n",
      "Data for Jeffrey Driskel in year 2024 already exists. Skipping...\n",
      "Data written for Jeffrey Driskel\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DrisJe00.htm\n",
      "Processing QB 124/208: Nathan Sudfeld\n",
      "Data for Nathan Sudfeld in year 2017 already exists. Skipping...\n",
      "Data for Nathan Sudfeld in year 2018 already exists. Skipping...\n",
      "Data for Nathan Sudfeld in year  already exists. Skipping...\n",
      "Data for Nathan Sudfeld in year 2020 already exists. Skipping...\n",
      "Data for Nathan Sudfeld in year  already exists. Skipping...\n",
      "Data for Nathan Sudfeld in year 2022 already exists. Skipping...\n",
      "Data written for Nathan Sudfeld\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/SudfNa00.htm\n",
      "Processing QB 125/208: Carson Wentz\n",
      "Data for Carson Wentz in year 2016 already exists. Skipping...\n",
      "Data for Carson Wentz in year 2017 already exists. Skipping...\n",
      "Data for Carson Wentz in year 2018 already exists. Skipping...\n",
      "Data for Carson Wentz in year 2019 already exists. Skipping...\n",
      "Data for Carson Wentz in year 2020 already exists. Skipping...\n",
      "Data for Carson Wentz in year 2021 already exists. Skipping...\n",
      "Data for Carson Wentz in year 2022 already exists. Skipping...\n",
      "Data for Carson Wentz in year 2023 already exists. Skipping...\n",
      "Data for Carson Wentz in year 2024 already exists. Skipping...\n",
      "Data written for Carson Wentz\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WentCa00.htm\n",
      "Processing QB 126/208: Rayne Prescott\n",
      "Data for Rayne Prescott in year 2016 already exists. Skipping...\n",
      "Data for Rayne Prescott in year 2017 already exists. Skipping...\n",
      "Data for Rayne Prescott in year 2018 already exists. Skipping...\n",
      "Data for Rayne Prescott in year 2019 already exists. Skipping...\n",
      "Data for Rayne Prescott in year 2020 already exists. Skipping...\n",
      "Data for Rayne Prescott in year 2021 already exists. Skipping...\n",
      "Data for Rayne Prescott in year 2022 already exists. Skipping...\n",
      "Data for Rayne Prescott in year 2023 already exists. Skipping...\n",
      "Data for Rayne Prescott in year 2024 already exists. Skipping...\n",
      "Data written for Rayne Prescott\n",
      "Processed URL: https://www.pro-football-reference.com/players/P/PresDa01.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing QB 127/208: Jared Goff\n",
      "Data for Jared Goff in year 2016 already exists. Skipping...\n",
      "Data for Jared Goff in year 2017 already exists. Skipping...\n",
      "Data for Jared Goff in year 2018 already exists. Skipping...\n",
      "Data for Jared Goff in year 2019 already exists. Skipping...\n",
      "Data for Jared Goff in year 2020 already exists. Skipping...\n",
      "Data for Jared Goff in year 2021 already exists. Skipping...\n",
      "Data for Jared Goff in year 2022 already exists. Skipping...\n",
      "Data for Jared Goff in year 2023 already exists. Skipping...\n",
      "Data for Jared Goff in year 2024 already exists. Skipping...\n",
      "Data written for Jared Goff\n",
      "Processed URL: https://www.pro-football-reference.com/players/G/GoffJa00.htm\n",
      "Processing QB 128/208: Jacoby Brissett\n",
      "Data for Jacoby Brissett in year 2016 already exists. Skipping...\n",
      "Data for Jacoby Brissett in year 2017 already exists. Skipping...\n",
      "Data for Jacoby Brissett in year 2018 already exists. Skipping...\n",
      "Data for Jacoby Brissett in year 2019 already exists. Skipping...\n",
      "Data for Jacoby Brissett in year 2020 already exists. Skipping...\n",
      "Data for Jacoby Brissett in year 2021 already exists. Skipping...\n",
      "Data for Jacoby Brissett in year 2022 already exists. Skipping...\n",
      "Data for Jacoby Brissett in year 2023 already exists. Skipping...\n",
      "Data for Jacoby Brissett in year 2024 already exists. Skipping...\n",
      "Data written for Jacoby Brissett\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BrisJa00.htm\n",
      "Processing QB 129/208: Phillip Walker\n",
      "Data for Phillip Walker in year 2020 already exists. Skipping...\n",
      "Data for Phillip Walker in year 2021 already exists. Skipping...\n",
      "Data for Phillip Walker in year 2022 already exists. Skipping...\n",
      "Data for Phillip Walker in year 2023 already exists. Skipping...\n",
      "Data written for Phillip Walker\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WalkPh00.htm\n",
      "Processing QB 130/208: Nicholas Mullens\n",
      "Data for Nicholas Mullens in year 2018 already exists. Skipping...\n",
      "Data for Nicholas Mullens in year 2019 already exists. Skipping...\n",
      "Data for Nicholas Mullens in year 2020 already exists. Skipping...\n",
      "Data for Nicholas Mullens in year 2021 already exists. Skipping...\n",
      "Data for Nicholas Mullens in year 2022 already exists. Skipping...\n",
      "Data for Nicholas Mullens in year 2023 already exists. Skipping...\n",
      "Data for Nicholas Mullens in year 2024 already exists. Skipping...\n",
      "Data written for Nicholas Mullens\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MullNi00.htm\n",
      "Processing QB 131/208: Derrick Watson\n",
      "Data for Derrick Watson in year 2017 already exists. Skipping...\n",
      "Data for Derrick Watson in year 2018 already exists. Skipping...\n",
      "Data for Derrick Watson in year 2019 already exists. Skipping...\n",
      "Data for Derrick Watson in year 2020 already exists. Skipping...\n",
      "Data for Derrick Watson in year  already exists. Skipping...\n",
      "Data for Derrick Watson in year 2022 already exists. Skipping...\n",
      "Data for Derrick Watson in year 2023 already exists. Skipping...\n",
      "Data for Derrick Watson in year 2024 already exists. Skipping...\n",
      "Data written for Derrick Watson\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WatsDe00.htm\n",
      "Processing QB 132/208: Cooper Rush\n",
      "Data for Cooper Rush in year 2017 already exists. Skipping...\n",
      "Data for Cooper Rush in year 2018 already exists. Skipping...\n",
      "Data for Cooper Rush in year 2019 already exists. Skipping...\n",
      "Data for Cooper Rush in year  already exists. Skipping...\n",
      "Data for Cooper Rush in year 2021 already exists. Skipping...\n",
      "Data for Cooper Rush in year 2022 already exists. Skipping...\n",
      "Data for Cooper Rush in year 2023 already exists. Skipping...\n",
      "Data for Cooper Rush in year 2024 already exists. Skipping...\n",
      "Data written for Cooper Rush\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RushCo00.htm\n",
      "Processing QB 133/208: Mitchell Trubisky\n",
      "Data for Mitchell Trubisky in year 2017 already exists. Skipping...\n",
      "Data for Mitchell Trubisky in year 2018 already exists. Skipping...\n",
      "Data for Mitchell Trubisky in year 2019 already exists. Skipping...\n",
      "Data for Mitchell Trubisky in year 2020 already exists. Skipping...\n",
      "Data for Mitchell Trubisky in year 2021 already exists. Skipping...\n",
      "Data for Mitchell Trubisky in year 2022 already exists. Skipping...\n",
      "Data for Mitchell Trubisky in year 2023 already exists. Skipping...\n",
      "Data for Mitchell Trubisky in year 2024 already exists. Skipping...\n",
      "Data written for Mitchell Trubisky\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TrubMi00.htm\n",
      "Processing QB 134/208: Patrick Mahomes\n",
      "Data for Patrick Mahomes in year 2017 already exists. Skipping...\n",
      "Data for Patrick Mahomes in year 2018 already exists. Skipping...\n",
      "Data for Patrick Mahomes in year 2019 already exists. Skipping...\n",
      "Data for Patrick Mahomes in year 2020 already exists. Skipping...\n",
      "Data for Patrick Mahomes in year 2021 already exists. Skipping...\n",
      "Data for Patrick Mahomes in year 2022 already exists. Skipping...\n",
      "Data for Patrick Mahomes in year 2023 already exists. Skipping...\n",
      "Data for Patrick Mahomes in year 2024 already exists. Skipping...\n",
      "Data written for Patrick Mahomes\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MahoPa00.htm\n",
      "Processing QB 135/208: Casey Beathard\n",
      "Data for Casey Beathard in year 2017 already exists. Skipping...\n",
      "Data for Casey Beathard in year 2018 already exists. Skipping...\n",
      "Data for Casey Beathard in year  already exists. Skipping...\n",
      "Data for Casey Beathard in year 2020 already exists. Skipping...\n",
      "Data for Casey Beathard in year 2021 already exists. Skipping...\n",
      "Data for Casey Beathard in year 2022 already exists. Skipping...\n",
      "Data for Casey Beathard in year 2023 already exists. Skipping...\n",
      "Data written for Casey Beathard\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BeatC.00.htm\n",
      "Processing QB 136/208: Robert Dobbs\n",
      "Data for Robert Dobbs in year 2018 already exists. Skipping...\n",
      "Data for Robert Dobbs in year  already exists. Skipping...\n",
      "Data for Robert Dobbs in year 2020 already exists. Skipping...\n",
      "Data for Robert Dobbs in year  already exists. Skipping...\n",
      "Data for Robert Dobbs in year 2022 already exists. Skipping...\n",
      "Data for Robert Dobbs in year 2023 already exists. Skipping...\n",
      "Data for Robert Dobbs in year 2023 already exists. Skipping...\n",
      "Data for Robert Dobbs in year 2023 already exists. Skipping...\n",
      "Data for Robert Dobbs in year 2024 already exists. Skipping...\n",
      "Data written for Robert Dobbs\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DobbJo00.htm\n",
      "Processing QB 137/208: Nathan Peterman\n",
      "Data for Nathan Peterman in year 2017 already exists. Skipping...\n",
      "Data for Nathan Peterman in year 2018 already exists. Skipping...\n",
      "Data for Nathan Peterman in year  already exists. Skipping...\n",
      "Data for Nathan Peterman in year 2020 already exists. Skipping...\n",
      "Data for Nathan Peterman in year 2021 already exists. Skipping...\n",
      "Data for Nathan Peterman in year 2022 already exists. Skipping...\n",
      "Data for Nathan Peterman in year 2023 already exists. Skipping...\n",
      "Data written for Nathan Peterman\n",
      "Processed URL: https://www.pro-football-reference.com/players/P/PeteNa00.htm\n",
      "Processing QB 138/208: Michael White\n",
      "Data for Michael White in year 2021 already exists. Skipping...\n",
      "Data for Michael White in year 2022 already exists. Skipping...\n",
      "Data for Michael White in year 2023 already exists. Skipping...\n",
      "Data for Michael White in year 2024 already exists. Skipping...\n",
      "Data written for Michael White\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WhitMi01.htm\n",
      "Processing QB 139/208: Logan Woodside\n",
      "Data for Logan Woodside in year 2020 already exists. Skipping...\n",
      "Data for Logan Woodside in year  already exists. Skipping...\n",
      "Data for Logan Woodside in year 2023 already exists. Skipping...\n",
      "Data written for Logan Woodside\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WoodLo00.htm\n",
      "Processing QB 140/208: Kyle Allen\n",
      "Data for Kyle Allen in year 2018 already exists. Skipping...\n",
      "Data for Kyle Allen in year 2019 already exists. Skipping...\n",
      "Data for Kyle Allen in year 2020 already exists. Skipping...\n",
      "Data for Kyle Allen in year 2021 already exists. Skipping...\n",
      "Data for Kyle Allen in year 2022 already exists. Skipping...\n",
      "Data for Kyle Allen in year 2023 already exists. Skipping...\n",
      "Data for Kyle Allen in year 2024 already exists. Skipping...\n",
      "Data written for Kyle Allen\n",
      "Processed URL: https://www.pro-football-reference.com/players/A/AlleKy00.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing QB 141/208: Mason Rudolph\n",
      "Data for Mason Rudolph in year 2019 already exists. Skipping...\n",
      "Data for Mason Rudolph in year 2020 already exists. Skipping...\n",
      "Data for Mason Rudolph in year 2021 already exists. Skipping...\n",
      "Data for Mason Rudolph in year  already exists. Skipping...\n",
      "Data for Mason Rudolph in year 2023 already exists. Skipping...\n",
      "Data for Mason Rudolph in year 2024 already exists. Skipping...\n",
      "Data written for Mason Rudolph\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RudoMa00.htm\n",
      "Processing QB 142/208: Lamar Jackson\n",
      "Data for Lamar Jackson in year 2018 already exists. Skipping...\n",
      "Data for Lamar Jackson in year 2019 already exists. Skipping...\n",
      "Data for Lamar Jackson in year 2020 already exists. Skipping...\n",
      "Data for Lamar Jackson in year 2021 already exists. Skipping...\n",
      "Data for Lamar Jackson in year 2022 already exists. Skipping...\n",
      "Data for Lamar Jackson in year 2023 already exists. Skipping...\n",
      "Data for Lamar Jackson in year 2024 already exists. Skipping...\n",
      "Data written for Lamar Jackson\n",
      "Processed URL: https://www.pro-football-reference.com/players/J/JackLa00.htm\n",
      "Processing QB 143/208: Baker Mayfield\n",
      "Data for Baker Mayfield in year 2018 already exists. Skipping...\n",
      "Data for Baker Mayfield in year 2019 already exists. Skipping...\n",
      "Data for Baker Mayfield in year 2020 already exists. Skipping...\n",
      "Data for Baker Mayfield in year 2021 already exists. Skipping...\n",
      "Data for Baker Mayfield in year 2022 already exists. Skipping...\n",
      "Data for Baker Mayfield in year 2022 already exists. Skipping...\n",
      "Data for Baker Mayfield in year 2022 already exists. Skipping...\n",
      "Data for Baker Mayfield in year 2023 already exists. Skipping...\n",
      "Data for Baker Mayfield in year 2024 already exists. Skipping...\n",
      "Data written for Baker Mayfield\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MayfBa00.htm\n",
      "Processing QB 144/208: Joshua Allen\n",
      "Data for Joshua Allen in year 2018 already exists. Skipping...\n",
      "Data for Joshua Allen in year 2019 already exists. Skipping...\n",
      "Data for Joshua Allen in year 2020 already exists. Skipping...\n",
      "Data for Joshua Allen in year 2021 already exists. Skipping...\n",
      "Data for Joshua Allen in year 2022 already exists. Skipping...\n",
      "Data for Joshua Allen in year 2023 already exists. Skipping...\n",
      "Data for Joshua Allen in year 2024 already exists. Skipping...\n",
      "Data written for Joshua Allen\n",
      "Processed URL: https://www.pro-football-reference.com/players/A/AlleJo02.htm\n",
      "Processing QB 145/208: Sam Darnold\n",
      "Data for Sam Darnold in year 2018 already exists. Skipping...\n",
      "Data for Sam Darnold in year 2019 already exists. Skipping...\n",
      "Data for Sam Darnold in year 2020 already exists. Skipping...\n",
      "Data for Sam Darnold in year 2021 already exists. Skipping...\n",
      "Data for Sam Darnold in year 2022 already exists. Skipping...\n",
      "Data for Sam Darnold in year 2023 already exists. Skipping...\n",
      "Data for Sam Darnold in year 2024 already exists. Skipping...\n",
      "Data written for Sam Darnold\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DarnSa00.htm\n",
      "Processing QB 146/208: Trace McSorley\n",
      "Data for Trace McSorley in year 2019 already exists. Skipping...\n",
      "Data for Trace McSorley in year 2020 already exists. Skipping...\n",
      "Data for Trace McSorley in year  already exists. Skipping...\n",
      "Data for Trace McSorley in year 2022 already exists. Skipping...\n",
      "Data written for Trace McSorley\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/McSoTr00.htm\n",
      "Processing QB 147/208: Kyler Murray\n",
      "Data for Kyler Murray in year 2019 already exists. Skipping...\n",
      "Data for Kyler Murray in year 2020 already exists. Skipping...\n",
      "Data for Kyler Murray in year 2021 already exists. Skipping...\n",
      "Data for Kyler Murray in year 2022 already exists. Skipping...\n",
      "Data for Kyler Murray in year 2023 already exists. Skipping...\n",
      "Data for Kyler Murray in year 2024 already exists. Skipping...\n",
      "Data written for Kyler Murray\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MurrKy00.htm\n",
      "Processing QB 148/208: Will Grier\n",
      "Data for Will Grier in year 2019 already exists. Skipping...\n",
      "Data written for Will Grier\n",
      "Processed URL: https://www.pro-football-reference.com/players/G/GrieWi00.htm\n",
      "Processing QB 149/208: Jarrett Stidham\n",
      "Data for Jarrett Stidham in year 2019 already exists. Skipping...\n",
      "Data for Jarrett Stidham in year 2020 already exists. Skipping...\n",
      "Data for Jarrett Stidham in year  already exists. Skipping...\n",
      "Data for Jarrett Stidham in year 2022 already exists. Skipping...\n",
      "Data for Jarrett Stidham in year 2023 already exists. Skipping...\n",
      "Data for Jarrett Stidham in year 2024 already exists. Skipping...\n",
      "Data written for Jarrett Stidham\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/StidJa00.htm\n",
      "Processing QB 150/208: Easton Stick\n",
      "Data for Easton Stick in year 2020 already exists. Skipping...\n",
      "Data for Easton Stick in year  already exists. Skipping...\n",
      "Data for Easton Stick in year 2023 already exists. Skipping...\n",
      "Data written for Easton Stick\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/SticEa00.htm\n",
      "Processing QB 151/208: Gardner Minshew\n",
      "Data for Gardner Minshew in year 2019 already exists. Skipping...\n",
      "Data for Gardner Minshew in year 2020 already exists. Skipping...\n",
      "Data for Gardner Minshew in year 2021 already exists. Skipping...\n",
      "Data for Gardner Minshew in year 2022 already exists. Skipping...\n",
      "Data for Gardner Minshew in year 2023 already exists. Skipping...\n",
      "Data for Gardner Minshew in year 2024 already exists. Skipping...\n",
      "Data written for Gardner Minshew\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MinsGa00.htm\n",
      "Processing QB 152/208: Drew Lock\n",
      "Data for Drew Lock in year 2019 already exists. Skipping...\n",
      "Data for Drew Lock in year 2020 already exists. Skipping...\n",
      "Data for Drew Lock in year 2021 already exists. Skipping...\n",
      "Data for Drew Lock in year  already exists. Skipping...\n",
      "Data for Drew Lock in year 2023 already exists. Skipping...\n",
      "Data for Drew Lock in year 2024 already exists. Skipping...\n",
      "Data written for Drew Lock\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LockDr00.htm\n",
      "Processing QB 153/208: Daniel Jones\n",
      "Data for Daniel Jones in year 2019 already exists. Skipping...\n",
      "Data for Daniel Jones in year 2020 already exists. Skipping...\n",
      "Data for Daniel Jones in year 2021 already exists. Skipping...\n",
      "Data for Daniel Jones in year 2022 already exists. Skipping...\n",
      "Data for Daniel Jones in year 2023 already exists. Skipping...\n",
      "Data for Daniel Jones in year 2024 already exists. Skipping...\n",
      "Data written for Daniel Jones\n",
      "Processed URL: https://www.pro-football-reference.com/players/J/JoneDa05.htm\n",
      "Processing QB 154/208: Tyler Huntley\n",
      "Data for Tyler Huntley in year 2020 already exists. Skipping...\n",
      "Data for Tyler Huntley in year 2021 already exists. Skipping...\n",
      "Data for Tyler Huntley in year 2022 already exists. Skipping...\n",
      "Data for Tyler Huntley in year 2023 already exists. Skipping...\n",
      "Data for Tyler Huntley in year 2024 already exists. Skipping...\n",
      "Data written for Tyler Huntley\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HuntTy01.htm\n",
      "Processing QB 155/208: William Fromm\n",
      "Data for William Fromm in year 2021 already exists. Skipping...\n",
      "Data written for William Fromm\n",
      "Processed URL: https://www.pro-football-reference.com/players/F/FromJa00.htm\n",
      "Processing QB 156/208: Tua Tagovailoa\n",
      "Data for Tua Tagovailoa in year 2020 already exists. Skipping...\n",
      "Data for Tua Tagovailoa in year 2021 already exists. Skipping...\n",
      "Data for Tua Tagovailoa in year 2022 already exists. Skipping...\n",
      "Data for Tua Tagovailoa in year 2023 already exists. Skipping...\n",
      "Data for Tua Tagovailoa in year 2024 already exists. Skipping...\n",
      "Data written for Tua Tagovailoa\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TagoTu00.htm\n",
      "Processing QB 157/208: Jordan Love\n",
      "Data for Jordan Love in year 2021 already exists. Skipping...\n",
      "Data for Jordan Love in year 2022 already exists. Skipping...\n",
      "Data for Jordan Love in year 2023 already exists. Skipping...\n",
      "Data for Jordan Love in year 2024 already exists. Skipping...\n",
      "Data written for Jordan Love\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LoveJo03.htm\n",
      "Processing QB 158/208: Jake Luton\n",
      "Data for Jake Luton in year 2020 already exists. Skipping...\n",
      "Data written for Jake Luton\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LutoJa00.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing QB 159/208: Justin Herbert\n",
      "Data for Justin Herbert in year 2020 already exists. Skipping...\n",
      "Data for Justin Herbert in year 2021 already exists. Skipping...\n",
      "Data for Justin Herbert in year 2022 already exists. Skipping...\n",
      "Data for Justin Herbert in year 2023 already exists. Skipping...\n",
      "Data for Justin Herbert in year 2024 already exists. Skipping...\n",
      "Data written for Justin Herbert\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HerbJu00.htm\n",
      "Processing QB 160/208: Benjamin DiNucci\n",
      "Data for Benjamin DiNucci in year 2020 already exists. Skipping...\n",
      "Data written for Benjamin DiNucci\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DiNuBe00.htm\n",
      "Processing QB 161/208: Jalen Hurts\n",
      "Data for Jalen Hurts in year 2020 already exists. Skipping...\n",
      "Data for Jalen Hurts in year 2021 already exists. Skipping...\n",
      "Data for Jalen Hurts in year 2022 already exists. Skipping...\n",
      "Data for Jalen Hurts in year 2023 already exists. Skipping...\n",
      "Data for Jalen Hurts in year 2024 already exists. Skipping...\n",
      "Data written for Jalen Hurts\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HurtJa00.htm\n",
      "Processing QB 162/208: Joe Burrow\n",
      "Data for Joe Burrow in year 2020 already exists. Skipping...\n",
      "Data for Joe Burrow in year 2021 already exists. Skipping...\n",
      "Data for Joe Burrow in year 2022 already exists. Skipping...\n",
      "Data for Joe Burrow in year 2023 already exists. Skipping...\n",
      "Data for Joe Burrow in year 2024 already exists. Skipping...\n",
      "Data written for Joe Burrow\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BurrJo01.htm\n",
      "Processing QB 163/208: Samuel Ehlinger\n",
      "Data for Samuel Ehlinger in year 2021 already exists. Skipping...\n",
      "Data for Samuel Ehlinger in year 2022 already exists. Skipping...\n",
      "Data for Samuel Ehlinger in year 2023 already exists. Skipping...\n",
      "Data written for Samuel Ehlinger\n",
      "Processed URL: https://www.pro-football-reference.com/players/E/EhliSa00.htm\n",
      "Processing QB 164/208: Davis Mills\n",
      "Data for Davis Mills in year 2021 already exists. Skipping...\n",
      "Data for Davis Mills in year 2022 already exists. Skipping...\n",
      "Data for Davis Mills in year 2023 already exists. Skipping...\n",
      "Data for Davis Mills in year 2024 already exists. Skipping...\n",
      "Data written for Davis Mills\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MillDa02.htm\n",
      "Processing QB 165/208: Kyle Trask\n",
      "Data for Kyle Trask in year 2022 already exists. Skipping...\n",
      "Data for Kyle Trask in year 2023 already exists. Skipping...\n",
      "Data for Kyle Trask in year 2024 already exists. Skipping...\n",
      "Data written for Kyle Trask\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TrasKy00.htm\n",
      "Processing QB 166/208: Ian Book\n",
      "Data for Ian Book in year 2021 already exists. Skipping...\n",
      "Data written for Ian Book\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BookIa00.htm\n",
      "Processing QB 167/208: Justin Fields\n",
      "Data for Justin Fields in year 2021 already exists. Skipping...\n",
      "Data for Justin Fields in year 2022 already exists. Skipping...\n",
      "Data for Justin Fields in year 2023 already exists. Skipping...\n",
      "Data for Justin Fields in year 2024 already exists. Skipping...\n",
      "Data written for Justin Fields\n",
      "Processed URL: https://www.pro-football-reference.com/players/F/FielJu00.htm\n",
      "Processing QB 168/208: Trevor Lawrence\n",
      "Data for Trevor Lawrence in year 2021 already exists. Skipping...\n",
      "Data for Trevor Lawrence in year 2022 already exists. Skipping...\n",
      "Data for Trevor Lawrence in year 2023 already exists. Skipping...\n",
      "Data for Trevor Lawrence in year 2024 already exists. Skipping...\n",
      "Data written for Trevor Lawrence\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LawrTr00.htm\n",
      "Processing QB 169/208: Michael Jones\n",
      "Data for Michael Jones in year 2021 already exists. Skipping...\n",
      "Data for Michael Jones in year 2022 already exists. Skipping...\n",
      "Data for Michael Jones in year 2023 already exists. Skipping...\n",
      "Data for Michael Jones in year 2024 already exists. Skipping...\n",
      "Data written for Michael Jones\n",
      "Processed URL: https://www.pro-football-reference.com/players/J/JoneMa05.htm\n",
      "Processing QB 170/208: Trey Lance\n",
      "Data for Trey Lance in year 2021 already exists. Skipping...\n",
      "Data for Trey Lance in year 2022 already exists. Skipping...\n",
      "Data for Trey Lance in year  already exists. Skipping...\n",
      "Data for Trey Lance in year 2024 already exists. Skipping...\n",
      "Data written for Trey Lance\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LancTr00.htm\n",
      "Processing QB 171/208: Zachary Wilson\n",
      "Data for Zachary Wilson in year 2021 already exists. Skipping...\n",
      "Data for Zachary Wilson in year 2022 already exists. Skipping...\n",
      "Data for Zachary Wilson in year 2023 already exists. Skipping...\n",
      "Data written for Zachary Wilson\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WilsZa00.htm\n",
      "Processing QB 172/208: Sam Howell\n",
      "Data for Sam Howell in year 2022 already exists. Skipping...\n",
      "Data for Sam Howell in year 2023 already exists. Skipping...\n",
      "Data for Sam Howell in year 2024 already exists. Skipping...\n",
      "Data written for Sam Howell\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HoweSa00.htm\n",
      "Processing QB 173/208: Anthony Brown\n",
      "Data for Anthony Brown in year 2022 already exists. Skipping...\n",
      "Data written for Anthony Brown\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BrowAn06.htm\n",
      "Processing QB 174/208: Chris Oladokun\n",
      "Data for Chris Oladokun in year 2024 already exists. Skipping...\n",
      "Data written for Chris Oladokun\n",
      "Processed URL: https://www.pro-football-reference.com/players/O/OladCh00.htm\n",
      "Processing QB 175/208: Skylar Thompson\n",
      "Data for Skylar Thompson in year 2022 already exists. Skipping...\n",
      "Data for Skylar Thompson in year  already exists. Skipping...\n",
      "Data for Skylar Thompson in year 2024 already exists. Skipping...\n",
      "Data written for Skylar Thompson\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/ThomSk00.htm\n",
      "Processing QB 176/208: Brock Purdy\n",
      "Data for Brock Purdy in year 2022 already exists. Skipping...\n",
      "Data for Brock Purdy in year 2023 already exists. Skipping...\n",
      "Data for Brock Purdy in year 2024 already exists. Skipping...\n",
      "Data written for Brock Purdy\n",
      "Processed URL: https://www.pro-football-reference.com/players/P/PurdBr00.htm\n",
      "Processing QB 177/208: Kenny Pickett\n",
      "Data for Kenny Pickett in year 2022 already exists. Skipping...\n",
      "Data for Kenny Pickett in year 2023 already exists. Skipping...\n",
      "Data for Kenny Pickett in year 2024 already exists. Skipping...\n",
      "Data written for Kenny Pickett\n",
      "Processed URL: https://www.pro-football-reference.com/players/P/PickKe00.htm\n",
      "Processing QB 178/208: Bailey Zappe\n",
      "Data for Bailey Zappe in year 2022 already exists. Skipping...\n",
      "Data for Bailey Zappe in year 2023 already exists. Skipping...\n",
      "Data for Bailey Zappe in year 2024 already exists. Skipping...\n",
      "Data written for Bailey Zappe\n",
      "Processed URL: https://www.pro-football-reference.com/players/Z/ZappBa00.htm\n",
      "Processing QB 179/208: Desmond Ridder\n",
      "Data for Desmond Ridder in year 2022 already exists. Skipping...\n",
      "Data for Desmond Ridder in year 2023 already exists. Skipping...\n",
      "Data for Desmond Ridder in year 2024 already exists. Skipping...\n",
      "Data written for Desmond Ridder\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RiddDe00.htm\n",
      "Processing QB 180/208: Malik Willis\n",
      "Data for Malik Willis in year 2022 already exists. Skipping...\n",
      "Data for Malik Willis in year 2023 already exists. Skipping...\n",
      "Data for Malik Willis in year 2024 already exists. Skipping...\n",
      "Data written for Malik Willis\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WillMa12.htm\n",
      "Processing QB 181/208: Matt Corral\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/C/CorrMa00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/C/CorrMa00.htm\n",
      "Processing QB 182/208: Sean Clifford\n",
      "Data for Sean Clifford in year 2023 already exists. Skipping...\n",
      "Data written for Sean Clifford\n",
      "Processed URL: https://www.pro-football-reference.com/players/C/ClifSe00.htm\n",
      "Processing QB 183/208: Tanner McKee\n",
      "Data for Tanner McKee in year 2024 already exists. Skipping...\n",
      "Data written for Tanner McKee\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/McKeTa01.htm\n",
      "Processing QB 184/208: Tyson Bagent\n",
      "Data for Tyson Bagent in year 2023 already exists. Skipping...\n",
      "Data for Tyson Bagent in year 2024 already exists. Skipping...\n",
      "Data written for Tyson Bagent\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BageTy00.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing QB 185/208: Tommy DeVito\n",
      "Data for Tommy DeVito in year 2023 already exists. Skipping...\n",
      "Data for Tommy DeVito in year 2024 already exists. Skipping...\n",
      "Data written for Tommy DeVito\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DeViTo00.htm\n",
      "Processing QB 186/208: Hendon Hooker\n",
      "Data for Hendon Hooker in year 2024 already exists. Skipping...\n",
      "Data written for Hendon Hooker\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HookHe00.htm\n",
      "Processing QB 187/208: Aidan O'Connell\n",
      "Data for Aidan O'Connell in year 2023 already exists. Skipping...\n",
      "Data for Aidan O'Connell in year 2024 already exists. Skipping...\n",
      "Data written for Aidan O'Connell\n",
      "Processed URL: https://www.pro-football-reference.com/players/O/OConAi00.htm\n",
      "Processing QB 188/208: Clayton Tune\n",
      "Data for Clayton Tune in year 2023 already exists. Skipping...\n",
      "Data for Clayton Tune in year 2024 already exists. Skipping...\n",
      "Data written for Clayton Tune\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TuneCl00.htm\n",
      "Processing QB 189/208: Dorian Thompson-Robinson\n",
      "Data for Dorian Thompson-Robinson in year 2023 already exists. Skipping...\n",
      "Data for Dorian Thompson-Robinson in year 2024 already exists. Skipping...\n",
      "Data written for Dorian Thompson-Robinson\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/ThomDo02.htm\n",
      "Processing QB 190/208: Jaren Hall\n",
      "Data for Jaren Hall in year 2023 already exists. Skipping...\n",
      "Data written for Jaren Hall\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HallJa00.htm\n",
      "Processing QB 191/208: Max Duggan\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/D/DuggMa00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DuggMa00.htm\n",
      "Processing QB 192/208: Jake Haener\n",
      "Data for Jake Haener in year 2024 already exists. Skipping...\n",
      "Data written for Jake Haener\n",
      "Processed URL: https://www.pro-football-reference.com/players/H/HaenJa00.htm\n",
      "Processing QB 193/208: Stetson Bennett\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/BennSt00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/B/BennSt00.htm\n",
      "Processing QB 194/208: Bryce Young\n",
      "Data for Bryce Young in year 2023 already exists. Skipping...\n",
      "Data for Bryce Young in year 2024 already exists. Skipping...\n",
      "Data written for Bryce Young\n",
      "Processed URL: https://www.pro-football-reference.com/players/Y/YounBr01.htm\n",
      "Processing QB 195/208: Will Levis\n",
      "Data for Will Levis in year 2023 already exists. Skipping...\n",
      "Data for Will Levis in year 2024 already exists. Skipping...\n",
      "Data written for Will Levis\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LeviWi00.htm\n",
      "Processing QB 196/208: Coleridge Stroud\n",
      "Data for Coleridge Stroud in year 2023 already exists. Skipping...\n",
      "Data for Coleridge Stroud in year 2024 already exists. Skipping...\n",
      "Data written for Coleridge Stroud\n",
      "Processed URL: https://www.pro-football-reference.com/players/S/StroCJ00.htm\n",
      "Processing QB 197/208: Anthony Richardson\n",
      "Data for Anthony Richardson in year 2023 already exists. Skipping...\n",
      "Data for Anthony Richardson in year 2024 already exists. Skipping...\n",
      "Data written for Anthony Richardson\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RichAn03.htm\n",
      "Processing QB 198/208: Michael Pratt\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/P/PratMi00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/P/PratMi00.htm\n",
      "Processing QB 199/208: Spencer Rattler\n",
      "Data written for Spencer Rattler\n",
      "Processed URL: https://www.pro-football-reference.com/players/R/RattSp00.htm\n",
      "Processing QB 200/208: Joe Milton\n",
      "Data written for Joe Milton\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MiltJo00.htm\n",
      "Processing QB 201/208: Bo Nix\n",
      "Data written for Bo Nix\n",
      "Processed URL: https://www.pro-football-reference.com/players/N/NixxBo00.htm\n",
      "Processing QB 202/208: Jordan Travis\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/T/TravJo00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/T/TravJo00.htm\n",
      "Processing QB 203/208: Devin Leary\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/L/LearDe00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/L/LearDe00.htm\n",
      "Processing QB 204/208: Drake Maye\n",
      "Data written for Drake Maye\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/MayeDr00.htm\n",
      "Processing QB 205/208: Jayden Daniels\n",
      "Data written for Jayden Daniels\n",
      "Processed URL: https://www.pro-football-reference.com/players/D/DaniJa02.htm\n",
      "Processing QB 206/208: Michael Penix\n",
      "Data written for Michael Penix\n",
      "Processed URL: https://www.pro-football-reference.com/players/P/PeniMi00.htm\n",
      "Processing QB 207/208: Caleb Williams\n",
      "Data written for Caleb Williams\n",
      "Processed URL: https://www.pro-football-reference.com/players/W/WillCa03.htm\n",
      "Processing QB 208/208: Jonathan McCarthy\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/McCaJJ00.htm\n",
      "Processed URL: https://www.pro-football-reference.com/players/M/McCaJJ00.htm\n",
      "Data saved to game_logs_qb.csv\n"
     ]
    }
   ],
   "source": [
    "# # QB game tables\n",
    "# # Not in nfl.db currently\n",
    "\n",
    "# df = pd.read_csv('./data/rosters.csv')\n",
    "\n",
    "# # Drop rows where 'pfr_id' is missing\n",
    "# df = df.dropna(subset=['pfr_id'])\n",
    "\n",
    "# # Filter for Quarterbacks\n",
    "# qbs = df[df['position'] == 'QB']\n",
    "\n",
    "# # Hardcode the headers\n",
    "headers = [\n",
    "    'Player URL', 'Position', 'First Name', 'Last Name', 'Year', 'Age', 'Tm', 'Pos', 'No.', 'G', 'GS', 'QBrec', 'Cmp', 'Att', 'Cmp%', 'Yds', 'TD', \n",
    "    'TD%', 'Int', 'Int%', '1D', 'Succ%', 'Lng', 'Y/A', 'AY/A', 'Y/C', 'Y/G', 'Rate', 'QBR', \n",
    "    'Sk', 'Yds', 'Sk%', 'NY/A', 'ANY/A', '4QC', 'GWD', 'AV', 'Awards'\n",
    "]\n",
    "\n",
    "# # Open a CSV file to write the data\n",
    "# with open('./data/game_logs_qb.csv', 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(headers)  # Write the hardcoded headers\n",
    "\n",
    "#     # Initialize a counter for progress tracking\n",
    "#     total_qbs = len(qbs)\n",
    "#     qb_counter = 0\n",
    "\n",
    "#     # Iterate over each quarterback and scrape data\n",
    "#     for index, qb in qbs.iterrows():\n",
    "#         qb_counter += 1\n",
    "#         url = qb['url']\n",
    "#         print(f\"Processing QB {qb_counter}/{total_qbs}: {qb['first_name']} {qb['last_name']}\")\n",
    "#         first_name = qb['first_name']  # Get the player's first name\n",
    "#         last_name = qb['last_name']    # Get the player's last name\n",
    "#         position = 'QB'  # Assuming position is always QB as per your filter\n",
    "\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 200:\n",
    "#             soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#             game_logs_table = soup.find('table', {'id': 'passing'})  # Adjust this ID if needed\n",
    "\n",
    "#             if game_logs_table:  # Check if the table is found\n",
    "#                 data_rows = game_logs_table.find('tbody').find_all('tr')\n",
    "\n",
    "#                 for row in data_rows:\n",
    "#                     cells = row.find_all(['th', 'td'])\n",
    "#                     data = [cell.text.strip() for cell in cells]\n",
    "#                     data = [url, position, first_name, last_name] + data\n",
    "#                     writer.writerow(data)\n",
    "\n",
    "                \n",
    "#                 print(f\"Data written for {first_name} {last_name}\")\n",
    "\n",
    "#             else:\n",
    "#                 print(f\"No game logs table found for URL: {url}\")\n",
    "#         else:\n",
    "#             print(f\"Failed to retrieve URL: {url} with status code: {response.status_code}\")\n",
    "\n",
    "#         print(f'Processed URL: {url}')  # Print the URL being processed\n",
    "#         time.sleep(2)  # Add a 3-second delay after processing each URL\n",
    "\n",
    "# print('Data saved to game_logs_qb.csv')\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/rosters.csv')\n",
    "\n",
    "# Drop rows where 'pfr_id' is missing\n",
    "df = df.dropna(subset=['pfr_id'])\n",
    "\n",
    "# Filter for Quarterbacks\n",
    "qbs = df[df['position'] == 'QB']\n",
    "\n",
    "\n",
    "# Load existing QB data from CSV\n",
    "try:\n",
    "    existing_qb_data = pd.read_csv('./data/game_logs_qb.csv')\n",
    "    # Create a set of (Player URL, Year) tuples to check for duplicates\n",
    "    existing_url_years = set(zip(existing_qb_data['Player URL'], existing_qb_data['Year']))\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, initialize an empty set\n",
    "    existing_qb_data = pd.DataFrame(columns=headers)\n",
    "    existing_url_years = set()\n",
    "\n",
    "# Open the CSV file in append mode\n",
    "with open('./data/game_logs_qb.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # If the file is empty, write the headers first\n",
    "    if file.tell() == 0:\n",
    "        writer.writerow(headers)\n",
    "\n",
    "    # Initialize a counter for progress tracking\n",
    "    total_qbs = len(qbs)\n",
    "    qb_counter = 0\n",
    "    max_retries = 3  # Set maximum number of retries\n",
    "\n",
    "    # Iterate over each quarterback and scrape data\n",
    "    for index, qb in qbs.iterrows():\n",
    "        qb_counter += 1\n",
    "        url = qb['url']\n",
    "        first_name = qb['first_name']\n",
    "        last_name = qb['last_name']\n",
    "        position = 'QB'\n",
    "\n",
    "        print(f\"Processing QB {qb_counter}/{total_qbs}: {first_name} {last_name}\")\n",
    "\n",
    "        retries = 0  # Retry counter\n",
    "        success = False  # Flag for successful request\n",
    "\n",
    "        while retries < max_retries and not success:\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)  # Add timeout to avoid long waiting periods\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    game_logs_table = soup.find('table', {'id': 'passing'})  # Adjust this ID if needed\n",
    "\n",
    "                    if game_logs_table:  # Check if the table is found\n",
    "                        data_rows = game_logs_table.find('tbody').find_all('tr')\n",
    "\n",
    "                        for row in data_rows:\n",
    "                            cells = row.find_all(['th', 'td'])\n",
    "                            data = [cell.text.strip() for cell in cells]\n",
    "\n",
    "                            # Extract 'Year' from the data (assumes year is in the correct column)\n",
    "                            year = data[0]  # Adjust index if Year is in another column\n",
    "\n",
    "                            # Check if the (url, year) combination already exists\n",
    "                            if (url, year) in existing_url_years:\n",
    "                                print(f\"Data for {first_name} {last_name} in year {year} already exists. Skipping...\")\n",
    "                                continue\n",
    "\n",
    "                            # If unique, append data with player information\n",
    "                            data = [url, position, first_name, last_name] + data\n",
    "                            writer.writerow(data)\n",
    "                            existing_url_years.add((url, year))  # Add the new combination to the set\n",
    "\n",
    "                        print(f\"Data written for {first_name} {last_name}\")\n",
    "                        success = True  # Mark as successful\n",
    "                    else:\n",
    "                        print(f\"No game logs table found for URL: {url}\")\n",
    "                        success = True  # No need to retry if there's no table\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve URL: {url} with status code: {response.status_code}\")\n",
    "                    retries += 1\n",
    "\n",
    "            except (Timeout, RequestException) as e:\n",
    "                retries += 1\n",
    "                print(f\"Error processing {url}: {e}. Retrying ({retries}/{max_retries})...\")\n",
    "                time.sleep(2 ** retries)  # Exponential backoff\n",
    "\n",
    "        # If not successful after all retries, skip the QB\n",
    "        if not success:\n",
    "            print(f\"Skipping {first_name} {last_name} after {max_retries} failed attempts.\")\n",
    "\n",
    "        print(f'Processed URL: {url}')\n",
    "        time.sleep(2.25)  # Add a delay after processing each URL\n",
    "\n",
    "print('Data saved to game_logs_qb.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4198dcbd-16c6-4479-b4c9-6b442e172b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already processed TE: Marcedes Lewis\n",
      "Skipping already processed TE: Jimmy Graham\n",
      "Skipping already processed TE: Zachary Ertz\n",
      "Skipping already processed TE: Travis Kelce\n",
      "Skipping already processed TE: Logan Thomas\n",
      "Skipping already processed TE: Jordan Matthews\n",
      "Skipping already processed TE: Chris Manhertz\n",
      "Skipping already processed TE: MyCole Pruitt\n",
      "Skipping already processed TE: Darren Waller\n",
      "Skipping already processed TE: Eric Tomlinson\n",
      "Skipping already processed TE: Blake Bell\n",
      "Skipping already processed TE: Christopher Uzomah\n",
      "Skipping already processed TE: Jesse James\n",
      "Skipping already processed TE: Geoffrey Swaim\n",
      "Skipping already processed TE: Tyler Kroft\n",
      "Skipping already processed TE: Austin Hooper\n",
      "Skipping already processed TE: Nicholas Vannett\n",
      "Skipping already processed TE: Stephen Anderson\n",
      "Skipping already processed TE: John Holtz\n",
      "Skipping already processed TE: Hunter Henry\n",
      "Skipping already processed TE: Tyler Higbee\n",
      "Skipping already processed TE: Mo Alie-Cox\n",
      "Skipping already processed TE: Johnny Mundt\n",
      "Skipping already processed TE: Darrell Daniels\n",
      "Skipping already processed TE: George Kittle\n",
      "Skipping already processed TE: Jacob Hollister\n",
      "Skipping already processed TE: Pharaoh Brown\n",
      "Skipping already processed TE: Anthony Firkser\n",
      "Skipping already processed TE: Eric Saubert\n",
      "Skipping already processed TE: Roderick Seals-Jones\n",
      "Skipping already processed TE: Colin Thompson\n",
      "Skipping already processed TE: Robert Tonyan\n",
      "Skipping already processed TE: Mason Schreck\n",
      "Skipping already processed TE: Jonnu Smith\n",
      "Skipping already processed TE: Evan Engram\n",
      "Skipping already processed TE: David Njoku\n",
      "Skipping already processed TE: Gerald Everett\n",
      "Skipping already processed TE: Ross Dwelley\n",
      "Skipping already processed TE: Will Dissly\n",
      "Skipping already processed TE: Tyler Conklin\n",
      "Skipping already processed TE: Dallas Goedert\n",
      "Skipping already processed TE: Jordan Akins\n",
      "Skipping already processed TE: Ian Thomas\n",
      "Skipping already processed TE: Dalton Schultz\n",
      "Skipping already processed TE: Troy Fumagalli\n",
      "Skipping already processed TE: Mark Andrews\n",
      "Skipping already processed TE: Durham Smythe\n",
      "Skipping already processed TE: Michael Gesicki\n",
      "Skipping already processed TE: Hayden Hurst\n",
      "Skipping already processed TE: Irvin Smith\n",
      "Skipping already processed TE: Foster Moreau\n",
      "Skipping already processed TE: Kaden Smith\n",
      "Skipping already processed TE: Stephen Carlson\n",
      "Skipping already processed TE: Brandon Dillon\n",
      "Skipping already processed TE: Zach Gentry\n",
      "Skipping already processed TE: Thomas Hockenson\n",
      "Skipping already processed TE: Josh Oliver\n",
      "Skipping already processed TE: Trevon Wesco\n",
      "Skipping already processed TE: Tommy Sweeney\n",
      "Processing TE 1/291: Alize Mack\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/MackAl00.htm\n",
      "Skipping already processed TE: Donald Parham\n",
      "Processing TE 2/291: Hakeem Butler\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/ButlHa00.htm\n",
      "Skipping already processed TE: Joe Fortson\n",
      "Skipping already processed TE: Kendall Blanton\n",
      "Skipping already processed TE: Drew Sample\n",
      "Skipping already processed TE: Noah Fant\n",
      "Skipping already processed TE: Jace Sternberger\n",
      "Skipping already processed TE: Dawson Knox\n",
      "Skipping already processed TE: Ben Ellefson\n",
      "Processing TE 3/291: Noah Togiai\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/T/TogiNo00.htm\n",
      "Skipping already processed TE: Sean McKeon\n",
      "Skipping already processed TE: Juwan Johnson\n",
      "Skipping already processed TE: Devin Asiasi\n",
      "Skipping already processed TE: Dalton Keene\n",
      "Skipping already processed TE: Harrison Bryant\n",
      "Skipping already processed TE: Colby Parkinson\n",
      "Skipping already processed TE: Tyler Davis\n",
      "Skipping already processed TE: Cole Kmet\n",
      "Skipping already processed TE: Josiah Deguara\n",
      "Skipping already processed TE: Adam Trautman\n",
      "Skipping already processed TE: Albert Okwuegbunam\n",
      "Skipping already processed TE: Brycen Hopkins\n",
      "Skipping already processed TE: Charlie Woerner\n",
      "Skipping already processed TE: Stephen Sullivan\n",
      "Skipping already processed TE: Dominique Dafney\n",
      "Processing TE 4/291: Sammis Reyes\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/R/ReyeSa00.htm\n",
      "Skipping already processed TE: Kenny Yeboah\n",
      "Processing TE 5/291: Matt Bushman\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/BushMa00.htm\n",
      "Skipping already processed TE: Shane Zylstra\n",
      "Skipping already processed TE: Brevin Jordan\n",
      "Processing TE 6/291: Ben Mason\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/MasoBe00.htm\n",
      "Skipping already processed TE: John Bates\n",
      "Skipping already processed TE: Noah Gray\n",
      "Skipping already processed TE: Zach Davidson\n",
      "Processing TE 7/291: Miller Forristall\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/F/ForrMi00.htm\n",
      "Skipping already processed TE: Jack Stoll\n",
      "Skipping already processed TE: Brock Wright\n",
      "Skipping already processed TE: Kylen Granson\n",
      "Skipping already processed TE: Luke Farrell\n",
      "Skipping already processed TE: Pat Freiermuth\n",
      "Skipping already processed TE: Kyle Pitts\n",
      "Skipping already processed TE: Hunter Long\n",
      "Skipping already processed TE: Tommy Tremble\n",
      "Skipping already processed TE: Tre' McKitty\n",
      "Processing TE 8/291: Nikola Kalinic\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/K/KaliNi00.htm\n",
      "Skipping already processed TE: Cole Turner\n",
      "Skipping already processed TE: Grant Calcaterra\n",
      "Processing TE 9/291: Jake Tonges\n",
      "Data written for Jake Tonges\n",
      "Processing TE 10/291: Armani Rogers\n",
      "Data written for Armani Rogers\n",
      "Processing TE 11/291: Greg Dulcich\n",
      "Data written for Greg Dulcich\n",
      "Processing TE 12/291: Teagan Quitoriano\n",
      "Data written for Teagan Quitoriano\n",
      "Processing TE 13/291: James Mitchell\n",
      "Data written for James Mitchell\n",
      "Processing TE 14/291: Andrew Ogletree\n",
      "Data written for Andrew Ogletree\n",
      "Processing TE 15/291: Connor Heyward\n",
      "Data written for Connor Heyward\n",
      "Processing TE 16/291: John FitzPatrick\n",
      "Data written for John FitzPatrick\n",
      "Processing TE 17/291: Ko Kieft\n",
      "Data written for Ko Kieft\n",
      "Processing TE 18/291: Nick Muse\n",
      "Data written for Nick Muse\n",
      "Processing TE 19/291: Stone Smartt\n",
      "Data written for Stone Smartt\n",
      "Processing TE 20/291: Lucas Krull\n",
      "Data written for Lucas Krull\n",
      "Processing TE 21/291: Peyton Hendershot\n",
      "Data written for Peyton Hendershot\n",
      "Processing TE 22/291: Trey McBride\n",
      "Data written for Trey McBride\n",
      "Processing TE 23/291: Jelani Woods\n",
      "Data written for Jelani Woods\n",
      "Processing TE 24/291: Jeremy Ruckert\n",
      "Data written for Jeremy Ruckert\n",
      "Processing TE 25/291: Chigoziem Okonkwo\n",
      "Data written for Chigoziem Okonkwo\n",
      "Processing TE 26/291: Isaiah Likely\n",
      "Data written for Isaiah Likely\n",
      "Processing TE 27/291: Jake Ferguson\n",
      "Data written for Jake Ferguson\n",
      "Processing TE 28/291: Charlie Kolar\n",
      "Data written for Charlie Kolar\n",
      "Processing TE 29/291: Daniel Bellinger\n",
      "Data written for Daniel Bellinger\n",
      "Processing TE 30/291: Cade Otton\n",
      "Data written for Cade Otton\n",
      "Processing TE 31/291: Will Mallory\n",
      "Data written for Will Mallory\n",
      "Processing TE 32/291: Zack Kuntz\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/K/KuntZa00.htm\n",
      "Processing TE 33/291: Brady Russell\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/R/RussBr00.htm\n",
      "Processing TE 34/291: Luke Schoonmaker\n",
      "Data written for Luke Schoonmaker\n",
      "Processing TE 35/291: Darnell Washington\n",
      "Data written for Darnell Washington\n",
      "Processing TE 36/291: Cameron Latu\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/L/LatuCa01.htm\n",
      "Processing TE 37/291: Josh Whyle\n",
      "Data written for Josh Whyle\n",
      "Processing TE 38/291: Brayden Willis\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/W/WillBr06.htm\n",
      "Processing TE 39/291: Julian Hill\n",
      "Data written for Julian Hill\n",
      "Processing TE 40/291: Nate Adkins\n",
      "Data written for Nate Adkins\n",
      "Processing TE 41/291: Ben Sims\n",
      "Data written for Ben Sims\n",
      "Processing TE 42/291: Dalton Kincaid\n",
      "Data written for Dalton Kincaid\n",
      "Processing TE 43/291: Brenton Strange\n",
      "Data written for Brenton Strange\n",
      "Processing TE 44/291: Tucker Kraft\n",
      "Data written for Tucker Kraft\n",
      "Processing TE 45/291: Elijah Higgins\n",
      "Data written for Elijah Higgins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TE 46/291: Payne Durham\n",
      "Data written for Payne Durham\n",
      "Processing TE 47/291: Sam LaPorta\n",
      "Data written for Sam LaPorta\n",
      "Processing TE 48/291: Michael Mayer\n",
      "Data written for Michael Mayer\n",
      "Processing TE 49/291: Davis Allen\n",
      "Data written for Davis Allen\n",
      "Processing TE 50/291: Luke Musgrave\n",
      "Data written for Luke Musgrave\n",
      "Skipping already processed TE: Marcedes Lewis\n",
      "Skipping already processed TE: Zachary Ertz\n",
      "Skipping already processed TE: Travis Kelce\n",
      "Skipping already processed TE: Logan Thomas\n",
      "Skipping already processed TE: Jordan Matthews\n",
      "Skipping already processed TE: Chris Manhertz\n",
      "Skipping already processed TE: MyCole Pruitt\n",
      "Skipping already processed TE: Darren Waller\n",
      "Skipping already processed TE: Eric Tomlinson\n",
      "Skipping already processed TE: Christopher Uzomah\n",
      "Skipping already processed TE: Geoffrey Swaim\n",
      "Skipping already processed TE: Austin Hooper\n",
      "Skipping already processed TE: Nicholas Vannett\n",
      "Skipping already processed TE: Hunter Henry\n",
      "Skipping already processed TE: Tyler Higbee\n",
      "Skipping already processed TE: Mo Alie-Cox\n",
      "Skipping already processed TE: Johnny Mundt\n",
      "Skipping already processed TE: George Kittle\n",
      "Processing TE 51/291: Taysom Hill\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/H/HillTa00.htm\n",
      "Skipping already processed TE: Jacob Hollister\n",
      "Skipping already processed TE: Pharaoh Brown\n",
      "Skipping already processed TE: Anthony Firkser\n",
      "Skipping already processed TE: Eric Saubert\n",
      "Skipping already processed TE: Robert Tonyan\n",
      "Skipping already processed TE: Jonnu Smith\n",
      "Skipping already processed TE: Evan Engram\n",
      "Skipping already processed TE: David Njoku\n",
      "Skipping already processed TE: Gerald Everett\n",
      "Skipping already processed TE: Ross Dwelley\n",
      "Skipping already processed TE: Will Dissly\n",
      "Processing TE 52/291: Tyler Conklin\n",
      "Data written for Tyler Conklin\n",
      "Skipping already processed TE: Dallas Goedert\n",
      "Skipping already processed TE: Jordan Akins\n",
      "Skipping already processed TE: Ian Thomas\n",
      "Skipping already processed TE: Dalton Schultz\n",
      "Skipping already processed TE: Mark Andrews\n",
      "Skipping already processed TE: Durham Smythe\n",
      "Skipping already processed TE: Michael Gesicki\n",
      "Skipping already processed TE: Hayden Hurst\n",
      "Skipping already processed TE: Irvin Smith\n",
      "Skipping already processed TE: Foster Moreau\n",
      "Skipping already processed TE: Stephen Carlson\n",
      "Skipping already processed TE: Zach Gentry\n",
      "Skipping already processed TE: Thomas Hockenson\n",
      "Skipping already processed TE: Josh Oliver\n",
      "Skipping already processed TE: Trevon Wesco\n",
      "Skipping already processed TE: Tommy Sweeney\n",
      "Skipping already processed TE: Donald Parham\n",
      "Skipping already processed TE: Joseph Fortson\n",
      "Processing TE 53/291: N'Keal Harry\n",
      "Data written for N'Keal Harry\n",
      "Skipping already processed TE: Drew Sample\n",
      "Skipping already processed TE: Noah Fant\n",
      "Skipping already processed TE: Dawson Knox\n",
      "Skipping already processed TE: Sean McKeon\n",
      "Skipping already processed TE: Juwan Johnson\n",
      "Skipping already processed TE: Dalton Keene\n",
      "Skipping already processed TE: Harrison Bryant\n",
      "Skipping already processed TE: Colby Parkinson\n",
      "Skipping already processed TE: Tyler Davis\n",
      "Skipping already processed TE: Cole Kmet\n",
      "Skipping already processed TE: Josiah Deguara\n",
      "Skipping already processed TE: Adam Trautman\n",
      "Skipping already processed TE: Albert Okwuegbunam\n",
      "Skipping already processed TE: Charlie Woerner\n",
      "Skipping already processed TE: Stephen Sullivan\n",
      "Skipping already processed TE: Dominique Dafney\n",
      "Skipping already processed TE: Sammis Reyes\n",
      "Skipping already processed TE: Kenny Yeboah\n",
      "Skipping already processed TE: Shane Zylstra\n",
      "Skipping already processed TE: Brevin Jordan\n",
      "Skipping already processed TE: Ben Mason\n",
      "Skipping already processed TE: John Bates\n",
      "Skipping already processed TE: Noah Gray\n",
      "Skipping already processed TE: Zach Davidson\n",
      "Skipping already processed TE: Miller Forristall\n",
      "Skipping already processed TE: Jack Stoll\n",
      "Skipping already processed TE: Brock Wright\n",
      "Skipping already processed TE: Kylen Granson\n",
      "Skipping already processed TE: Luke Farrell\n",
      "Skipping already processed TE: Pat Freiermuth\n",
      "Skipping already processed TE: Kyle Pitts\n",
      "Skipping already processed TE: Hunter Long\n",
      "Skipping already processed TE: Tommy Tremble\n",
      "Skipping already processed TE: Tre' McKitty\n",
      "Skipping already processed TE: Nikola Kalinic\n",
      "Skipping already processed TE: Cole Turner\n",
      "Skipping already processed TE: Grant Calcaterra\n",
      "Skipping already processed TE: Jake Tonges\n",
      "Skipping already processed TE: Armani Rogers\n",
      "Skipping already processed TE: Greg Dulcich\n",
      "Skipping already processed TE: Teagan Quitoriano\n",
      "Skipping already processed TE: James Mitchell\n",
      "Skipping already processed TE: Andrew Ogletree\n",
      "Skipping already processed TE: Connor Heyward\n",
      "Skipping already processed TE: John FitzPatrick\n",
      "Skipping already processed TE: Ko Kieft\n",
      "Skipping already processed TE: Nick Muse\n",
      "Skipping already processed TE: Stone Smartt\n",
      "Skipping already processed TE: Lucas Krull\n",
      "Skipping already processed TE: Peyton Hendershot\n",
      "Skipping already processed TE: Trey McBride\n",
      "Skipping already processed TE: Jelani Woods\n",
      "Skipping already processed TE: Jeremy Ruckert\n",
      "Skipping already processed TE: Chigoziem Okonkwo\n",
      "Skipping already processed TE: Isaiah Likely\n",
      "Skipping already processed TE: Jake Ferguson\n",
      "Skipping already processed TE: Charlie Kolar\n",
      "Skipping already processed TE: Daniel Bellinger\n",
      "Skipping already processed TE: Cade Otton\n",
      "Skipping already processed TE: Will Mallory\n",
      "Skipping already processed TE: Zack Kuntz\n",
      "Skipping already processed TE: Brady Russell\n",
      "Skipping already processed TE: Luke Schoonmaker\n",
      "Skipping already processed TE: Darnell Washington\n",
      "Skipping already processed TE: Cameron Latu\n",
      "Skipping already processed TE: Josh Whyle\n",
      "Processing TE 54/291: Justin Shorter\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/S/ShorJu00.htm\n",
      "Skipping already processed TE: Brayden Willis\n",
      "Skipping already processed TE: Julian Hill\n",
      "Skipping already processed TE: Nate Adkins\n",
      "Skipping already processed TE: Ben Sims\n",
      "Skipping already processed TE: Dalton Kincaid\n",
      "Skipping already processed TE: Brenton Strange\n",
      "Skipping already processed TE: Tucker Kraft\n",
      "Skipping already processed TE: Elijah Higgins\n",
      "Skipping already processed TE: Payne Durham\n",
      "Skipping already processed TE: Sam LaPorta\n",
      "Skipping already processed TE: Michael Mayer\n",
      "Skipping already processed TE: Davis Allen\n",
      "Skipping already processed TE: Luke Musgrave\n",
      "Processing TE 55/291: Brock Bowers\n",
      "Data written for Brock Bowers\n",
      "Processing TE 56/291: Ja'Tavion Sanders\n",
      "Data written for Ja'Tavion Sanders\n",
      "Processing TE 57/291: Cade Stover\n",
      "Data written for Cade Stover\n",
      "Processing TE 58/291: Tanner McLachlan\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/McLaTa00.htm\n",
      "Processing TE 59/291: Jaheim Bell\n",
      "Data written for Jaheim Bell\n",
      "Processing TE 60/291: Tip Reiman\n",
      "Data written for Tip Reiman\n",
      "Processing TE 61/291: Devin Culp\n",
      "Data written for Devin Culp\n",
      "Processing TE 62/291: A.J. Barner\n",
      "Data written for A.J. Barner\n",
      "Processing TE 63/291: Erick All\n",
      "Data written for Erick All\n",
      "Processing TE 64/291: Jared Wiley\n",
      "Data written for Jared Wiley\n",
      "Processing TE 65/291: Theodore Johnson\n",
      "Data written for Theodore Johnson\n",
      "Processing TE 66/291: Ben Sinnott\n",
      "Data written for Ben Sinnott\n",
      "Data saved to game_logs_te.csv\n"
     ]
    }
   ],
   "source": [
    "# # TE game tables\n",
    "# # Not in nfl.db currently\n",
    "\n",
    "# df = pd.read_csv('./data/rosters.csv')\n",
    "\n",
    "# # Drop rows where 'pfr_id' is missing\n",
    "# df = df.dropna(subset=['pfr_id'])\n",
    "\n",
    "# # Filter for Tight Ends\n",
    "# tes = df[df['position'].str.lower() == 'te']\n",
    "\n",
    "# # Open a CSV file to write the data for tight ends\n",
    "# with open('./data/game_logs_te.csv', 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     headers_written = False  # To track if headers have been written to the file\n",
    "\n",
    "#     # Initialize a counter for progress tracking\n",
    "#     total_tes = len(tes)\n",
    "#     te_counter = 0\n",
    "\n",
    "#     # Iterate over each tight end and scrape data\n",
    "#     for index, te in tes.iterrows():\n",
    "#         te_counter += 1\n",
    "#         print(f\"Processing TE {te_counter}/{total_tes}: {te['first_name']} {te['last_name']}\")\n",
    "\n",
    "#         url = te['url']\n",
    "#         first_name = te['first_name']  # Get the player's first name\n",
    "#         last_name = te['last_name']    # Get the player's last name\n",
    "#         position = 'TE'  # Set the position to 'TE'\n",
    "\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 200:\n",
    "#             soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#             game_logs_table = soup.find('table', {'id': 'receiving_and_rushing'})\n",
    "\n",
    "#             if game_logs_table:  # Check if the table is found\n",
    "#                 header_row = game_logs_table.find('thead').find_all('tr')[-1]\n",
    "#                 data_rows = game_logs_table.find('tbody').find_all('tr')\n",
    "\n",
    "#                 if not headers_written:  # Write headers only once\n",
    "#                     headers = ['Player URL', 'Position', 'First Name', 'Last Name']  # Add new headers at the beginning\n",
    "#                     headers.extend(header.text.strip() for header in header_row.find_all('th'))  # Add existing headers\n",
    "#                     writer.writerow(headers)\n",
    "#                     headers_written = True\n",
    "\n",
    "#                 for row in data_rows:\n",
    "#                     cells = row.find_all(['th', 'td'])\n",
    "#                     data = [url, position, first_name, last_name]  # Start with additional data\n",
    "#                     data.extend(cell.text.strip() for cell in cells)  # Append scraped data\n",
    "#                     writer.writerow(data)\n",
    "#                 print(f\"Data written for {first_name} {last_name}\")\n",
    "\n",
    "#             else:\n",
    "#                 print(f\"No game logs table found for URL: {url}\")\n",
    "#         else:\n",
    "#             print(f\"Failed to retrieve URL: {url} with status code: {response.status_code}\")\n",
    "\n",
    "#         # print(f'Processed URL: {url}')  # Print the URL being processed\n",
    "#         time.sleep(2)  # Add a 3-second delay after processing each URL\n",
    "\n",
    "# print('Data saved to game_logs_te.csv')\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "\n",
    "# TE game tables\n",
    "# Not in nfl.db currently\n",
    "\n",
    "df = pd.read_csv('./data/rosters.csv')\n",
    "\n",
    "# Drop rows where 'pfr_id' is missing\n",
    "df = df.dropna(subset=['pfr_id'])\n",
    "\n",
    "# Filter for Tight Ends\n",
    "tes = df[df['position'].str.lower() == 'te']\n",
    "\n",
    "# Initialize processed players list\n",
    "processed_urls = set()\n",
    "\n",
    "# Check if the CSV file already exists to resume processing\n",
    "if os.path.exists('./data/game_logs_te.csv'):\n",
    "    with open('./data/game_logs_te.csv', 'r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the headers\n",
    "        for row in reader:\n",
    "            processed_urls.add(row[0])  # Assuming URL is the first column\n",
    "\n",
    "# Open the CSV file in append mode to add new data\n",
    "with open('./data/game_logs_te.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    headers_written = os.path.getsize('./data/game_logs_te.csv') > 0  # Check if headers are already written\n",
    "\n",
    "    # Initialize a counter for progress tracking\n",
    "    total_tes = len(tes)\n",
    "    te_counter = 0\n",
    "\n",
    "    # Iterate over each tight end and scrape data\n",
    "    for index, te in tes.iterrows():\n",
    "        url = te['url']\n",
    "        \n",
    "        # Skip if this URL has already been processed\n",
    "        if url in processed_urls:\n",
    "            print(f\"Skipping already processed TE: {te['first_name']} {te['last_name']}\")\n",
    "            continue\n",
    "\n",
    "        te_counter += 1\n",
    "        print(f\"Processing TE {te_counter}/{total_tes}: {te['first_name']} {te['last_name']}\")\n",
    "\n",
    "        first_name = te['first_name']  # Get the player's first name\n",
    "        last_name = te['last_name']    # Get the player's last name\n",
    "        position = 'TE'  # Set the position to 'TE'\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            game_logs_table = soup.find('table', {'id': 'receiving_and_rushing'})\n",
    "\n",
    "            if game_logs_table:  # Check if the table is found\n",
    "                header_row = game_logs_table.find('thead').find_all('tr')[-1]\n",
    "                data_rows = game_logs_table.find('tbody').find_all('tr')\n",
    "\n",
    "                if not headers_written:  # Write headers only once\n",
    "                    headers = ['Player URL', 'Position', 'First Name', 'Last Name']  # Add new headers at the beginning\n",
    "                    headers.extend(header.text.strip() for header in header_row.find_all('th'))  # Add existing headers\n",
    "                    writer.writerow(headers)\n",
    "                    headers_written = True\n",
    "\n",
    "                for row in data_rows:\n",
    "                    cells = row.find_all(['th', 'td'])\n",
    "                    data = [url, position, first_name, last_name]  # Start with additional data\n",
    "                    data.extend(cell.text.strip() for cell in cells)  # Append scraped data\n",
    "                    writer.writerow(data)\n",
    "                print(f\"Data written for {first_name} {last_name}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"No game logs table found for URL: {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve URL: {url} with status code: {response.status_code}\")\n",
    "\n",
    "        # Add the processed URL to the set to keep track\n",
    "        processed_urls.add(url)\n",
    "\n",
    "        time.sleep(2)  # Add a delay after processing each URL\n",
    "\n",
    "print('Data saved to game_logs_te.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ebaaa80-fd1d-4530-8d06-287beeb41e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing game logs found, starting fresh.\n",
      "Processing WR 1/582: Matthew Slater\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/S/SlatMa00.htm\n",
      "Processing WR 2/582: Quintorris Jones\n",
      "Data written for Quintorris Jones\n",
      "Processing WR 3/582: Randall Cobb\n",
      "Data written for Randall Cobb\n",
      "Processing WR 4/582: Cole Beasley\n",
      "Data written for Cole Beasley\n",
      "Processing WR 5/582: Marvin Jones\n",
      "Data written for Marvin Jones\n",
      "Processing WR 6/582: Adam Thielen\n",
      "Data written for Adam Thielen\n",
      "Processing WR 7/582: Marquise Goodwin\n",
      "Data written for Marquise Goodwin\n",
      "Processing WR 8/582: Keenan Allen\n",
      "Data written for Keenan Allen\n",
      "Processing WR 9/582: Robert Woods\n",
      "Data written for Robert Woods\n",
      "Processing WR 10/582: DeAndre Hopkins\n",
      "Data written for DeAndre Hopkins\n",
      "Processing WR 11/582: Willie Snead\n",
      "Data written for Willie Snead\n",
      "Processing WR 12/582: Odell Beckham\n",
      "Data written for Odell Beckham\n",
      "Processing WR 13/582: Brandin Cooks\n",
      "Data written for Brandin Cooks\n",
      "Processing WR 14/582: Martavis Bryant\n",
      "Data written for Martavis Bryant\n",
      "Processing WR 15/582: Davante Adams\n",
      "Data written for Davante Adams\n",
      "Processing WR 16/582: Mike Evans\n",
      "Data written for Mike Evans\n",
      "Processing WR 17/582: Allen Robinson\n",
      "Data written for Allen Robinson\n",
      "Processing WR 18/582: Amari Cooper\n",
      "Data written for Amari Cooper\n",
      "Processing WR 19/582: DeVante Parker\n",
      "Data written for DeVante Parker\n",
      "Processing WR 20/582: Nelson Agholor\n",
      "Data written for Nelson Agholor\n",
      "Processing WR 21/582: Stefon Diggs\n",
      "Data written for Stefon Diggs\n",
      "Processing WR 22/582: DeAndre Carter\n",
      "Data written for DeAndre Carter\n",
      "Processing WR 23/582: Damiere Byrd\n",
      "Data written for Damiere Byrd\n",
      "Processing WR 24/582: Jamison Crowder\n",
      "Data written for Jamison Crowder\n",
      "Processing WR 25/582: Adam Humphries\n",
      "Data written for Adam Humphries\n",
      "Processing WR 26/582: Breshad Perriman\n",
      "Data written for Breshad Perriman\n",
      "Processing WR 27/582: Christian Conley\n",
      "Data written for Christian Conley\n",
      "Processing WR 28/582: Ty Montgomery\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/MontTy01.htm\n",
      "Processing WR 29/582: Phillip Dorsett\n",
      "Data written for Phillip Dorsett\n",
      "Processing WR 30/582: Tyler Lockett\n",
      "Data written for Tyler Lockett\n",
      "Processing WR 31/582: Sterling Shepard\n",
      "Data written for Sterling Shepard\n",
      "Processing WR 32/582: Christopher Moore\n",
      "Data written for Christopher Moore\n",
      "Processing WR 33/582: Kalif Raymond\n",
      "Data written for Kalif Raymond\n",
      "Processing WR 34/582: Alex Erickson\n",
      "Data written for Alex Erickson\n",
      "Processing WR 35/582: Robbie Chosen\n",
      "Data written for Robbie Chosen\n",
      "Processing WR 36/582: Michael Thomas\n",
      "Data written for Michael Thomas\n",
      "Processing WR 37/582: Demarcus Robinson\n",
      "Data written for Demarcus Robinson\n",
      "Processing WR 38/582: Jakeem Grant\n",
      "Data written for Jakeem Grant\n",
      "Processing WR 39/582: Laquon Treadwell\n",
      "Data written for Laquon Treadwell\n",
      "Processing WR 40/582: Jaydon Mickens\n",
      "Data written for Jaydon Mickens\n",
      "Processing WR 41/582: Tyler Boyd\n",
      "Data written for Tyler Boyd\n",
      "Processing WR 42/582: Tyreek Hill\n",
      "Data written for Tyreek Hill\n",
      "Processing WR 43/582: Zachary Pascal\n",
      "Data written for Zachary Pascal\n",
      "Processing WR 44/582: Curtis Samuel\n",
      "Data written for Curtis Samuel\n",
      "Processing WR 45/582: Trent Taylor\n",
      "Data written for Trent Taylor\n",
      "Processing WR 46/582: Kendrick Bourne\n",
      "Data written for Kendrick Bourne\n",
      "Processing WR 47/582: Tim Patrick\n",
      "Data written for Tim Patrick\n",
      "Processing WR 48/582: John Ross\n",
      "Data written for John Ross\n",
      "Processing WR 49/582: Isaiah McKenzie\n",
      "Data written for Isaiah McKenzie\n",
      "Processing WR 50/582: Isaiah Ford\n",
      "Data written for Isaiah Ford\n",
      "Processing WR 51/582: Marcus Kemp\n",
      "Data written for Marcus Kemp\n",
      "Processing WR 52/582: Michael Williams\n",
      "Data written for Michael Williams\n",
      "Processing WR 53/582: Mack Hollins\n",
      "Data written for Mack Hollins\n",
      "Processing WR 54/582: Jamal Agnew\n",
      "Data written for Jamal Agnew\n",
      "Processing WR 55/582: David Moore\n",
      "Data written for David Moore\n",
      "Processing WR 56/582: Noah Brown\n",
      "Data written for Noah Brown\n",
      "Processing WR 57/582: Keelan Cole\n",
      "Data written for Keelan Cole\n",
      "Processing WR 58/582: Gregory Ward\n",
      "Data written for Gregory Ward\n",
      "Processing WR 59/582: John Smith-Schuster\n",
      "Data written for John Smith-Schuster\n",
      "Processing WR 60/582: Corey Davis\n",
      "Data written for Corey Davis\n",
      "Processing WR 61/582: Isaiah Jones\n",
      "Data written for Isaiah Jones\n",
      "Processing WR 62/582: Cooper Kupp\n",
      "Data written for Cooper Kupp\n",
      "Processing WR 63/582: Rod Godwin\n",
      "Data written for Rod Godwin\n",
      "Processing WR 64/582: Joshua Dedmon-Reynolds Reynolds\n",
      "Data written for Joshua Dedmon-Reynolds Reynolds\n",
      "Processing WR 65/582: Cam Sims\n",
      "Data written for Cam Sims\n",
      "Processing WR 66/582: Marquez Valdes-Scantling\n",
      "Data written for Marquez Valdes-Scantling\n",
      "Processing WR 67/582: Equanimeous St. Brown\n",
      "Data written for Equanimeous St. Brown\n",
      "Processing WR 68/582: Richard James\n",
      "Data written for Richard James\n",
      "Processing WR 69/582: Byron Pringle\n",
      "Data written for Byron Pringle\n",
      "Processing WR 70/582: Keith Kirkwood\n",
      "Data written for Keith Kirkwood\n",
      "Processing WR 71/582: David Quinn\n",
      "Data written for David Quinn\n",
      "Processing WR 72/582: Courtland Sutton\n",
      "Data written for Courtland Sutton\n",
      "Processing WR 73/582: Anthony Miller\n",
      "Data written for Anthony Miller\n",
      "Processing WR 74/582: Key'vantanie Coutee\n",
      "Data written for Key'vantanie Coutee\n",
      "Processing WR 75/582: Justin Watson\n",
      "Data written for Justin Watson\n",
      "Processing WR 76/582: Daurice Fountain\n",
      "Data written for Daurice Fountain\n",
      "Processing WR 77/582: Ray-Ray McCloud\n",
      "Data written for Ray-Ray McCloud\n",
      "Processing WR 78/582: Russell Gage\n",
      "Data written for Russell Gage\n",
      "Processing WR 79/582: Cedrick Wilson\n",
      "Data written for Cedrick Wilson\n",
      "Processing WR 80/582: Braxton Berrios\n",
      "Data written for Braxton Berrios\n",
      "Processing WR 81/582: Javon Wims\n",
      "Data written for Javon Wims\n",
      "Processing WR 82/582: Trent Sherfield\n",
      "Data written for Trent Sherfield\n",
      "Processing WR 83/582: Allen Lazard\n",
      "Data written for Allen Lazard\n",
      "Processing WR 84/582: Chris Lacy\n",
      "Data written for Chris Lacy\n",
      "Processing WR 85/582: Vyncint Smith\n",
      "Data written for Vyncint Smith\n",
      "Processing WR 86/582: Brandon Powell\n",
      "Data written for Brandon Powell\n",
      "Processing WR 87/582: James Washington\n",
      "Data written for James Washington\n",
      "Processing WR 88/582: Marcell Ateman\n",
      "Data written for Marcell Ateman\n",
      "Processing WR 89/582: Michael Gallup\n",
      "Data written for Michael Gallup\n",
      "Processing WR 90/582: Tre'Quan Smith\n",
      "Data written for Tre'Quan Smith\n",
      "Processing WR 91/582: Christian Kirk\n",
      "Data written for Christian Kirk\n",
      "Processing WR 92/582: Darrell Chark\n",
      "Data written for Darrell Chark\n",
      "Processing WR 93/582: Denniston Moore\n",
      "Data written for Denniston Moore\n",
      "Processing WR 94/582: Calvin Ridley\n",
      "Data written for Calvin Ridley\n",
      "Processing WR 95/582: KhaDarel Lott Hodge\n",
      "Data written for KhaDarel Lott Hodge\n",
      "Processing WR 96/582: Dante Pettis\n",
      "Data written for Dante Pettis\n",
      "Processing WR 97/582: Malik Turner\n",
      "Data written for Malik Turner\n",
      "Processing WR 98/582: Steven Sims\n",
      "Data written for Steven Sims\n",
      "Processing WR 99/582: Trinity Benson\n",
      "Data written for Trinity Benson\n",
      "Processing WR 100/582: Jakobi Meyers\n",
      "Data written for Jakobi Meyers\n",
      "Processing WR 101/582: Jalen Hurd\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/H/HurdJa00.htm\n",
      "Processing WR 102/582: Hunter Renfrow\n",
      "Data written for Hunter Renfrow\n",
      "Processing WR 103/582: Ashton Dulin\n",
      "Data written for Ashton Dulin\n",
      "Processing WR 104/582: Penny Hart\n",
      "Data written for Penny Hart\n",
      "Processing WR 105/582: Davion Davis\n",
      "Data written for Davion Davis\n",
      "Processing WR 106/582: Keelan Doss\n",
      "Data written for Keelan Doss\n",
      "Processing WR 107/582: Mecole Hardman\n",
      "Data written for Mecole Hardman\n",
      "Processing WR 108/582: Darrius Shepherd\n",
      "Data written for Darrius Shepherd\n",
      "Processing WR 109/582: Olamide Zaccheaus\n",
      "Data written for Olamide Zaccheaus\n",
      "Processing WR 110/582: Deonte Harty\n",
      "Data written for Deonte Harty\n",
      "Processing WR 111/582: Diontae Johnson\n",
      "Data written for Diontae Johnson\n",
      "Processing WR 112/582: Jose Joaquin Arcega-Whiteside\n",
      "Data written for Jose Joaquin Arcega-Whiteside\n",
      "Processing WR 113/582: KeeSean Johnson\n",
      "Data written for KeeSean Johnson\n",
      "Processing WR 114/582: Scott Miller\n",
      "Data written for Scott Miller\n",
      "Processing WR 115/582: Trenton Irwin\n",
      "Data written for Trenton Irwin\n",
      "Processing WR 116/582: Stanley Morgan\n",
      "Data written for Stanley Morgan\n",
      "Processing WR 117/582: Lil'Jordan Humphrey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written for Lil'Jordan Humphrey\n",
      "Processing WR 118/582: Jalen Guyton\n",
      "Data written for Jalen Guyton\n",
      "Processing WR 119/582: Tyron Johnson\n",
      "Data written for Tyron Johnson\n",
      "Processing WR 120/582: Malik Taylor\n",
      "Data written for Malik Taylor\n",
      "Processing WR 121/582: Greg Dortch\n",
      "Data written for Greg Dortch\n",
      "Processing WR 122/582: Jeffrey Smith\n",
      "Data written for Jeffrey Smith\n",
      "Processing WR 123/582: Andrew Isabella\n",
      "Data written for Andrew Isabella\n",
      "Processing WR 124/582: Darius Slayton\n",
      "Data written for Darius Slayton\n",
      "Processing WR 125/582: Thomas Kennedy\n",
      "Data written for Thomas Kennedy\n",
      "Processing WR 126/582: Gary Jennings\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JennGa00.htm\n",
      "Processing WR 127/582: Juwann Winfree\n",
      "Data written for Juwann Winfree\n",
      "Processing WR 128/582: Nsimba Webster\n",
      "Data written for Nsimba Webster\n",
      "Processing WR 129/582: N'Keal Harry\n",
      "Data written for N'Keal Harry\n",
      "Processing WR 130/582: Parris Campbell\n",
      "Data written for Parris Campbell\n",
      "Processing WR 131/582: DeKaylin Metcalf\n",
      "Data written for DeKaylin Metcalf\n",
      "Processing WR 132/582: Kaleb Olszewski\n",
      "Data written for Kaleb Olszewski\n",
      "Processing WR 133/582: Terry McLaurin\n",
      "Data written for Terry McLaurin\n",
      "Processing WR 134/582: Marquise Brown\n",
      "Data written for Marquise Brown\n",
      "Processing WR 135/582: Arthur Brown\n",
      "Data written for Arthur Brown\n",
      "Processing WR 136/582: Miles Boykin\n",
      "Data written for Miles Boykin\n",
      "Processing WR 137/582: Tyshun Samuel\n",
      "Data written for Tyshun Samuel\n",
      "Processing WR 138/582: Kirk Merritt\n",
      "Data written for Kirk Merritt\n",
      "Processing WR 139/582: Kendall Hinton\n",
      "Data written for Kendall Hinton\n",
      "Processing WR 140/582: Daniel Chisena\n",
      "Data written for Daniel Chisena\n",
      "Processing WR 141/582: Isaiah Zuber\n",
      "Data written for Isaiah Zuber\n",
      "Processing WR 142/582: James Proche\n",
      "Data written for James Proche\n",
      "Processing WR 143/582: Isaiah Hodgins\n",
      "Data written for Isaiah Hodgins\n",
      "Processing WR 144/582: Nicholas Westbrook-Ikhine\n",
      "Data written for Nicholas Westbrook-Ikhine\n",
      "Processing WR 145/582: Kristian Wilkerson\n",
      "Data written for Kristian Wilkerson\n",
      "Processing WR 146/582: Gabriel Davis\n",
      "Data written for Gabriel Davis\n",
      "Processing WR 147/582: Marquez Callaway\n",
      "Data written for Marquez Callaway\n",
      "Processing WR 148/582: Dezmon Patmon\n",
      "Data written for Dezmon Patmon\n",
      "Processing WR 149/582: Donovan Peoples-Jones\n",
      "Data written for Donovan Peoples-Jones\n",
      "Processing WR 150/582: Freddie Swain\n",
      "Data written for Freddie Swain\n",
      "Processing WR 151/582: Michael Pittman\n",
      "Data written for Michael Pittman\n",
      "Processing WR 152/582: Collin Johnson\n",
      "Data written for Collin Johnson\n",
      "Processing WR 153/582: Denzel Mims\n",
      "Data written for Denzel Mims\n",
      "Processing WR 154/582: Bennie Jennings\n",
      "Data written for Bennie Jennings\n",
      "Processing WR 155/582: Brandon Aiyuk\n",
      "Data written for Brandon Aiyuk\n",
      "Processing WR 156/582: Laviska Shenault\n",
      "Data written for Laviska Shenault\n",
      "Processing WR 157/582: Terrance Watkins\n",
      "Data written for Terrance Watkins\n",
      "Processing WR 158/582: Isaiah Coulter\n",
      "Data written for Isaiah Coulter\n",
      "Processing WR 159/582: Darnell Mooney\n",
      "Data written for Darnell Mooney\n",
      "Processing WR 160/582: Justin Jefferson\n",
      "Data written for Justin Jefferson\n",
      "Processing WR 161/582: Chase Claypool\n",
      "Data written for Chase Claypool\n",
      "Processing WR 162/582: Devin Duvernay\n",
      "Data written for Devin Duvernay\n",
      "Processing WR 163/582: K.J. Osborn\n",
      "Data written for K.J. Osborn\n",
      "Processing WR 164/582: Cedarian Lamb\n",
      "Data written for Cedarian Lamb\n",
      "Processing WR 165/582: Lynn Bowden\n",
      "Data written for Lynn Bowden\n",
      "Processing WR 166/582: Joseph Reed\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/R/ReedJo03.htm\n",
      "Processing WR 167/582: Jalen Reagor\n",
      "Data written for Jalen Reagor\n",
      "Processing WR 168/582: John Hightower\n",
      "Data written for John Hightower\n",
      "Processing WR 169/582: Jerry Jeudy\n",
      "Data written for Jerry Jeudy\n",
      "Processing WR 170/582: Tamaurice Higgins\n",
      "Data written for Tamaurice Higgins\n",
      "Processing WR 171/582: Kahlee Hamler\n",
      "Data written for Kahlee Hamler\n",
      "Processing WR 172/582: Vanchii Jefferson\n",
      "Data written for Vanchii Jefferson\n",
      "Processing WR 173/582: Tyler Johnson\n",
      "Data written for Tyler Johnson\n",
      "Processing WR 174/582: Tyrie Cleveland\n",
      "Data written for Tyrie Cleveland\n",
      "Processing WR 175/582: Mike Strachan\n",
      "Data written for Mike Strachan\n",
      "Processing WR 176/582: Tarik Black\n",
      "Data written for Tarik Black\n",
      "Processing WR 177/582: Tre Nixon\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/N/NixoTr01.htm\n",
      "Processing WR 178/582: Rashod Bateman\n",
      "Data written for Rashod Bateman\n",
      "Processing WR 179/582: Nico Collins\n",
      "Data written for Nico Collins\n",
      "Processing WR 180/582: Marquez Stevenson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/S/StevMa00.htm\n",
      "Processing WR 181/582: Shiyun Smith\n",
      "Data written for Shiyun Smith\n",
      "Processing WR 182/582: Jaylen Waddle\n",
      "Data written for Jaylen Waddle\n",
      "Processing WR 183/582: D'Wayne Eskridge\n",
      "Data written for D'Wayne Eskridge\n",
      "Processing WR 184/582: Dyami Brown\n",
      "Data written for Dyami Brown\n",
      "Processing WR 185/582: Dez Fitzpatrick\n",
      "Data written for Dez Fitzpatrick\n",
      "Processing WR 186/582: Tylan Wallace\n",
      "Data written for Tylan Wallace\n",
      "Processing WR 187/582: Ihmir Smith-Marsette\n",
      "Data written for Ihmir Smith-Marsette\n",
      "Processing WR 188/582: Simione Fehoko\n",
      "Data written for Simione Fehoko\n",
      "Processing WR 189/582: Cornell Powell\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/P/PoweCo00.htm\n",
      "Processing WR 190/582: Racey McMath\n",
      "Data written for Racey McMath\n",
      "Processing WR 191/582: Seth Williams\n",
      "Data written for Seth Williams\n",
      "Processing WR 192/582: Dax Milne\n",
      "Data written for Dax Milne\n",
      "Processing WR 193/582: Austin Trammell\n",
      "Data written for Austin Trammell\n",
      "Processing WR 194/582: Chatarius Atwell\n",
      "Data written for Chatarius Atwell\n",
      "Processing WR 195/582: Jalen Camp\n",
      "Data written for Jalen Camp\n",
      "Processing WR 196/582: Ben Skowronek\n",
      "Data written for Ben Skowronek\n",
      "Processing WR 197/582: Jaelon Darden\n",
      "Data written for Jaelon Darden\n",
      "Processing WR 198/582: Ja'Marr Chase\n",
      "Data written for Ja'Marr Chase\n",
      "Processing WR 199/582: Charles Saunders\n",
      "Data written for Charles Saunders\n",
      "Processing WR 200/582: DeVonta Smith\n",
      "Data written for DeVonta Smith\n",
      "Processing WR 201/582: Kadarius Toney\n",
      "Data written for Kadarius Toney\n",
      "Processing WR 202/582: Jacob Harris\n",
      "Data written for Jacob Harris\n",
      "Processing WR 203/582: Rondale Moore\n",
      "Data written for Rondale Moore\n",
      "Processing WR 204/582: Kawaan Baker\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/BakeKa00.htm\n",
      "Processing WR 205/582: Frank Darby\n",
      "Data written for Frank Darby\n",
      "Processing WR 206/582: Terrace Marshall\n",
      "Data written for Terrace Marshall\n",
      "Processing WR 207/582: Michael Bandy\n",
      "Data written for Michael Bandy\n",
      "Processing WR 208/582: Amon-Ra St. Brown\n",
      "Data written for Amon-Ra St. Brown\n",
      "Processing WR 209/582: Elijah Moore\n",
      "Data written for Elijah Moore\n",
      "Processing WR 210/582: Josh Palmer\n",
      "Data written for Josh Palmer\n",
      "Processing WR 211/582: Amari Rodgers\n",
      "Data written for Amari Rodgers\n",
      "Processing WR 212/582: Anthony Schwartz\n",
      "Data written for Anthony Schwartz\n",
      "Processing WR 213/582: Lance McCutcheon\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/McCuLa01.htm\n",
      "Processing WR 214/582: Bo Melton\n",
      "Data written for Bo Melton\n",
      "Processing WR 215/582: Dareke Young\n",
      "Data written for Dareke Young\n",
      "Processing WR 216/582: Samori Toure\n",
      "Data written for Samori Toure\n",
      "Processing WR 217/582: Britain Covey\n",
      "Data written for Britain Covey\n",
      "Processing WR 218/582: Raleigh Webb\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/W/WebbRa02.htm\n",
      "Processing WR 219/582: Justyn Ross\n",
      "Data written for Justyn Ross\n",
      "Processing WR 220/582: Drake London\n",
      "Data written for Drake London\n",
      "Processing WR 221/582: Chris Olave\n",
      "Data written for Chris Olave\n",
      "Processing WR 222/582: Jameson Williams\n",
      "Data written for Jameson Williams\n",
      "Processing WR 223/582: George Pickens\n",
      "Data written for George Pickens\n",
      "Processing WR 224/582: David Bell\n",
      "Data written for David Bell\n",
      "Processing WR 225/582: Khalil Shakir\n",
      "Data written for Khalil Shakir\n",
      "Processing WR 226/582: Montrell Washington\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/W/WashMo00.htm\n",
      "Processing WR 227/582: Kyle Philips\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written for Kyle Philips\n",
      "Processing WR 228/582: Jalen Nailor\n",
      "Data written for Jalen Nailor\n",
      "Processing WR 229/582: Braylon Sanders\n",
      "Data written for Braylon Sanders\n",
      "Processing WR 230/582: Brandon Johnson\n",
      "Data written for Brandon Johnson\n",
      "Processing WR 231/582: Jalen Virgil\n",
      "Data written for Jalen Virgil\n",
      "Processing WR 232/582: Kwamie Lassiter\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/L/LassKw20.htm\n",
      "Processing WR 233/582: Johnny Johnson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JohnJo12.htm\n",
      "Processing WR 234/582: Deven Thompkins\n",
      "Data written for Deven Thompkins\n",
      "Processing WR 235/582: Dovontavean Martin\n",
      "Data written for Dovontavean Martin\n",
      "Processing WR 236/582: Rashid Shaheed\n",
      "Data written for Rashid Shaheed\n",
      "Processing WR 237/582: Dennis Houston\n",
      "Data written for Dennis Houston\n",
      "Processing WR 238/582: Jared Bernhardt\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/BernJa00.htm\n",
      "Processing WR 239/582: John Metchie\n",
      "Data written for John Metchie\n",
      "Processing WR 240/582: Alec Pierce\n",
      "Data written for Alec Pierce\n",
      "Processing WR 241/582: Jalen Tolbert\n",
      "Data written for Jalen Tolbert\n",
      "Processing WR 242/582: Garrett Wilson\n",
      "Data written for Garrett Wilson\n",
      "Processing WR 243/582: Jahan Dotson\n",
      "Data written for Jahan Dotson\n",
      "Processing WR 244/582: Treylon Burks\n",
      "Data written for Treylon Burks\n",
      "Processing WR 245/582: Velus Jones\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JoneVe00.htm\n",
      "Processing WR 246/582: Romeo Doubs\n",
      "Data written for Romeo Doubs\n",
      "Processing WR 247/582: Danny Gray\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/G/GrayDa02.htm\n",
      "Processing WR 248/582: Calvin Austin\n",
      "Data written for Calvin Austin\n",
      "Processing WR 249/582: Skyy Moore\n",
      "Data written for Skyy Moore\n",
      "Processing WR 250/582: Erik Ezukanma\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/E/EzukEr00.htm\n",
      "Processing WR 251/582: Tyquan Thornton\n",
      "Data written for Tyquan Thornton\n",
      "Processing WR 252/582: Charles Robinson\n",
      "Data written for Charles Robinson\n",
      "Processing WR 253/582: Christian Watson\n",
      "Data written for Christian Watson\n",
      "Processing WR 254/582: Daylen Baldwin\n",
      "Data written for Daylen Baldwin\n",
      "Processing WR 255/582: Dontayvion Wicks\n",
      "Data written for Dontayvion Wicks\n",
      "Processing WR 256/582: Malik Heath\n",
      "Data written for Malik Heath\n",
      "Processing WR 257/582: Jason Brownlee\n",
      "Data written for Jason Brownlee\n",
      "Processing WR 258/582: Xavier Gipson\n",
      "Data written for Xavier Gipson\n",
      "Processing WR 259/582: Jaxon Smith-Njigba\n",
      "Data written for Jaxon Smith-Njigba\n",
      "Processing WR 260/582: Quentin Johnston\n",
      "Data written for Quentin Johnston\n",
      "Processing WR 261/582: Michael Wilson\n",
      "Data written for Michael Wilson\n",
      "Processing WR 262/582: Tre Tucker\n",
      "Data written for Tre Tucker\n",
      "Processing WR 263/582: Derius Davis\n",
      "Data written for Derius Davis\n",
      "Processing WR 264/582: Charlie Jones\n",
      "Data written for Charlie Jones\n",
      "Processing WR 265/582: Justin Shorter\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/S/ShorJu00.htm\n",
      "Processing WR 266/582: Christopher Washington\n",
      "Data written for Christopher Washington\n",
      "Processing WR 267/582: Kayshon Boutte\n",
      "Data written for Kayshon Boutte\n",
      "Processing WR 268/582: Atorian Perry\n",
      "Data written for Atorian Perry\n",
      "Processing WR 269/582: Xavier Hutchinson\n",
      "Data written for Xavier Hutchinson\n",
      "Processing WR 270/582: Andrei Iosivas\n",
      "Data written for Andrei Iosivas\n",
      "Processing WR 271/582: Demario Douglas\n",
      "Data written for Demario Douglas\n",
      "Processing WR 272/582: Antoine Green\n",
      "Data written for Antoine Green\n",
      "Processing WR 273/582: Colton Dowell\n",
      "Data written for Colton Dowell\n",
      "Processing WR 274/582: Jalen Brooks\n",
      "Data written for Jalen Brooks\n",
      "Processing WR 275/582: Ronnie Bell\n",
      "Data written for Ronnie Bell\n",
      "Processing WR 276/582: Elijah Cooks\n",
      "Data written for Elijah Cooks\n",
      "Processing WR 277/582: Jake Bobo\n",
      "Data written for Jake Bobo\n",
      "Processing WR 278/582: Rakim Jarrett\n",
      "Data written for Rakim Jarrett\n",
      "Processing WR 279/582: Mitchell Tinsley\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/T/TinsMi00.htm\n",
      "Processing WR 280/582: Shedrick Jackson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JackSh01.htm\n",
      "Processing WR 281/582: Jalin Hyatt\n",
      "Data written for Jalin Hyatt\n",
      "Processing WR 282/582: Tyler Scott\n",
      "Data written for Tyler Scott\n",
      "Processing WR 283/582: Dylan Drummond\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/D/DrumDy00.htm\n",
      "Processing WR 284/582: Kearis Jackson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JackKe02.htm\n",
      "Processing WR 285/582: Marvin Mims\n",
      "Data written for Marvin Mims\n",
      "Processing WR 286/582: Nathaniel Dell\n",
      "Data written for Nathaniel Dell\n",
      "Processing WR 287/582: Cedric Tillman\n",
      "Data written for Cedric Tillman\n",
      "Processing WR 288/582: Jordan Addison\n",
      "Data written for Jordan Addison\n",
      "Processing WR 289/582: Joshua Downs\n",
      "Data written for Joshua Downs\n",
      "Processing WR 290/582: Trey Palmer\n",
      "Data written for Trey Palmer\n",
      "Processing WR 291/582: Jonathan Mingo\n",
      "Data written for Jonathan Mingo\n",
      "Processing WR 292/582: Xavien Flowers\n",
      "Data written for Xavien Flowers\n",
      "Processing WR 293/582: Rashee Rice\n",
      "Data written for Rashee Rice\n",
      "Processing WR 294/582: Puka Nacua\n",
      "Data written for Puka Nacua\n",
      "Processing WR 295/582: Jayden Reed\n",
      "Data written for Jayden Reed\n",
      "Processing WR 296/582: Adam Thielen\n",
      "Data written for Adam Thielen\n",
      "Processing WR 297/582: Keenan Allen\n",
      "Data written for Keenan Allen\n",
      "Processing WR 298/582: Robert Woods\n",
      "Data written for Robert Woods\n",
      "Processing WR 299/582: DeAndre Hopkins\n",
      "Data written for DeAndre Hopkins\n",
      "Processing WR 300/582: Odell Beckham\n",
      "Data written for Odell Beckham\n",
      "Processing WR 301/582: Brandin Cooks\n",
      "Data written for Brandin Cooks\n",
      "Processing WR 302/582: Martavis Bryant\n",
      "Data written for Martavis Bryant\n",
      "Processing WR 303/582: Davante Adams\n",
      "Data written for Davante Adams\n",
      "Processing WR 304/582: Mike Evans\n",
      "Data written for Mike Evans\n",
      "Processing WR 305/582: Allen Robinson\n",
      "Data written for Allen Robinson\n",
      "Processing WR 306/582: Amari Cooper\n",
      "Data written for Amari Cooper\n",
      "Processing WR 307/582: DeVante Parker\n",
      "Data written for DeVante Parker\n",
      "Processing WR 308/582: Nelson Agholor\n",
      "Data written for Nelson Agholor\n",
      "Processing WR 309/582: Stefon Diggs\n",
      "Data written for Stefon Diggs\n",
      "Processing WR 310/582: DeAndre Carter\n",
      "Data written for DeAndre Carter\n",
      "Processing WR 311/582: Damiere Byrd\n",
      "Data written for Damiere Byrd\n",
      "Processing WR 312/582: Jamison Crowder\n",
      "Data written for Jamison Crowder\n",
      "Processing WR 313/582: Christian Conley\n",
      "Data written for Christian Conley\n",
      "Processing WR 314/582: Phillip Dorsett\n",
      "Data written for Phillip Dorsett\n",
      "Processing WR 315/582: Tyler Lockett\n",
      "Data written for Tyler Lockett\n",
      "Processing WR 316/582: Sterling Shepard\n",
      "Data written for Sterling Shepard\n",
      "Processing WR 317/582: Christopher Moore\n",
      "Data written for Christopher Moore\n",
      "Processing WR 318/582: Kalif Raymond\n",
      "Data written for Kalif Raymond\n",
      "Processing WR 319/582: Alex Erickson\n",
      "Data written for Alex Erickson\n",
      "Processing WR 320/582: Robbie Chosen\n",
      "Data written for Robbie Chosen\n",
      "Processing WR 321/582: Demarcus Robinson\n",
      "Data written for Demarcus Robinson\n",
      "Processing WR 322/582: Jakeem Grant\n",
      "Data written for Jakeem Grant\n",
      "Processing WR 323/582: Laquon Treadwell\n",
      "Data written for Laquon Treadwell\n",
      "Processing WR 324/582: Rashard Higgins\n",
      "Data written for Rashard Higgins\n",
      "Processing WR 325/582: Tyler Boyd\n",
      "Data written for Tyler Boyd\n",
      "Processing WR 326/582: Tyreek Hill\n",
      "Data written for Tyreek Hill\n",
      "Processing WR 327/582: Zachary Pascal\n",
      "Data written for Zachary Pascal\n",
      "Processing WR 328/582: Curtis Samuel\n",
      "Data written for Curtis Samuel\n",
      "Processing WR 329/582: Trent Taylor\n",
      "Data written for Trent Taylor\n",
      "Processing WR 330/582: Kendrick Bourne\n",
      "Data written for Kendrick Bourne\n",
      "Processing WR 331/582: Tim Patrick\n",
      "Data written for Tim Patrick\n",
      "Processing WR 332/582: John Ross\n",
      "Data written for John Ross\n",
      "Processing WR 333/582: Isaiah McKenzie\n",
      "Data written for Isaiah McKenzie\n",
      "Processing WR 334/582: Michael Williams\n",
      "Data written for Michael Williams\n",
      "Processing WR 335/582: Mack Hollins\n",
      "Data written for Mack Hollins\n",
      "Processing WR 336/582: Jamal Agnew\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written for Jamal Agnew\n",
      "Processing WR 337/582: David Moore\n",
      "Data written for David Moore\n",
      "Processing WR 338/582: Noah Brown\n",
      "Data written for Noah Brown\n",
      "Processing WR 339/582: John Smith-Schuster\n",
      "Data written for John Smith-Schuster\n",
      "Processing WR 340/582: Isaiah Jones\n",
      "Data written for Isaiah Jones\n",
      "Processing WR 341/582: Cooper Kupp\n",
      "Data written for Cooper Kupp\n",
      "Processing WR 342/582: Rod Godwin\n",
      "Data written for Rod Godwin\n",
      "Processing WR 343/582: Joshua Dedmon-Reynolds Reynolds\n",
      "Data written for Joshua Dedmon-Reynolds Reynolds\n",
      "Processing WR 344/582: Cam Sims\n",
      "Data written for Cam Sims\n",
      "Processing WR 345/582: Marquez Valdes-Scantling\n",
      "Data written for Marquez Valdes-Scantling\n",
      "Processing WR 346/582: Equanimeous St. Brown\n",
      "Data written for Equanimeous St. Brown\n",
      "Processing WR 347/582: Byron Pringle\n",
      "Data written for Byron Pringle\n",
      "Processing WR 348/582: Keith Kirkwood\n",
      "Data written for Keith Kirkwood\n",
      "Processing WR 349/582: Courtland Sutton\n",
      "Data written for Courtland Sutton\n",
      "Processing WR 350/582: Anthony Miller\n",
      "Data written for Anthony Miller\n",
      "Processing WR 351/582: Justin Watson\n",
      "Data written for Justin Watson\n",
      "Processing WR 352/582: Daurice Fountain\n",
      "Data written for Daurice Fountain\n",
      "Processing WR 353/582: Deon Cain\n",
      "Data written for Deon Cain\n",
      "Processing WR 354/582: Ray-Ray McCloud\n",
      "Data written for Ray-Ray McCloud\n",
      "Processing WR 355/582: Russell Gage\n",
      "Data written for Russell Gage\n",
      "Processing WR 356/582: Cedrick Wilson\n",
      "Data written for Cedrick Wilson\n",
      "Processing WR 357/582: Braxton Berrios\n",
      "Data written for Braxton Berrios\n",
      "Processing WR 358/582: Trent Sherfield\n",
      "Data written for Trent Sherfield\n",
      "Processing WR 359/582: Allen Lazard\n",
      "Data written for Allen Lazard\n",
      "Processing WR 360/582: Brandon Powell\n",
      "Data written for Brandon Powell\n",
      "Processing WR 361/582: Deontay Burnett\n",
      "Data written for Deontay Burnett\n",
      "Processing WR 362/582: Alexander McGough\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/McGoAl00.htm\n",
      "Processing WR 363/582: Michael Gallup\n",
      "Data written for Michael Gallup\n",
      "Processing WR 364/582: Tre'Quan Smith\n",
      "Data written for Tre'Quan Smith\n",
      "Processing WR 365/582: Christian Kirk\n",
      "Data written for Christian Kirk\n",
      "Processing WR 366/582: Darrell Chark\n",
      "Data written for Darrell Chark\n",
      "Processing WR 367/582: Denniston Moore\n",
      "Data written for Denniston Moore\n",
      "Processing WR 368/582: Calvin Ridley\n",
      "Data written for Calvin Ridley\n",
      "Processing WR 369/582: KhaDarel Lott Hodge\n",
      "Data written for KhaDarel Lott Hodge\n",
      "Processing WR 370/582: Dante Pettis\n",
      "Data written for Dante Pettis\n",
      "Processing WR 371/582: Malik Turner\n",
      "Data written for Malik Turner\n",
      "Processing WR 372/582: Steven Sims\n",
      "Data written for Steven Sims\n",
      "Processing WR 373/582: Jakobi Meyers\n",
      "Data written for Jakobi Meyers\n",
      "Processing WR 374/582: Ashton Dulin\n",
      "Data written for Ashton Dulin\n",
      "Processing WR 375/582: Davion Davis\n",
      "Data written for Davion Davis\n",
      "Processing WR 376/582: Mecole Hardman\n",
      "Data written for Mecole Hardman\n",
      "Processing WR 377/582: Olamide Zaccheaus\n",
      "Data written for Olamide Zaccheaus\n",
      "Processing WR 378/582: Deonte Harty\n",
      "Data written for Deonte Harty\n",
      "Processing WR 379/582: Diontae Johnson\n",
      "Data written for Diontae Johnson\n",
      "Processing WR 380/582: Kelvin Harmon\n",
      "Data written for Kelvin Harmon\n",
      "Processing WR 381/582: Scott Miller\n",
      "Data written for Scott Miller\n",
      "Processing WR 382/582: Trenton Irwin\n",
      "Data written for Trenton Irwin\n",
      "Processing WR 383/582: Stanley Morgan\n",
      "Data written for Stanley Morgan\n",
      "Processing WR 384/582: Lil'Jordan Humphrey\n",
      "Data written for Lil'Jordan Humphrey\n",
      "Processing WR 385/582: Jalen Guyton\n",
      "Data written for Jalen Guyton\n",
      "Processing WR 386/582: Tyron Johnson\n",
      "Data written for Tyron Johnson\n",
      "Processing WR 387/582: Malik Taylor\n",
      "Data written for Malik Taylor\n",
      "Processing WR 388/582: Greg Dortch\n",
      "Data written for Greg Dortch\n",
      "Processing WR 389/582: Jeffrey Smith\n",
      "Data written for Jeffrey Smith\n",
      "Processing WR 390/582: Andrew Isabella\n",
      "Data written for Andrew Isabella\n",
      "Processing WR 391/582: Hakeem Butler\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/ButlHa00.htm\n",
      "Processing WR 392/582: Darius Slayton\n",
      "Data written for Darius Slayton\n",
      "Processing WR 393/582: Thomas Kennedy\n",
      "Data written for Thomas Kennedy\n",
      "Processing WR 394/582: Juwann Winfree\n",
      "Data written for Juwann Winfree\n",
      "Processing WR 395/582: Nsimba Webster\n",
      "Data written for Nsimba Webster\n",
      "Processing WR 396/582: Parris Campbell\n",
      "Data written for Parris Campbell\n",
      "Processing WR 397/582: DeKaylin Metcalf\n",
      "Data written for DeKaylin Metcalf\n",
      "Processing WR 398/582: Kaleb Olszewski\n",
      "Data written for Kaleb Olszewski\n",
      "Processing WR 399/582: Terry McLaurin\n",
      "Data written for Terry McLaurin\n",
      "Processing WR 400/582: Marquise Brown\n",
      "Data written for Marquise Brown\n",
      "Processing WR 401/582: Arthur Brown\n",
      "Data written for Arthur Brown\n",
      "Processing WR 402/582: Miles Boykin\n",
      "Data written for Miles Boykin\n",
      "Processing WR 403/582: Tyshun Samuel\n",
      "Data written for Tyshun Samuel\n",
      "Processing WR 404/582: Austin Mack\n",
      "Data written for Austin Mack\n",
      "Processing WR 405/582: Daniel Chisena\n",
      "Data written for Daniel Chisena\n",
      "Processing WR 406/582: James Proche\n",
      "Data written for James Proche\n",
      "Processing WR 407/582: Isaiah Hodgins\n",
      "Data written for Isaiah Hodgins\n",
      "Processing WR 408/582: Nicholas Westbrook-Ikhine\n",
      "Data written for Nicholas Westbrook-Ikhine\n",
      "Processing WR 409/582: Kristian Wilkerson\n",
      "Data written for Kristian Wilkerson\n",
      "Processing WR 410/582: Gabriel Davis\n",
      "Data written for Gabriel Davis\n",
      "Processing WR 411/582: Marquez Callaway\n",
      "Data written for Marquez Callaway\n",
      "Processing WR 412/582: Donovan Peoples-Jones\n",
      "Data written for Donovan Peoples-Jones\n",
      "Processing WR 413/582: Freddie Swain\n",
      "Data written for Freddie Swain\n",
      "Processing WR 414/582: Michael Pittman\n",
      "Data written for Michael Pittman\n",
      "Processing WR 415/582: Collin Johnson\n",
      "Data written for Collin Johnson\n",
      "Processing WR 416/582: Denzel Mims\n",
      "Data written for Denzel Mims\n",
      "Processing WR 417/582: Bennie Jennings\n",
      "Data written for Bennie Jennings\n",
      "Processing WR 418/582: Brandon Aiyuk\n",
      "Data written for Brandon Aiyuk\n",
      "Processing WR 419/582: Laviska Shenault\n",
      "Data written for Laviska Shenault\n",
      "Processing WR 420/582: Terrance Watkins\n",
      "Data written for Terrance Watkins\n",
      "Processing WR 421/582: Quintez Cephus\n",
      "Data written for Quintez Cephus\n",
      "Processing WR 422/582: Darnell Mooney\n",
      "Data written for Darnell Mooney\n",
      "Processing WR 423/582: Justin Jefferson\n",
      "Data written for Justin Jefferson\n",
      "Processing WR 424/582: Chase Claypool\n",
      "Data written for Chase Claypool\n",
      "Processing WR 425/582: Devin Duvernay\n",
      "Data written for Devin Duvernay\n",
      "Processing WR 426/582: K.J. Osborn\n",
      "Data written for K.J. Osborn\n",
      "Processing WR 427/582: Cedarian Lamb\n",
      "Data written for Cedarian Lamb\n",
      "Processing WR 428/582: Jalen Reagor\n",
      "Data written for Jalen Reagor\n",
      "Processing WR 429/582: Jerry Jeudy\n",
      "Data written for Jerry Jeudy\n",
      "Processing WR 430/582: Tamaurice Higgins\n",
      "Data written for Tamaurice Higgins\n",
      "Processing WR 431/582: Kahlee Hamler\n",
      "Data written for Kahlee Hamler\n",
      "Processing WR 432/582: Vanchii Jefferson\n",
      "Data written for Vanchii Jefferson\n",
      "Processing WR 433/582: Tyler Johnson\n",
      "Data written for Tyler Johnson\n",
      "Processing WR 434/582: Tyrie Cleveland\n",
      "Data written for Tyrie Cleveland\n",
      "Processing WR 435/582: Mike Strachan\n",
      "Data written for Mike Strachan\n",
      "Processing WR 436/582: Tarik Black\n",
      "Data written for Tarik Black\n",
      "Processing WR 437/582: Rashod Bateman\n",
      "Data written for Rashod Bateman\n",
      "Processing WR 438/582: Nico Collins\n",
      "Data written for Nico Collins\n",
      "Processing WR 439/582: Jaylen Waddle\n",
      "Data written for Jaylen Waddle\n",
      "Processing WR 440/582: D'Wayne Eskridge\n",
      "Data written for D'Wayne Eskridge\n",
      "Processing WR 441/582: Dyami Brown\n",
      "Data written for Dyami Brown\n",
      "Processing WR 442/582: Dez Fitzpatrick\n",
      "Data written for Dez Fitzpatrick\n",
      "Processing WR 443/582: Tylan Wallace\n",
      "Data written for Tylan Wallace\n",
      "Processing WR 444/582: Ihmir Smith-Marsette\n",
      "Data written for Ihmir Smith-Marsette\n",
      "Processing WR 445/582: Simione Fehoko\n",
      "Data written for Simione Fehoko\n",
      "Processing WR 446/582: Cornell Powell\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/P/PoweCo00.htm\n",
      "Processing WR 447/582: Racey McMath\n",
      "Data written for Racey McMath\n",
      "Processing WR 448/582: Seth Williams\n",
      "Data written for Seth Williams\n",
      "Processing WR 449/582: Dax Milne\n",
      "Data written for Dax Milne\n",
      "Processing WR 450/582: Austin Trammell\n",
      "Data written for Austin Trammell\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing WR 451/582: Chatarius Atwell\n",
      "Data written for Chatarius Atwell\n",
      "Processing WR 452/582: Jalen Camp\n",
      "Data written for Jalen Camp\n",
      "Processing WR 453/582: Ben Skowronek\n",
      "Data written for Ben Skowronek\n",
      "Processing WR 454/582: Jaelon Darden\n",
      "Data written for Jaelon Darden\n",
      "Processing WR 455/582: Ja'Marr Chase\n",
      "Data written for Ja'Marr Chase\n",
      "Processing WR 456/582: DeVonta Smith\n",
      "Data written for DeVonta Smith\n",
      "Processing WR 457/582: Kadarius Toney\n",
      "Data written for Kadarius Toney\n",
      "Processing WR 458/582: Jacob Harris\n",
      "Data written for Jacob Harris\n",
      "Processing WR 459/582: Rondale Moore\n",
      "Data written for Rondale Moore\n",
      "Processing WR 460/582: Kawaan Baker\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/BakeKa00.htm\n",
      "Processing WR 461/582: Terrace Marshall\n",
      "Data written for Terrace Marshall\n",
      "Processing WR 462/582: Michael Bandy\n",
      "Data written for Michael Bandy\n",
      "Processing WR 463/582: Amon-Ra St. Brown\n",
      "Data written for Amon-Ra St. Brown\n",
      "Processing WR 464/582: Elijah Moore\n",
      "Data written for Elijah Moore\n",
      "Processing WR 465/582: Josh Palmer\n",
      "Data written for Josh Palmer\n",
      "Processing WR 466/582: Anthony Schwartz\n",
      "Data written for Anthony Schwartz\n",
      "Processing WR 467/582: Lance McCutcheon\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/McCuLa01.htm\n",
      "Processing WR 468/582: Bo Melton\n",
      "Data written for Bo Melton\n",
      "Processing WR 469/582: Dareke Young\n",
      "Data written for Dareke Young\n",
      "Processing WR 470/582: Samori Toure\n",
      "Data written for Samori Toure\n",
      "Processing WR 471/582: Britain Covey\n",
      "Data written for Britain Covey\n",
      "Processing WR 472/582: Raleigh Webb\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/W/WebbRa02.htm\n",
      "Processing WR 473/582: Justyn Ross\n",
      "Data written for Justyn Ross\n",
      "Processing WR 474/582: Drake London\n",
      "Data written for Drake London\n",
      "Processing WR 475/582: Chris Olave\n",
      "Data written for Chris Olave\n",
      "Processing WR 476/582: Jameson Williams\n",
      "Data written for Jameson Williams\n",
      "Processing WR 477/582: George Pickens\n",
      "Data written for George Pickens\n",
      "Processing WR 478/582: David Bell\n",
      "Data written for David Bell\n",
      "Processing WR 479/582: Khalil Shakir\n",
      "Data written for Khalil Shakir\n",
      "Processing WR 480/582: Montrell Washington\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/W/WashMo00.htm\n",
      "Processing WR 481/582: Kyle Philips\n",
      "Data written for Kyle Philips\n",
      "Processing WR 482/582: Jalen Nailor\n",
      "Data written for Jalen Nailor\n",
      "Processing WR 483/582: Braylon Sanders\n",
      "Data written for Braylon Sanders\n",
      "Processing WR 484/582: Brandon Johnson\n",
      "Data written for Brandon Johnson\n",
      "Processing WR 485/582: Jalen Virgil\n",
      "Data written for Jalen Virgil\n",
      "Processing WR 486/582: Kwamie Lassiter\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/L/LassKw20.htm\n",
      "Processing WR 487/582: Johnny Johnson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JohnJo12.htm\n",
      "Processing WR 488/582: Deven Thompkins\n",
      "Data written for Deven Thompkins\n",
      "Processing WR 489/582: Dovontavean Martin\n",
      "Data written for Dovontavean Martin\n",
      "Processing WR 490/582: Rashid Shaheed\n",
      "Data written for Rashid Shaheed\n",
      "Processing WR 491/582: Dennis Houston\n",
      "Data written for Dennis Houston\n",
      "Processing WR 492/582: John Metchie\n",
      "Data written for John Metchie\n",
      "Processing WR 493/582: Alec Pierce\n",
      "Data written for Alec Pierce\n",
      "Processing WR 494/582: Jalen Tolbert\n",
      "Data written for Jalen Tolbert\n",
      "Processing WR 495/582: Garrett Wilson\n",
      "Data written for Garrett Wilson\n",
      "Processing WR 496/582: Jahan Dotson\n",
      "Data written for Jahan Dotson\n",
      "Processing WR 497/582: Treylon Burks\n",
      "Data written for Treylon Burks\n",
      "Processing WR 498/582: Romeo Doubs\n",
      "Data written for Romeo Doubs\n",
      "Processing WR 499/582: Danny Gray\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/G/GrayDa02.htm\n",
      "Processing WR 500/582: Calvin Austin\n",
      "Data written for Calvin Austin\n",
      "Processing WR 501/582: Skyy Moore\n",
      "Data written for Skyy Moore\n",
      "Processing WR 502/582: Erik Ezukanma\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/E/EzukEr00.htm\n",
      "Processing WR 503/582: Tyquan Thornton\n",
      "Data written for Tyquan Thornton\n",
      "Processing WR 504/582: Charles Robinson\n",
      "Data written for Charles Robinson\n",
      "Processing WR 505/582: Christian Watson\n",
      "Data written for Christian Watson\n",
      "Processing WR 506/582: Daylen Baldwin\n",
      "Data written for Daylen Baldwin\n",
      "Processing WR 507/582: Dontayvion Wicks\n",
      "Data written for Dontayvion Wicks\n",
      "Processing WR 508/582: Malik Heath\n",
      "Data written for Malik Heath\n",
      "Processing WR 509/582: Jason Brownlee\n",
      "Data written for Jason Brownlee\n",
      "Processing WR 510/582: Xavier Gipson\n",
      "Data written for Xavier Gipson\n",
      "Processing WR 511/582: Jaxon Smith-Njigba\n",
      "Data written for Jaxon Smith-Njigba\n",
      "Processing WR 512/582: Quentin Johnston\n",
      "Data written for Quentin Johnston\n",
      "Processing WR 513/582: Michael Wilson\n",
      "Data written for Michael Wilson\n",
      "Processing WR 514/582: Tre Tucker\n",
      "Data written for Tre Tucker\n",
      "Processing WR 515/582: Derius Davis\n",
      "Data written for Derius Davis\n",
      "Processing WR 516/582: Charlie Jones\n",
      "Data written for Charlie Jones\n",
      "Processing WR 517/582: Christopher Washington\n",
      "Data written for Christopher Washington\n",
      "Processing WR 518/582: Kayshon Boutte\n",
      "Data written for Kayshon Boutte\n",
      "Processing WR 519/582: Atorian Perry\n",
      "Data written for Atorian Perry\n",
      "Processing WR 520/582: Xavier Hutchinson\n",
      "Data written for Xavier Hutchinson\n",
      "Processing WR 521/582: Andrei Iosivas\n",
      "Data written for Andrei Iosivas\n",
      "Processing WR 522/582: Demario Douglas\n",
      "Data written for Demario Douglas\n",
      "Processing WR 523/582: Antoine Green\n",
      "Data written for Antoine Green\n",
      "Processing WR 524/582: Colton Dowell\n",
      "Data written for Colton Dowell\n",
      "Processing WR 525/582: Jalen Brooks\n",
      "Data written for Jalen Brooks\n",
      "Processing WR 526/582: Ronnie Bell\n",
      "Data written for Ronnie Bell\n",
      "Processing WR 527/582: Elijah Cooks\n",
      "Data written for Elijah Cooks\n",
      "Processing WR 528/582: Jake Bobo\n",
      "Data written for Jake Bobo\n",
      "Processing WR 529/582: Rakim Jarrett\n",
      "Data written for Rakim Jarrett\n",
      "Processing WR 530/582: Mitchell Tinsley\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/T/TinsMi00.htm\n",
      "Processing WR 531/582: Shedrick Jackson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JackSh01.htm\n",
      "Processing WR 532/582: Malik Cunningham\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/C/CunnMa00.htm\n",
      "Processing WR 533/582: Jalin Hyatt\n",
      "Data written for Jalin Hyatt\n",
      "Processing WR 534/582: Tyler Scott\n",
      "Data written for Tyler Scott\n",
      "Processing WR 535/582: Dylan Drummond\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/D/DrumDy00.htm\n",
      "Processing WR 536/582: Kearis Jackson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JackKe02.htm\n",
      "Processing WR 537/582: Marvin Mims\n",
      "Data written for Marvin Mims\n",
      "Processing WR 538/582: Nathaniel Dell\n",
      "Data written for Nathaniel Dell\n",
      "Processing WR 539/582: Cedric Tillman\n",
      "Data written for Cedric Tillman\n",
      "Processing WR 540/582: Jordan Addison\n",
      "Data written for Jordan Addison\n",
      "Processing WR 541/582: Joshua Downs\n",
      "Data written for Joshua Downs\n",
      "Processing WR 542/582: Trey Palmer\n",
      "Data written for Trey Palmer\n",
      "Processing WR 543/582: Jonathan Mingo\n",
      "Data written for Jonathan Mingo\n",
      "Processing WR 544/582: Xavien Flowers\n",
      "Data written for Xavien Flowers\n",
      "Processing WR 545/582: Rashee Rice\n",
      "Data written for Rashee Rice\n",
      "Processing WR 546/582: Puka Nacua\n",
      "Data written for Puka Nacua\n",
      "Processing WR 547/582: Jayden Reed\n",
      "Data written for Jayden Reed\n",
      "Processing WR 548/582: Johnny Wilson\n",
      "Data written for Johnny Wilson\n",
      "Processing WR 549/582: Malik Nabers\n",
      "Data written for Malik Nabers\n",
      "Processing WR 550/582: Xavier Legette\n",
      "Data written for Xavier Legette\n",
      "Processing WR 551/582: Luke McCaffrey\n",
      "Data written for Luke McCaffrey\n",
      "Processing WR 552/582: Jacob Cowing\n",
      "Data written for Jacob Cowing\n",
      "Processing WR 553/582: Jamari Thrash\n",
      "Data written for Jamari Thrash\n",
      "Processing WR 554/582: Jerrod Means\n",
      "Data written for Jerrod Means\n",
      "Processing WR 555/582: Jha'quan Jackson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JackJh00.htm\n",
      "Processing WR 556/582: Casey Washington\n",
      "Data written for Casey Washington\n",
      "Processing WR 557/582: Tejhaun Palmer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No game logs table found for URL: https://www.pro-football-reference.com/players/P/PalmTe00.htm\n",
      "Processing WR 558/582: Ryan Flournoy\n",
      "Data written for Ryan Flournoy\n",
      "Processing WR 559/582: Brenden Rice\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/R/RiceBr00.htm\n",
      "Processing WR 560/582: Devaughn Vele\n",
      "Data written for Devaughn Vele\n",
      "Processing WR 561/582: Cornelius Johnson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JohnCo02.htm\n",
      "Processing WR 562/582: Roman Wilson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/W/WilsRo02.htm\n",
      "Processing WR 563/582: Ainias Smith\n",
      "Data written for Ainias Smith\n",
      "Processing WR 564/582: Jordan Whittington\n",
      "Data written for Jordan Whittington\n",
      "Processing WR 565/582: Devontez Walker\n",
      "Data written for Devontez Walker\n",
      "Processing WR 566/582: Jermaine Burton\n",
      "Data written for Jermaine Burton\n",
      "Processing WR 567/582: Anthony Gould\n",
      "Data written for Anthony Gould\n",
      "Processing WR 568/582: Marvin Harrison\n",
      "Data written for Marvin Harrison\n",
      "Processing WR 569/582: Javon Baker\n",
      "Data written for Javon Baker\n",
      "Processing WR 570/582: Jalen McMillan\n",
      "Data written for Jalen McMillan\n",
      "Processing WR 571/582: Troy Franklin\n",
      "Data written for Troy Franklin\n",
      "Processing WR 572/582: Malik Washington\n",
      "Data written for Malik Washington\n",
      "Processing WR 573/582: Tahj Washington\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/W/WashTa00.htm\n",
      "Processing WR 574/582: Adonai Mitchell\n",
      "Data written for Adonai Mitchell\n",
      "Processing WR 575/582: Brian Thomas\n",
      "Data written for Brian Thomas\n",
      "Processing WR 576/582: Xavier Worthy\n",
      "Data written for Xavier Worthy\n",
      "Processing WR 577/582: Keon Coleman\n",
      "Data written for Keon Coleman\n",
      "Processing WR 578/582: Ja'Lynn Polk\n",
      "Data written for Ja'Lynn Polk\n",
      "Processing WR 579/582: Andrew McConkey\n",
      "Data written for Andrew McConkey\n",
      "Processing WR 580/582: Ricky Pearsall\n",
      "Data written for Ricky Pearsall\n",
      "Processing WR 581/582: Rome Odunze\n",
      "Data written for Rome Odunze\n",
      "Processing WR 582/582: Malachi Corley\n",
      "Data written for Malachi Corley\n",
      "Data saved to game_logs_wr.csv\n"
     ]
    }
   ],
   "source": [
    "# WR game tables\n",
    "# Not in nfl.db currently\n",
    "\n",
    "# ### Loop through wide receivers\n",
    "# df = pd.read_csv('./data/rosters.csv')\n",
    "\n",
    "# # Drop rows where 'pfr_id' is missing\n",
    "# df = df.dropna(subset=['pfr_id'])\n",
    "\n",
    "# # Filter for Wide Receivers ('WR' or 'wr')\n",
    "# wrs = df[df['position'].str.lower() == 'wr']\n",
    "\n",
    "# # Open a CSV file to write the data for wide receivers\n",
    "# with open('./data/game_logs_wr.csv', 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     headers_written = False  # To track if headers have been written to the file\n",
    "\n",
    "#     # Initialize a counter for progress tracking\n",
    "#     total_wrs = len(wrs)\n",
    "#     wr_counter = 0\n",
    "\n",
    "#     # Iterate over each wide receiver and scrape data\n",
    "#     for index, wr in wrs.iterrows():\n",
    "#         wr_counter += 1\n",
    "#         url = wr['url']\n",
    "#         print(f\"Processing WR {wr_counter}/{total_wrs}: {wr['first_name']} {wr['last_name']}\")\n",
    "#         first_name = wr['first_name']  # Get the player's first name\n",
    "#         last_name = wr['last_name']    # Get the player's last name\n",
    "#         position = 'WR'  # Set the position to 'WR'\n",
    "\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 200:\n",
    "#             soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#             game_logs_table = soup.find('table', {'id': 'receiving_and_rushing'})\n",
    "\n",
    "#             if game_logs_table:  # Check if the table is found\n",
    "#                 header_row = game_logs_table.find('thead').find_all('tr')[-1]\n",
    "#                 data_rows = game_logs_table.find('tbody').find_all('tr')\n",
    "\n",
    "#                 if not headers_written:  # Write headers only once\n",
    "#                     headers = ['Player URL', 'Position', 'First Name', 'Last Name']  # Add new headers at the beginning\n",
    "#                     headers.extend(header.text.strip() for header in header_row.find_all('th'))  # Add existing headers\n",
    "#                     writer.writerow(headers)\n",
    "#                     headers_written = True\n",
    "\n",
    "#                 for row in data_rows:\n",
    "#                     cells = row.find_all(['th', 'td'])\n",
    "#                     data = [url, position, first_name, last_name]  # Start with additional data\n",
    "#                     data.extend(cell.text.strip() for cell in cells)  # Append scraped data\n",
    "#                     writer.writerow(data)\n",
    "#                 print(f\"Data written for {first_name} {last_name}\")\n",
    "\n",
    "#             else:\n",
    "#                 print(f\"No game logs table found for URL: {url}\")\n",
    "#         else:\n",
    "#             print(f\"Failed to retrieve URL: {url} with status code: {response.status_code}\")\n",
    "\n",
    "#         # print(f'Processed URL: {url}')  # Print the URL being processed\n",
    "#         time.sleep(2)  # Add a 3-second delay after processing each URL\n",
    "\n",
    "# print('Data saved to game_logs_wr.csv')\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/rosters.csv')\n",
    "\n",
    "# Drop rows where 'pfr_id' is missing\n",
    "df = df.dropna(subset=['pfr_id'])\n",
    "\n",
    "# Filter for Wide Receivers ('WR' or 'wr')\n",
    "wrs = df[df['position'].str.lower() == 'wr']\n",
    "\n",
    "# Read existing game logs if the file already exists\n",
    "existing_data = pd.DataFrame()\n",
    "try:\n",
    "    existing_data = pd.read_csv('./data/game_logs_wr.csv')\n",
    "    # Extract unique player URLs from the existing data\n",
    "    existing_urls = existing_data['Player URL'].unique()\n",
    "    # Filter out already processed wide receivers\n",
    "    wrs = wrs[~wrs['url'].isin(existing_urls)]\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, proceed with all wide receivers\n",
    "    print('No existing game logs found, starting fresh.')\n",
    "\n",
    "# Open a CSV file to write the data for wide receivers\n",
    "with open('./data/game_logs_wr.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    headers_written = False  # To track if headers have been written to the file\n",
    "\n",
    "    # Initialize a counter for progress tracking\n",
    "    total_wrs = len(wrs)\n",
    "    wr_counter = 0\n",
    "\n",
    "    # Iterate over each wide receiver and scrape data\n",
    "    for index, wr in wrs.iterrows():\n",
    "        wr_counter += 1\n",
    "        url = wr['url']\n",
    "        print(f\"Processing WR {wr_counter}/{total_wrs}: {wr['first_name']} {wr['last_name']}\")\n",
    "        first_name = wr['first_name']  # Get the player's first name\n",
    "        last_name = wr['last_name']    # Get the player's last name\n",
    "        position = 'WR'  # Set the position to 'WR'\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            game_logs_table = soup.find('table', {'id': 'receiving_and_rushing'})\n",
    "\n",
    "            if game_logs_table:  # Check if the table is found\n",
    "                header_row = game_logs_table.find('thead').find_all('tr')[-1]\n",
    "                data_rows = game_logs_table.find('tbody').find_all('tr')\n",
    "\n",
    "                if not headers_written:  # Write headers only once\n",
    "                    headers = ['Player URL', 'Position', 'First Name', 'Last Name']  # Add new headers at the beginning\n",
    "                    headers.extend(header.text.strip() for header in header_row.find_all('th'))  # Add existing headers\n",
    "                    writer.writerow(headers)\n",
    "                    headers_written = True\n",
    "\n",
    "                for row in data_rows:\n",
    "                    cells = row.find_all(['th', 'td'])\n",
    "                    data = [url, position, first_name, last_name]  # Start with additional data\n",
    "                    data.extend(cell.text.strip() for cell in cells)  # Append scraped data\n",
    "                    writer.writerow(data)\n",
    "                print(f\"Data written for {first_name} {last_name}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"No game logs table found for URL: {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve URL: {url} with status code: {response.status_code}\")\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "print('Data saved to game_logs_wr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fca3762-ecf8-40a4-8a9f-45985e57ed0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RB 1/360: Taiwan Jones\n",
      "Data written for Taiwan Jones\n",
      "Processing RB 2/360: Brandon Bolden\n",
      "Data written for Brandon Bolden\n",
      "Processing RB 3/360: Kyle Juszczyk\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JuszKy00.htm\n",
      "Processing RB 4/360: Latavius Murray\n",
      "Data written for Latavius Murray\n",
      "Processing RB 5/360: Cordarrelle Patterson\n",
      "Data written for Cordarrelle Patterson\n",
      "Processing RB 6/360: Damien Williams\n",
      "Data written for Damien Williams\n",
      "Processing RB 7/360: Jerick McKinnon\n",
      "Data written for Jerick McKinnon\n",
      "Processing RB 8/360: Michael Burton\n",
      "Data written for Michael Burton\n",
      "Processing RB 9/360: Dominique Mostert\n",
      "Data written for Dominique Mostert\n",
      "Processing RB 10/360: Ameer Abdullah\n",
      "Data written for Ameer Abdullah\n",
      "Processing RB 11/360: Melvin Gordon\n",
      "Data written for Melvin Gordon\n",
      "Processing RB 12/360: Dwayne Washington\n",
      "Data written for Dwayne Washington\n",
      "Processing RB 13/360: Derrick Henry\n",
      "Data written for Derrick Henry\n",
      "Processing RB 14/360: Cortez Ham\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/H/HamxC.00.htm\n",
      "Processing RB 15/360: Jonathan Williams\n",
      "Data written for Jonathan Williams\n",
      "Processing RB 16/360: Ezekiel Elliott\n",
      "Data written for Ezekiel Elliott\n",
      "Processing RB 17/360: Kenyan Drake\n",
      "Data written for Kenyan Drake\n",
      "Processing RB 18/360: Christian McCaffrey\n",
      "Data written for Christian McCaffrey\n",
      "Processing RB 19/360: Aaron Jones\n",
      "Data written for Aaron Jones\n",
      "Processing RB 20/360: Alex Armah\n",
      "Data written for Alex Armah\n",
      "Processing RB 21/360: Matthew Breida\n",
      "Data written for Matthew Breida\n",
      "Processing RB 22/360: Patrick Ricard\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/R/RicaPa00.htm\n",
      "Processing RB 23/360: Samaje Perine\n",
      "Data written for Samaje Perine\n",
      "Processing RB 24/360: James Conner\n",
      "Data written for James Conner\n",
      "Processing RB 25/360: Tarik Cohen\n",
      "Data written for Tarik Cohen\n",
      "Processing RB 26/360: Brian Hill\n",
      "Data written for Brian Hill\n",
      "Processing RB 27/360: Austin Ekeler\n",
      "Data written for Austin Ekeler\n",
      "Processing RB 28/360: Corey Clement\n",
      "Data written for Corey Clement\n",
      "Processing RB 29/360: Oluwadare Ogunbowale\n",
      "Data written for Oluwadare Ogunbowale\n",
      "Processing RB 30/360: Leonard Fournette\n",
      "Data written for Leonard Fournette\n",
      "Processing RB 31/360: Dalvin Cook\n",
      "Data written for Dalvin Cook\n",
      "Processing RB 32/360: Joe Mixon\n",
      "Data written for Joe Mixon\n",
      "Processing RB 33/360: Alvin Kamara\n",
      "Data written for Alvin Kamara\n",
      "Processing RB 34/360: Kareem Hunt\n",
      "Data written for Kareem Hunt\n",
      "Processing RB 35/360: D'Onta Foreman\n",
      "Data written for D'Onta Foreman\n",
      "Processing RB 36/360: Jamaal Williams\n",
      "Data written for Jamaal Williams\n",
      "Processing RB 37/360: Marlon Mack\n",
      "Data written for Marlon Mack\n",
      "Processing RB 38/360: Jeremy McNichols\n",
      "Data written for Jeremy McNichols\n",
      "Processing RB 39/360: Jeffery Wilson\n",
      "Data written for Jeffery Wilson\n",
      "Processing RB 40/360: Jason Cabinda\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/C/CabiJa00.htm\n",
      "Processing RB 41/360: Gus Edwards\n",
      "Data written for Gus Edwards\n",
      "Processing RB 42/360: Michael Boone\n",
      "Data written for Michael Boone\n",
      "Processing RB 43/360: Darrel Williams\n",
      "Data written for Darrel Williams\n",
      "Processing RB 44/360: Nyheim Hines\n",
      "Data written for Nyheim Hines\n",
      "Processing RB 45/360: Jordan Wilkins\n",
      "Data written for Jordan Wilkins\n",
      "Processing RB 46/360: Boston Scott\n",
      "Data written for Boston Scott\n",
      "Processing RB 47/360: Charles Bawden\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/BawdNi00.htm\n",
      "Processing RB 48/360: Justin Jackson\n",
      "Data written for Justin Jackson\n",
      "Processing RB 49/360: Godwin Igwebuike\n",
      "Data written for Godwin Igwebuike\n",
      "Processing RB 50/360: Chase Edmonds\n",
      "Data written for Chase Edmonds\n",
      "Processing RB 51/360: Rashaad Penny\n",
      "Data written for Rashaad Penny\n",
      "Processing RB 52/360: Nicholas Chubb\n",
      "Data written for Nicholas Chubb\n",
      "Processing RB 53/360: John Kelly\n",
      "Data written for John Kelly\n",
      "Processing RB 54/360: Ronald Jones\n",
      "Data written for Ronald Jones\n",
      "Processing RB 55/360: Royce Freeman\n",
      "Data written for Royce Freeman\n",
      "Processing RB 56/360: Saquon Barkley\n",
      "Data written for Saquon Barkley\n",
      "Processing RB 57/360: Sony Michel\n",
      "Data written for Sony Michel\n",
      "Processing RB 58/360: Alexander Mattison\n",
      "Data written for Alexander Mattison\n",
      "Processing RB 59/360: Justice Hill\n",
      "Data written for Justice Hill\n",
      "Processing RB 60/360: Darwin Thompson\n",
      "Data written for Darwin Thompson\n",
      "Processing RB 61/360: Devine Ozigbo\n",
      "Data written for Devine Ozigbo\n",
      "Processing RB 62/360: Austin Walter\n",
      "Data written for Austin Walter\n",
      "Processing RB 63/360: Benny Snell\n",
      "Data written for Benny Snell\n",
      "Processing RB 64/360: Miles Sanders\n",
      "Data written for Miles Sanders\n",
      "Processing RB 65/360: Devin Singletary\n",
      "Data written for Devin Singletary\n",
      "Processing RB 66/360: Tony Pollard\n",
      "Data written for Tony Pollard\n",
      "Processing RB 67/360: Qadree Ollison\n",
      "Data written for Qadree Ollison\n",
      "Processing RB 68/360: Trayveon Williams\n",
      "Data written for Trayveon Williams\n",
      "Processing RB 69/360: Myles Gaskin\n",
      "Data written for Myles Gaskin\n",
      "Processing RB 70/360: Patrick Laird\n",
      "Data written for Patrick Laird\n",
      "Processing RB 71/360: Ty Johnson\n",
      "Data written for Ty Johnson\n",
      "Processing RB 72/360: Craig Reynolds\n",
      "Data written for Craig Reynolds\n",
      "Processing RB 73/360: Travis Homer\n",
      "Data written for Travis Homer\n",
      "Processing RB 74/360: D'Ernest Johnson\n",
      "Data written for D'Ernest Johnson\n",
      "Processing RB 75/360: Damien Harris\n",
      "Data written for Damien Harris\n",
      "Processing RB 76/360: Darrell Henderson\n",
      "Data written for Darrell Henderson\n",
      "Processing RB 77/360: David Montgomery\n",
      "Data written for David Montgomery\n",
      "Processing RB 78/360: Joshua Jacobs\n",
      "Data written for Joshua Jacobs\n",
      "Processing RB 79/360: Jakob Johnson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JohnJa12.htm\n",
      "Processing RB 80/360: Jamycal Hasty\n",
      "Data written for Jamycal Hasty\n",
      "Processing RB 81/360: James Robinson\n",
      "Data written for James Robinson\n",
      "Processing RB 82/360: Tony Jones\n",
      "Data written for Tony Jones\n",
      "Processing RB 83/360: Jonathan Ward\n",
      "Data written for Jonathan Ward\n",
      "Processing RB 84/360: Salvon Ahmed\n",
      "Data written for Salvon Ahmed\n",
      "Processing RB 85/360: J.J. Taylor\n",
      "Data written for J.J. Taylor\n",
      "Processing RB 86/360: Rico Dowdle\n",
      "Data written for Rico Dowdle\n",
      "Processing RB 87/360: Jkaylin Dobbins\n",
      "Data written for Jkaylin Dobbins\n",
      "Processing RB 88/360: Jonathan Taylor\n",
      "Data written for Jonathan Taylor\n",
      "Processing RB 89/360: Zaccheus Moss\n",
      "Data written for Zaccheus Moss\n",
      "Processing RB 90/360: Algiers Dillon\n",
      "Data written for Algiers Dillon\n",
      "Processing RB 91/360: La'Mical Perine\n",
      "Data written for La'Mical Perine\n",
      "Processing RB 92/360: D'Andre Swift\n",
      "Data written for D'Andre Swift\n",
      "Processing RB 93/360: Jason Huntley\n",
      "Data written for Jason Huntley\n",
      "Processing RB 94/360: Darrynton Evans\n",
      "Data written for Darrynton Evans\n",
      "Processing RB 95/360: Antonio Gibson\n",
      "Data written for Antonio Gibson\n",
      "Processing RB 96/360: Anthony McFarland\n",
      "Data written for Anthony McFarland\n",
      "Processing RB 97/360: Clyde Edwards-Helaire\n",
      "Data written for Clyde Edwards-Helaire\n",
      "Processing RB 98/360: Joshua Kelley\n",
      "Data written for Joshua Kelley\n",
      "Processing RB 99/360: Eno Benjamin\n",
      "Data written for Eno Benjamin\n",
      "Processing RB 100/360: Cam Akers\n",
      "Data written for Cam Akers\n",
      "Processing RB 101/360: Demetrius Dallas\n",
      "Data written for Demetrius Dallas\n",
      "Processing RB 102/360: Ke'Shawn Vaughn\n",
      "Data written for Ke'Shawn Vaughn\n",
      "Processing RB 103/360: Ty'Son Williams\n",
      "Data written for Ty'Son Williams\n",
      "Processing RB 104/360: Deon Jackson\n",
      "Data written for Deon Jackson\n",
      "Processing RB 105/360: Spencer Brown\n",
      "Data written for Spencer Brown\n",
      "Processing RB 106/360: Chuba Hubbard\n",
      "Data written for Chuba Hubbard\n",
      "Processing RB 107/360: Elijah Mitchell\n",
      "Data written for Elijah Mitchell\n",
      "Processing RB 108/360: Gary Brightwell\n",
      "Data written for Gary Brightwell\n",
      "Processing RB 109/360: Gerrid Doaks\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/D/DoakGe00.htm\n",
      "Processing RB 110/360: Nate McCrary\n",
      "Data written for Nate McCrary\n",
      "Processing RB 111/360: Larry Rountree\n",
      "Data written for Larry Rountree\n",
      "Processing RB 112/360: Demetric Felton\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/F/FeltDe00.htm\n",
      "Processing RB 113/360: Jermar Jefferson\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written for Jermar Jefferson\n",
      "Processing RB 114/360: Jaret Patterson\n",
      "Data written for Jaret Patterson\n",
      "Processing RB 115/360: Kene Nwangwu\n",
      "Data written for Kene Nwangwu\n",
      "Processing RB 116/360: Chris Evans\n",
      "Data written for Chris Evans\n",
      "Processing RB 117/360: Jake Funk\n",
      "Data written for Jake Funk\n",
      "Processing RB 118/360: Rhamondre Stevenson\n",
      "Data written for Rhamondre Stevenson\n",
      "Processing RB 119/360: Najee Harris\n",
      "Data written for Najee Harris\n",
      "Processing RB 120/360: Khalil Herbert\n",
      "Data written for Khalil Herbert\n",
      "Processing RB 121/360: Kenneth Gainwell\n",
      "Data written for Kenneth Gainwell\n",
      "Processing RB 122/360: Michael Carter\n",
      "Data written for Michael Carter\n",
      "Processing RB 123/360: Avery Williams\n",
      "Data written for Avery Williams\n",
      "Processing RB 124/360: Travis Etienne\n",
      "Data written for Travis Etienne\n",
      "Processing RB 125/360: Trey Sermon\n",
      "Data written for Trey Sermon\n",
      "Processing RB 126/360: Javonte Williams\n",
      "Data written for Javonte Williams\n",
      "Processing RB 127/360: Tyler Badie\n",
      "Data written for Tyler Badie\n",
      "Processing RB 128/360: Trestan Ebner\n",
      "Data written for Trestan Ebner\n",
      "Processing RB 129/360: Zonovan Knight\n",
      "Data written for Zonovan Knight\n",
      "Processing RB 130/360: Isiah Pacheco\n",
      "Data written for Isiah Pacheco\n",
      "Processing RB 131/360: Jaylen Warren\n",
      "Data written for Jaylen Warren\n",
      "Processing RB 132/360: James Cook\n",
      "Data written for James Cook\n",
      "Processing RB 133/360: Rachaad White\n",
      "Data written for Rachaad White\n",
      "Processing RB 134/360: Dameon Pierce\n",
      "Data written for Dameon Pierce\n",
      "Processing RB 135/360: Tyler Allgeier\n",
      "Data written for Tyler Allgeier\n",
      "Processing RB 136/360: Jarod Conner\n",
      "Data written for Jarod Conner\n",
      "Processing RB 137/360: Jerome Ford\n",
      "Data written for Jerome Ford\n",
      "Processing RB 138/360: Ty Chandler\n",
      "Data written for Ty Chandler\n",
      "Processing RB 139/360: Kevin Harris\n",
      "Data written for Kevin Harris\n",
      "Processing RB 140/360: Keaontay Ingram\n",
      "Data written for Keaontay Ingram\n",
      "Processing RB 141/360: Brittain Brown\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/BrowBr06.htm\n",
      "Processing RB 142/360: Alexander Horvath\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/H/HorvZa00.htm\n",
      "Processing RB 143/360: Raheem Blackshear\n",
      "Data written for Raheem Blackshear\n",
      "Processing RB 144/360: Troy Hairston\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/H/HairTr00.htm\n",
      "Processing RB 145/360: Jordan Mason\n",
      "Data written for Jordan Mason\n",
      "Processing RB 146/360: Ronnie Rivers\n",
      "Data written for Ronnie Rivers\n",
      "Processing RB 147/360: Malik Davis\n",
      "Data written for Malik Davis\n",
      "Processing RB 148/360: Julius Chestnut\n",
      "Data written for Julius Chestnut\n",
      "Processing RB 149/360: Hassan Haskins\n",
      "Data written for Hassan Haskins\n",
      "Processing RB 150/360: Brian Robinson\n",
      "Data written for Brian Robinson\n",
      "Processing RB 151/360: Tyrion Davis-Price\n",
      "Data written for Tyrion Davis-Price\n",
      "Processing RB 152/360: Kyren Williams\n",
      "Data written for Kyren Williams\n",
      "Processing RB 153/360: Zamir White\n",
      "Data written for Zamir White\n",
      "Processing RB 154/360: Isaiah Spiller\n",
      "Data written for Isaiah Spiller\n",
      "Processing RB 155/360: Pierre Strong\n",
      "Data written for Pierre Strong\n",
      "Processing RB 156/360: Breece Hall\n",
      "Data written for Breece Hall\n",
      "Processing RB 157/360: Kenneth Walker\n",
      "Data written for Kenneth Walker\n",
      "Processing RB 158/360: Israel Abanikanda\n",
      "Data written for Israel Abanikanda\n",
      "Processing RB 159/360: Eric Gray\n",
      "Data written for Eric Gray\n",
      "Processing RB 160/360: Evan Hull\n",
      "Data written for Evan Hull\n",
      "Processing RB 161/360: Arthur Nichols\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/N/NichLe00.htm\n",
      "Processing RB 162/360: Keaton Mitchell\n",
      "Data written for Keaton Mitchell\n",
      "Processing RB 163/360: Owen Wright\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/W/WrigOw00.htm\n",
      "Processing RB 164/360: Bijan Robinson\n",
      "Data written for Bijan Robinson\n",
      "Processing RB 165/360: Kendre Miller\n",
      "Data written for Kendre Miller\n",
      "Processing RB 166/360: Cartavious Bigsby\n",
      "Data written for Cartavious Bigsby\n",
      "Processing RB 167/360: Chase Brown\n",
      "Data written for Chase Brown\n",
      "Processing RB 168/360: Christopher Rodriguez\n",
      "Data written for Christopher Rodriguez\n",
      "Processing RB 169/360: Christopher Vaughn\n",
      "Data written for Christopher Vaughn\n",
      "Processing RB 170/360: Kenny McIntosh\n",
      "Data written for Kenny McIntosh\n",
      "Processing RB 171/360: Derek Parish\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/P/PariDe00.htm\n",
      "Processing RB 172/360: Mohamed Ibrahim\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/I/IbraMo00.htm\n",
      "Processing RB 173/360: Jordan Mims\n",
      "Data written for Jordan Mims\n",
      "Processing RB 174/360: Emari Demercado\n",
      "Data written for Emari Demercado\n",
      "Processing RB 175/360: Hunter Luepke\n",
      "Data written for Hunter Luepke\n",
      "Processing RB 176/360: Jaleel McLaughlin\n",
      "Data written for Jaleel McLaughlin\n",
      "Processing RB 177/360: Emanuel Wilson\n",
      "Data written for Emanuel Wilson\n",
      "Processing RB 178/360: Elijah Dotson\n",
      "Data written for Elijah Dotson\n",
      "Processing RB 179/360: SaRodorick Thompson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/T/ThomSa00.htm\n",
      "Processing RB 180/360: Sean Tucker\n",
      "Data written for Sean Tucker\n",
      "Processing RB 181/360: Roschon Johnson\n",
      "Data written for Roschon Johnson\n",
      "Processing RB 182/360: Tyjae Spears\n",
      "Data written for Tyjae Spears\n",
      "Processing RB 183/360: Devon Achane\n",
      "Data written for Devon Achane\n",
      "Processing RB 184/360: DeWayne McBride\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/McBrDe00.htm\n",
      "Processing RB 185/360: Zachary Evans\n",
      "Data written for Zachary Evans\n",
      "Processing RB 186/360: Jahmyr Gibbs\n",
      "Data written for Jahmyr Gibbs\n",
      "Processing RB 187/360: Zach Charbonnet\n",
      "Data written for Zach Charbonnet\n",
      "Processing RB 188/360: Kyle Juszczyk\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JuszKy00.htm\n",
      "Processing RB 189/360: Cordarrelle Patterson\n",
      "Skipping Cordarrelle Patterson, data already exists.\n",
      "Processing RB 190/360: Michael Burton\n",
      "Skipping Michael Burton, data already exists.\n",
      "Processing RB 191/360: Dominique Mostert\n",
      "Skipping Dominique Mostert, data already exists.\n",
      "Processing RB 192/360: Ameer Abdullah\n",
      "Skipping Ameer Abdullah, data already exists.\n",
      "Processing RB 193/360: Derrick Henry\n",
      "Skipping Derrick Henry, data already exists.\n",
      "Processing RB 194/360: Cortez Ham\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/H/HamxC.00.htm\n",
      "Processing RB 195/360: Ezekiel Elliott\n",
      "Skipping Ezekiel Elliott, data already exists.\n",
      "Processing RB 196/360: Christian McCaffrey\n",
      "Skipping Christian McCaffrey, data already exists.\n",
      "Processing RB 197/360: Aaron Jones\n",
      "Skipping Aaron Jones, data already exists.\n",
      "Processing RB 198/360: Matthew Breida\n",
      "Skipping Matthew Breida, data already exists.\n",
      "Processing RB 199/360: Patrick Ricard\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/R/RicaPa00.htm\n",
      "Processing RB 200/360: Samaje Perine\n",
      "Skipping Samaje Perine, data already exists.\n",
      "Processing RB 201/360: James Conner\n",
      "Skipping James Conner, data already exists.\n",
      "Processing RB 202/360: Tarik Cohen\n",
      "Skipping Tarik Cohen, data already exists.\n",
      "Processing RB 203/360: Austin Ekeler\n",
      "Skipping Austin Ekeler, data already exists.\n",
      "Processing RB 204/360: Oluwadare Ogunbowale\n",
      "Skipping Oluwadare Ogunbowale, data already exists.\n",
      "Processing RB 205/360: Dalvin Cook\n",
      "Skipping Dalvin Cook, data already exists.\n",
      "Processing RB 206/360: Joe Mixon\n",
      "Skipping Joe Mixon, data already exists.\n",
      "Processing RB 207/360: Alvin Kamara\n",
      "Skipping Alvin Kamara, data already exists.\n",
      "Processing RB 208/360: Kareem Hunt\n",
      "Skipping Kareem Hunt, data already exists.\n",
      "Processing RB 209/360: D'Onta Foreman\n",
      "Skipping D'Onta Foreman, data already exists.\n",
      "Processing RB 210/360: Jamaal Williams\n",
      "Skipping Jamaal Williams, data already exists.\n",
      "Processing RB 211/360: Jeremy McNichols\n",
      "Skipping Jeremy McNichols, data already exists.\n",
      "Processing RB 212/360: Jeffery Wilson\n",
      "Skipping Jeffery Wilson, data already exists.\n",
      "Processing RB 213/360: Gus Edwards\n",
      "Skipping Gus Edwards, data already exists.\n",
      "Processing RB 214/360: Michael Boone\n",
      "Skipping Michael Boone, data already exists.\n",
      "Processing RB 215/360: Nyheim Miller-Hines\n",
      "Skipping Nyheim Miller-Hines, data already exists.\n",
      "Processing RB 216/360: Boston Scott\n",
      "Skipping Boston Scott, data already exists.\n",
      "Processing RB 217/360: Charles Bawden\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/BawdNi00.htm\n",
      "Processing RB 218/360: Chase Edmonds\n",
      "Skipping Chase Edmonds, data already exists.\n",
      "Processing RB 219/360: Rashaad Penny\n",
      "Skipping Rashaad Penny, data already exists.\n",
      "Processing RB 220/360: Nicholas Chubb\n",
      "Skipping Nicholas Chubb, data already exists.\n",
      "Processing RB 221/360: John Kelly\n",
      "Skipping John Kelly, data already exists.\n",
      "Processing RB 222/360: Royce Freeman\n",
      "Skipping Royce Freeman, data already exists.\n",
      "Processing RB 223/360: Saquon Barkley\n",
      "Skipping Saquon Barkley, data already exists.\n",
      "Processing RB 224/360: Alexander Mattison\n",
      "Skipping Alexander Mattison, data already exists.\n",
      "Processing RB 225/360: Justice Hill\n",
      "Skipping Justice Hill, data already exists.\n",
      "Processing RB 226/360: Miles Sanders\n",
      "Skipping Miles Sanders, data already exists.\n",
      "Processing RB 227/360: Devin Singletary\n",
      "Skipping Devin Singletary, data already exists.\n",
      "Processing RB 228/360: Tony Pollard\n",
      "Skipping Tony Pollard, data already exists.\n",
      "Processing RB 229/360: Trayveon Williams\n",
      "Skipping Trayveon Williams, data already exists.\n",
      "Processing RB 230/360: Myles Gaskin\n",
      "Skipping Myles Gaskin, data already exists.\n",
      "Processing RB 231/360: Ty Johnson\n",
      "Skipping Ty Johnson, data already exists.\n",
      "Processing RB 232/360: Craig Reynolds\n",
      "Skipping Craig Reynolds, data already exists.\n",
      "Processing RB 233/360: Travis Homer\n",
      "Skipping Travis Homer, data already exists.\n",
      "Processing RB 234/360: D'Ernest Johnson\n",
      "Skipping D'Ernest Johnson, data already exists.\n",
      "Processing RB 235/360: David Montgomery\n",
      "Skipping David Montgomery, data already exists.\n",
      "Processing RB 236/360: Joshua Jacobs\n",
      "Skipping Joshua Jacobs, data already exists.\n",
      "Processing RB 237/360: Jakob Johnson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JohnJa12.htm\n",
      "Processing RB 238/360: Jamycal Hasty\n",
      "Skipping Jamycal Hasty, data already exists.\n",
      "Processing RB 239/360: James Robinson\n",
      "Skipping James Robinson, data already exists.\n",
      "Processing RB 240/360: Tony Jones\n",
      "Skipping Tony Jones, data already exists.\n",
      "Processing RB 241/360: Jonathan Ward\n",
      "Skipping Jonathan Ward, data already exists.\n",
      "Processing RB 242/360: Salvon Ahmed\n",
      "Skipping Salvon Ahmed, data already exists.\n",
      "Processing RB 243/360: J.J. Taylor\n",
      "Skipping J.J. Taylor, data already exists.\n",
      "Processing RB 244/360: Rico Dowdle\n",
      "Skipping Rico Dowdle, data already exists.\n",
      "Processing RB 245/360: Jkaylin Dobbins\n",
      "Skipping Jkaylin Dobbins, data already exists.\n",
      "Processing RB 246/360: Jonathan Taylor\n",
      "Skipping Jonathan Taylor, data already exists.\n",
      "Processing RB 247/360: Zaccheus Moss\n",
      "Skipping Zaccheus Moss, data already exists.\n",
      "Processing RB 248/360: Algiers Dillon\n",
      "Skipping Algiers Dillon, data already exists.\n",
      "Processing RB 249/360: La'Mical Perine\n",
      "Skipping La'Mical Perine, data already exists.\n",
      "Processing RB 250/360: D'Andre Swift\n",
      "Skipping D'Andre Swift, data already exists.\n",
      "Processing RB 251/360: Darrynton Evans\n",
      "Skipping Darrynton Evans, data already exists.\n",
      "Processing RB 252/360: Antonio Gibson\n",
      "Skipping Antonio Gibson, data already exists.\n",
      "Processing RB 253/360: Anthony McFarland\n",
      "Skipping Anthony McFarland, data already exists.\n",
      "Processing RB 254/360: Clyde Edwards-Helaire\n",
      "Skipping Clyde Edwards-Helaire, data already exists.\n",
      "Processing RB 255/360: Joshua Kelley\n",
      "Skipping Joshua Kelley, data already exists.\n",
      "Processing RB 256/360: Cam Akers\n",
      "Skipping Cam Akers, data already exists.\n",
      "Processing RB 257/360: Demetrius Dallas\n",
      "Skipping Demetrius Dallas, data already exists.\n",
      "Processing RB 258/360: Ke'Shawn Vaughn\n",
      "Skipping Ke'Shawn Vaughn, data already exists.\n",
      "Processing RB 259/360: Deon Jackson\n",
      "Skipping Deon Jackson, data already exists.\n",
      "Processing RB 260/360: Spencer Brown\n",
      "Skipping Spencer Brown, data already exists.\n",
      "Processing RB 261/360: Chuba Hubbard\n",
      "Skipping Chuba Hubbard, data already exists.\n",
      "Processing RB 262/360: Elijah Mitchell\n",
      "Skipping Elijah Mitchell, data already exists.\n",
      "Processing RB 263/360: Gary Brightwell\n",
      "Skipping Gary Brightwell, data already exists.\n",
      "Processing RB 264/360: Nate McCrary\n",
      "Skipping Nate McCrary, data already exists.\n",
      "Processing RB 265/360: Demetric Felton\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/F/FeltDe00.htm\n",
      "Processing RB 266/360: Jermar Jefferson\n",
      "Skipping Jermar Jefferson, data already exists.\n",
      "Processing RB 267/360: Jaret Patterson\n",
      "Skipping Jaret Patterson, data already exists.\n",
      "Processing RB 268/360: Kene Nwangwu\n",
      "Skipping Kene Nwangwu, data already exists.\n",
      "Processing RB 269/360: Chris Evans\n",
      "Skipping Chris Evans, data already exists.\n",
      "Processing RB 270/360: Jake Funk\n",
      "Skipping Jake Funk, data already exists.\n",
      "Processing RB 271/360: Rhamondre Stevenson\n",
      "Skipping Rhamondre Stevenson, data already exists.\n",
      "Processing RB 272/360: Najee Harris\n",
      "Skipping Najee Harris, data already exists.\n",
      "Processing RB 273/360: Khalil Herbert\n",
      "Skipping Khalil Herbert, data already exists.\n",
      "Processing RB 274/360: Kenneth Gainwell\n",
      "Skipping Kenneth Gainwell, data already exists.\n",
      "Processing RB 275/360: Michael Carter\n",
      "Skipping Michael Carter, data already exists.\n",
      "Processing RB 276/360: Avery Williams\n",
      "Skipping Avery Williams, data already exists.\n",
      "Processing RB 277/360: Travis Etienne\n",
      "Skipping Travis Etienne, data already exists.\n",
      "Processing RB 278/360: Trey Sermon\n",
      "Skipping Trey Sermon, data already exists.\n",
      "Processing RB 279/360: Javonte Williams\n",
      "Skipping Javonte Williams, data already exists.\n",
      "Processing RB 280/360: Tyler Badie\n",
      "Skipping Tyler Badie, data already exists.\n",
      "Processing RB 281/360: Zonovan Knight\n",
      "Skipping Zonovan Knight, data already exists.\n",
      "Processing RB 282/360: Isiah Pacheco\n",
      "Skipping Isiah Pacheco, data already exists.\n",
      "Processing RB 283/360: Jaylen Warren\n",
      "Skipping Jaylen Warren, data already exists.\n",
      "Processing RB 284/360: James Cook\n",
      "Skipping James Cook, data already exists.\n",
      "Processing RB 285/360: Rachaad White\n",
      "Skipping Rachaad White, data already exists.\n",
      "Processing RB 286/360: Dameon Pierce\n",
      "Skipping Dameon Pierce, data already exists.\n",
      "Processing RB 287/360: Tyler Allgeier\n",
      "Skipping Tyler Allgeier, data already exists.\n",
      "Processing RB 288/360: Jarod Conner\n",
      "Skipping Jarod Conner, data already exists.\n",
      "Processing RB 289/360: Jerome Ford\n",
      "Skipping Jerome Ford, data already exists.\n",
      "Processing RB 290/360: Ty Chandler\n",
      "Skipping Ty Chandler, data already exists.\n",
      "Processing RB 291/360: Kevin Harris\n",
      "Skipping Kevin Harris, data already exists.\n",
      "Processing RB 292/360: Keaontay Ingram\n",
      "Skipping Keaontay Ingram, data already exists.\n",
      "Processing RB 293/360: Brittain Brown\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/B/BrowBr06.htm\n",
      "Processing RB 294/360: Alexander Horvath\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/H/HorvZa00.htm\n",
      "Processing RB 295/360: Raheem Blackshear\n",
      "Skipping Raheem Blackshear, data already exists.\n",
      "Processing RB 296/360: Troy Hairston\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/H/HairTr00.htm\n",
      "Processing RB 297/360: Jordan Mason\n",
      "Skipping Jordan Mason, data already exists.\n",
      "Processing RB 298/360: Ronnie Rivers\n",
      "Skipping Ronnie Rivers, data already exists.\n",
      "Processing RB 299/360: Malik Davis\n",
      "Skipping Malik Davis, data already exists.\n",
      "Processing RB 300/360: Julius Chestnut\n",
      "Skipping Julius Chestnut, data already exists.\n",
      "Processing RB 301/360: Hassan Haskins\n",
      "Skipping Hassan Haskins, data already exists.\n",
      "Processing RB 302/360: Velus Jones\n",
      "Data written for Velus Jones\n",
      "Processing RB 303/360: Brian Robinson\n",
      "Skipping Brian Robinson, data already exists.\n",
      "Processing RB 304/360: Tyrion Davis-Price\n",
      "Skipping Tyrion Davis-Price, data already exists.\n",
      "Processing RB 305/360: Kyren Williams\n",
      "Skipping Kyren Williams, data already exists.\n",
      "Processing RB 306/360: Zamir White\n",
      "Skipping Zamir White, data already exists.\n",
      "Processing RB 307/360: Isaiah Spiller\n",
      "Skipping Isaiah Spiller, data already exists.\n",
      "Processing RB 308/360: Pierre Strong\n",
      "Skipping Pierre Strong, data already exists.\n",
      "Processing RB 309/360: Breece Hall\n",
      "Skipping Breece Hall, data already exists.\n",
      "Processing RB 310/360: Kenneth Walker\n",
      "Skipping Kenneth Walker, data already exists.\n",
      "Processing RB 311/360: Israel Abanikanda\n",
      "Skipping Israel Abanikanda, data already exists.\n",
      "Processing RB 312/360: Eric Gray\n",
      "Skipping Eric Gray, data already exists.\n",
      "Processing RB 313/360: Evan Hull\n",
      "Skipping Evan Hull, data already exists.\n",
      "Processing RB 314/360: Arthur Nichols\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No game logs table found for URL: https://www.pro-football-reference.com/players/N/NichLe00.htm\n",
      "Processing RB 315/360: Keaton Mitchell\n",
      "Skipping Keaton Mitchell, data already exists.\n",
      "Processing RB 316/360: Owen Wright\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/W/WrigOw00.htm\n",
      "Processing RB 317/360: Bijan Robinson\n",
      "Skipping Bijan Robinson, data already exists.\n",
      "Processing RB 318/360: Kendre Miller\n",
      "Skipping Kendre Miller, data already exists.\n",
      "Processing RB 319/360: Cartavious Bigsby\n",
      "Skipping Cartavious Bigsby, data already exists.\n",
      "Processing RB 320/360: Chase Brown\n",
      "Skipping Chase Brown, data already exists.\n",
      "Processing RB 321/360: Christopher Rodriguez\n",
      "Skipping Christopher Rodriguez, data already exists.\n",
      "Processing RB 322/360: Scott Matlock\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/MatlSc00.htm\n",
      "Processing RB 323/360: Christopher Vaughn\n",
      "Skipping Christopher Vaughn, data already exists.\n",
      "Processing RB 324/360: Kenny McIntosh\n",
      "Skipping Kenny McIntosh, data already exists.\n",
      "Processing RB 325/360: Mohamed Ibrahim\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/I/IbraMo00.htm\n",
      "Processing RB 326/360: Jordan Mims\n",
      "Skipping Jordan Mims, data already exists.\n",
      "Processing RB 327/360: Emari Demercado\n",
      "Skipping Emari Demercado, data already exists.\n",
      "Processing RB 328/360: Hunter Luepke\n",
      "Skipping Hunter Luepke, data already exists.\n",
      "Processing RB 329/360: Jaleel McLaughlin\n",
      "Skipping Jaleel McLaughlin, data already exists.\n",
      "Processing RB 330/360: Emanuel Wilson\n",
      "Skipping Emanuel Wilson, data already exists.\n",
      "Processing RB 331/360: Elijah Dotson\n",
      "Skipping Elijah Dotson, data already exists.\n",
      "Processing RB 332/360: SaRodorick Thompson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/T/ThomSa00.htm\n",
      "Processing RB 333/360: Sean Tucker\n",
      "Skipping Sean Tucker, data already exists.\n",
      "Processing RB 334/360: Roschon Johnson\n",
      "Skipping Roschon Johnson, data already exists.\n",
      "Processing RB 335/360: Tyjae Spears\n",
      "Skipping Tyjae Spears, data already exists.\n",
      "Processing RB 336/360: Devon Achane\n",
      "Skipping Devon Achane, data already exists.\n",
      "Processing RB 337/360: DeWayne McBride\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/M/McBrDe00.htm\n",
      "Processing RB 338/360: Zachary Evans\n",
      "Skipping Zachary Evans, data already exists.\n",
      "Processing RB 339/360: Jahmyr Gibbs\n",
      "Skipping Jahmyr Gibbs, data already exists.\n",
      "Processing RB 340/360: Zach Charbonnet\n",
      "Skipping Zach Charbonnet, data already exists.\n",
      "Processing RB 341/360: Jonathon Brooks\n",
      "Data written for Jonathon Brooks\n",
      "Processing RB 342/360: Mar'Keise Irving\n",
      "Data written for Mar'Keise Irving\n",
      "Processing RB 343/360: Isaac Guerendo\n",
      "Data written for Isaac Guerendo\n",
      "Processing RB 344/360: Sione Vaki\n",
      "Data written for Sione Vaki\n",
      "Processing RB 345/360: Audric Estime\n",
      "Data written for Audric Estime\n",
      "Processing RB 346/360: Tyrone Tracy\n",
      "Data written for Tyrone Tracy\n",
      "Processing RB 347/360: Keilan Robinson\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/R/RobiKe03.htm\n",
      "Processing RB 348/360: Kimani Vidal\n",
      "Data written for Kimani Vidal\n",
      "Processing RB 349/360: Jase McClellan\n",
      "Data written for Jase McClellan\n",
      "Processing RB 350/360: Jawhar Jordan\n",
      "No game logs table found for URL: https://www.pro-football-reference.com/players/J/JordJa00.htm\n",
      "Processing RB 351/360: Dylan Laube\n",
      "Data written for Dylan Laube\n",
      "Processing RB 352/360: Blake Corum\n",
      "Data written for Blake Corum\n",
      "Processing RB 353/360: Will Shipley\n",
      "Data written for Will Shipley\n",
      "Processing RB 354/360: Braelon Allen\n",
      "Data written for Braelon Allen\n",
      "Processing RB 355/360: Rasheen Ali\n",
      "Data written for Rasheen Ali\n",
      "Processing RB 356/360: Isaiah Davis\n",
      "Data written for Isaiah Davis\n",
      "Processing RB 357/360: MarShawn Lloyd\n",
      "Data written for MarShawn Lloyd\n",
      "Processing RB 358/360: Jaylen Wright\n",
      "Data written for Jaylen Wright\n",
      "Processing RB 359/360: Re'Mahn Davis\n",
      "Data written for Re'Mahn Davis\n",
      "Processing RB 360/360: Trey Benson\n",
      "Data written for Trey Benson\n",
      "Data saved to game_logs_rb.csv\n"
     ]
    }
   ],
   "source": [
    "# # RB game tables\n",
    "# # Not in nfl.db currently\n",
    "\n",
    "# df = pd.read_csv('./data/rosters.csv')\n",
    "\n",
    "# # Drop rows where 'pfr_id' is missing\n",
    "# df = df.dropna(subset=['pfr_id'])\n",
    "\n",
    "# # Filter for Running Backs ('RB' or 'rb')\n",
    "# rbs = df[df['position'].str.lower() == 'rb']\n",
    "\n",
    "# # Open a CSV file to write the data for running backs\n",
    "# with open('./data/game_logs_rb.csv', 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     headers_written = False  # To track if headers have been written to the file\n",
    "\n",
    "#     # Initialize a counter for progress tracking\n",
    "#     total_rbs = len(rbs)\n",
    "#     rb_counter = 0\n",
    "\n",
    "#     # Iterate over each running back and scrape data\n",
    "#     for index, rb in rbs.iterrows():\n",
    "#         rb_counter += 1\n",
    "#         url = rb['url']\n",
    "#         print(f\"Processing RB {rb_counter}/{total_rbs}: {rb['first_name']} {rb['last_name']}\")\n",
    "#         first_name = rb['first_name']  # Get the player's first name\n",
    "#         last_name = rb['last_name']    # Get the player's last name\n",
    "#         position = 'RB'  # Set the position to 'RB'\n",
    "\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 200:\n",
    "#             soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#             game_logs_table = soup.find('table', {'id': 'rushing_and_receiving'})\n",
    "\n",
    "#             if game_logs_table:  # Check if the table is found\n",
    "#                 header_row = game_logs_table.find('thead').find_all('tr')[-1]\n",
    "#                 data_rows = game_logs_table.find('tbody').find_all('tr')\n",
    "\n",
    "#                 if not headers_written:  # Write headers only once\n",
    "#                     headers = ['Player URL', 'Position', 'First Name', 'Last Name']  # Add new headers at the beginning\n",
    "#                     headers.extend(header.text.strip() for header in header_row.find_all('th'))  # Add existing headers\n",
    "#                     writer.writerow(headers)\n",
    "#                     headers_written = True\n",
    "\n",
    "#                 for row in data_rows:\n",
    "#                     cells = row.find_all(['th', 'td'])\n",
    "#                     data = [url, position, first_name, last_name]  # Start with additional data\n",
    "#                     data.extend(cell.text.strip() for cell in cells)  # Append scraped data\n",
    "#                     writer.writerow(data)\n",
    "#                 print(f\"Data written for {first_name} {last_name}\")\n",
    "\n",
    "#             else:\n",
    "#                 print(f\"No game logs table found for URL: {url}\")\n",
    "#         else:\n",
    "#             print(f\"Failed to retrieve URL: {url} with status code: {response.status_code}\")\n",
    "\n",
    "#         # print(f'Processed URL: {url}')  # Print the URL being processed\n",
    "#         time.sleep(2)  # Add a 3-second delay after processing each URL\n",
    "\n",
    "# print('Data saved to game_logs_rb.csv')\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# RB game tables\n",
    "# Not in nfl.db currently\n",
    "\n",
    "df = pd.read_csv('./data/rosters.csv')\n",
    "\n",
    "# Drop rows where 'pfr_id' is missing\n",
    "df = df.dropna(subset=['pfr_id'])\n",
    "\n",
    "# Filter for Running Backs ('RB' or 'rb')\n",
    "rbs = df[df['position'].str.lower() == 'rb']\n",
    "\n",
    "# Load existing data from game_logs_rb.csv if it exists\n",
    "existing_data = []\n",
    "if os.path.exists('./data/game_logs_rb.csv'):\n",
    "    with open('./data/game_logs_rb.csv', 'r', newline='') as existing_file:\n",
    "        reader = csv.reader(existing_file)\n",
    "        existing_data = list(reader)\n",
    "\n",
    "# Extract the headers and existing player URLs\n",
    "if existing_data:\n",
    "    headers = existing_data[0]\n",
    "    existing_player_urls = {row[headers.index('Player URL')] for row in existing_data[1:]}\n",
    "else:\n",
    "    headers = []\n",
    "    existing_player_urls = set()\n",
    "\n",
    "# Open a CSV file to write the data for running backs\n",
    "with open('./data/game_logs_rb.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write headers if the file is empty or headers are not present\n",
    "    if not headers:\n",
    "        headers = ['Player URL', 'Position', 'First Name', 'Last Name']  # Add new headers at the beginning\n",
    "        # Assuming the code will later fetch the actual headers from the table.\n",
    "        headers_written = False\n",
    "    else:\n",
    "        headers_written = True\n",
    "\n",
    "    # Initialize a counter for progress tracking\n",
    "    total_rbs = len(rbs)\n",
    "    rb_counter = 0\n",
    "\n",
    "    # Iterate over each running back and scrape data\n",
    "    for index, rb in rbs.iterrows():\n",
    "        rb_counter += 1\n",
    "        url = rb['url']\n",
    "        print(f\"Processing RB {rb_counter}/{total_rbs}: {rb['first_name']} {rb['last_name']}\")\n",
    "        first_name = rb['first_name']  # Get the player's first name\n",
    "        last_name = rb['last_name']    # Get the player's last name\n",
    "        position = 'RB'  # Set the position to 'RB'\n",
    "\n",
    "        # Check if the player's data is already in the file\n",
    "        if url in existing_player_urls:\n",
    "            print(f\"Skipping {rb['first_name']} {rb['last_name']}, data already exists.\")\n",
    "            continue\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            game_logs_table = soup.find('table', {'id': 'rushing_and_receiving'})\n",
    "\n",
    "            if game_logs_table:  # Check if the table is found\n",
    "                header_row = game_logs_table.find('thead').find_all('tr')[-1]\n",
    "                data_rows = game_logs_table.find('tbody').find_all('tr')\n",
    "\n",
    "                if not headers_written:  # Write headers only once\n",
    "                    headers.extend(header.text.strip() for header in header_row.find_all('th'))  # Add existing headers\n",
    "                    writer.writerow(headers)\n",
    "                    headers_written = True\n",
    "\n",
    "                for row in data_rows:\n",
    "                    cells = row.find_all(['th', 'td'])\n",
    "                    data = [url, position, first_name, last_name]  # Start with additional data\n",
    "                    data.extend(cell.text.strip() for cell in cells)  # Append scraped data\n",
    "                    writer.writerow(data)\n",
    "                    existing_player_urls.add(url)  # Add the URL to the set to prevent duplicates during this run\n",
    "\n",
    "                print(f\"Data written for {first_name} {last_name}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"No game logs table found for URL: {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve URL: {url} with status code: {response.status_code}\")\n",
    "\n",
    "        time.sleep(2)  # Add a 2-second delay after processing each URL\n",
    "\n",
    "print('Data saved to game_logs_rb.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3705e67f-e9dd-407e-8ae2-fa69dd93c524",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     13\u001b[39m     data = pd.read_csv(file_path)\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Ensure the 'Year' column is treated as a string\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#     data['Year'] = data['Year'].astype(str)\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Identifying Pro Bowl and First-Team AP All-Pro selections\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     data[\u001b[33m'\u001b[39m\u001b[33mPro Bowl\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYear\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstr\u001b[49m.contains(\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m, regex=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     20\u001b[39m     data[\u001b[33m'\u001b[39m\u001b[33mFirst-Team AP All-Pro\u001b[39m\u001b[33m'\u001b[39m] = data[\u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m].str.contains(\u001b[33m'\u001b[39m\u001b[33m+\u001b[39m\u001b[33m'\u001b[39m, regex=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Cleaning up the 'Year' column\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/core/generic.py:6299\u001b[39m, in \u001b[36mNDFrame.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   6293\u001b[39m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set\n\u001b[32m   6294\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata\n\u001b[32m   6295\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessors\n\u001b[32m   6296\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6297\u001b[39m ):\n\u001b[32m   6298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[32m-> \u001b[39m\u001b[32m6299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/core/accessor.py:224\u001b[39m, in \u001b[36mCachedAccessor.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessor\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m accessor_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m._name, accessor_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/core/strings/accessor.py:191\u001b[39m, in \u001b[36mStringMethods.__init__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstring_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28mself\u001b[39m._inferred_dtype = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28mself\u001b[39m._is_categorical = \u001b[38;5;28misinstance\u001b[39m(data.dtype, CategoricalDtype)\n\u001b[32m    193\u001b[39m     \u001b[38;5;28mself\u001b[39m._is_string = \u001b[38;5;28misinstance\u001b[39m(data.dtype, StringDtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/core/strings/accessor.py:245\u001b[39m, in \u001b[36mStringMethods._validate\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    242\u001b[39m inferred_dtype = lib.infer_dtype(values, skipna=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan only use .str accessor with string values!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[31mAttributeError\u001b[39m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "# Clean em\n",
    "\n",
    "file_paths = [\n",
    "    './data/game_logs_qb.csv',\n",
    "    './data/game_logs_wr.csv',\n",
    "    './data/game_logs_rb.csv',\n",
    "    './data/game_logs_te.csv'\n",
    "]\n",
    "\n",
    "# Loop through each file path\n",
    "for file_path in file_paths:\n",
    "    # Load data from each CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure the 'Year' column is treated as a string\n",
    "    data['Year'] = data['Year'].astype(str)\n",
    "\n",
    "    # Identifying Pro Bowl and First-Team AP All-Pro selections\n",
    "    data['Pro Bowl'] = data['Year'].str.contains('*', regex=False)\n",
    "    data['First-Team AP All-Pro'] = data['Year'].str.contains('+', regex=False)\n",
    "\n",
    "    # Cleaning up the 'Year' column\n",
    "    data['Year'] = data['Year'].str.replace('*', '', regex=False)\n",
    "    data['Year'] = data['Year'].str.replace('+', '', regex=False)\n",
    "\n",
    "    # Save the modified DataFrame back to the same CSV file\n",
    "    data.to_csv(file_path, index=False)\n",
    "\n",
    "    # Print confirmation that the file has been updated\n",
    "    print(f\"File updated: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35cd180d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 36 fields in line 21, saw 39\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Loop through each file path\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Load data from each CSV file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Ensure the 'Year' column is treated as a string\u001b[39;00m\n\u001b[32m     15\u001b[39m     data[\u001b[33m'\u001b[39m\u001b[33mSeason\u001b[39m\u001b[33m'\u001b[39m] = data[\u001b[33m'\u001b[39m\u001b[33mSeason\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 36 fields in line 21, saw 39\n"
     ]
    }
   ],
   "source": [
    "# Clean em\n",
    "\n",
    "file_paths = [\n",
    "#     './data/game_logs_wr.csv',\n",
    "#     './data/game_logs_rb.csv',\n",
    "    './data/game_logs_te.csv'\n",
    "]\n",
    "\n",
    "# Loop through each file path\n",
    "for file_path in file_paths:\n",
    "    # Load data from each CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure the 'Year' column is treated as a string\n",
    "    data['Season'] = data['Season'].astype(str)\n",
    "\n",
    "    # Identifying Pro Bowl and First-Team AP All-Pro selections\n",
    "    data['Pro Bowl'] = data['Season'].str.contains('*', regex=False)\n",
    "    data['First-Team AP All-Pro'] = data['Season'].str.contains('+', regex=False)\n",
    "\n",
    "    # Cleaning up the 'Year' column\n",
    "    data['Season'] = data['Season'].str.replace('*', '', regex=False)\n",
    "    data['Season'] = data['Season'].str.replace('+', '', regex=False)\n",
    "\n",
    "    # Save the modified DataFrame back to the same CSV file\n",
    "    data.to_csv(file_path, index=False)\n",
    "\n",
    "    # Print confirmation that the file has been updated\n",
    "    print(f\"File updated: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ec2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!open data/game_logs_qb.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0f2d8-ade1-46e5-a750-d8012c8ec5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multi team RBs?\n",
    "\n",
    "# file_path = 'data/game_logs_rb.csv'\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "# # Identify rows with '2TM'\n",
    "# multi_team_rows = data[data['Tm'] == '2TM']\n",
    "# multi_team_indices = multi_team_rows.index\n",
    "\n",
    "# # Loop through each multi-team row\n",
    "# for idx in multi_team_indices:\n",
    "#     # Find individual team rows for the same player and year\n",
    "#     player = data.at[idx, 'First Name']\n",
    "#     last_name = data.at[idx, 'Last Name']\n",
    "#     year = data.at[idx, 'Year']\n",
    "#     age = data.at[idx, 'Age']\n",
    "    \n",
    "#     # Find corresponding team rows and update their year and age\n",
    "#     team_rows = data[(data['First Name'] == player) & (data['Last Name'] == last_name) & (data['Year'].isna()) & (data['Tm'] != '2TM')]\n",
    "#     data.loc[team_rows.index, 'Year'] = year\n",
    "#     data.loc[team_rows.index, 'Age'] = age\n",
    "\n",
    "# # Drop the multi-team rows\n",
    "# data = data[data['Tm'] != '2TM']\n",
    "\n",
    "# # Save the updated DataFrame to a new CSV file\n",
    "# output_file_path = 'data/game_logs_rb.csv'\n",
    "# data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# print(f\"Updated data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2124f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_with_backoff(url, max_retries=6, backoff_factor=1.5, timeout=15):\n",
    "    \"\"\"GET a URL with exponential backoff, jitter, and a session User-Agent. Returns a requests.Response or None.\"\"\"\n",
    "    session = get_with_backoff.session\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = session.get(url, timeout=timeout)\n",
    "            # Handle rate limiting and server errors with backoff\n",
    "            if resp.status_code == 429:\n",
    "                wait = backoff_factor * (2 ** (attempt - 1)) + random.uniform(0, 1)\n",
    "                print(f\"429 from {url}. Sleeping {wait:.1f}s (attempt {attempt}/{max_retries})\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            if resp.status_code >= 500:\n",
    "                wait = backoff_factor * (2 ** (attempt - 1)) + random.uniform(0, 1)\n",
    "                print(f\"Server error {resp.status_code} for {url}. Sleeping {wait:.1f}s (attempt {attempt}/{max_retries})\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            return resp\n",
    "        except RequestException as e:\n",
    "            wait = backoff_factor * (2 ** (attempt - 1)) + random.uniform(0, 1)\n",
    "            print(f\"RequestException for {url}: {e}. Sleeping {wait:.1f}s (attempt {attempt}/{max_retries})\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "    print(f\"Failed to GET {url} after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "# create a single session and attach it to the helper to avoid re-creating sockets\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                  '(KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 (nfl-ai-scraper)'\n",
    "})\n",
    "get_with_backoff.session = session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5c2dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Arizona Cardinals for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/crd/2023/gamelog/ for Arizona Cardinals in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/crd/2023/gamelog/ for Arizona Cardinals in 2023\n",
      "Processing Atlanta Falcons for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/atl/2023/gamelog/ for Atlanta Falcons in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/atl/2023/gamelog/ for Atlanta Falcons in 2023\n",
      "Processing Baltimore Ravens for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/rav/2023/gamelog/ for Baltimore Ravens in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/rav/2023/gamelog/ for Baltimore Ravens in 2023\n",
      "Processing Buffalo Bills for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/buf/2023/gamelog/ for Buffalo Bills in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/buf/2023/gamelog/ for Buffalo Bills in 2023\n",
      "Processing Carolina Panthers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/car/2023/gamelog/ for Carolina Panthers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/car/2023/gamelog/ for Carolina Panthers in 2023\n",
      "Processing Chicago Bears for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/chi/2023/gamelog/ for Chicago Bears in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/chi/2023/gamelog/ for Chicago Bears in 2023\n",
      "Processing Cincinnati Bengals for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/cin/2023/gamelog/ for Cincinnati Bengals in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/cin/2023/gamelog/ for Cincinnati Bengals in 2023\n",
      "Processing Cleveland Browns for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/cle/2023/gamelog/ for Cleveland Browns in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/cle/2023/gamelog/ for Cleveland Browns in 2023\n",
      "Processing Dallas Cowboys for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/dal/2023/gamelog/ for Dallas Cowboys in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/dal/2023/gamelog/ for Dallas Cowboys in 2023\n",
      "Processing Denver Broncos for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/den/2023/gamelog/ for Denver Broncos in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/den/2023/gamelog/ for Denver Broncos in 2023\n",
      "Processing Detroit Lions for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/det/2023/gamelog/ for Detroit Lions in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/det/2023/gamelog/ for Detroit Lions in 2023\n",
      "Processing Green Bay Packers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/gnb/2023/gamelog/ for Green Bay Packers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/gnb/2023/gamelog/ for Green Bay Packers in 2023\n",
      "Processing Houston Texans for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/htx/2023/gamelog/ for Houston Texans in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/htx/2023/gamelog/ for Houston Texans in 2023\n",
      "Processing Indianapolis Colts for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/clt/2023/gamelog/ for Indianapolis Colts in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/clt/2023/gamelog/ for Indianapolis Colts in 2023\n",
      "Processing Jacksonville Jaguars for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/jax/2023/gamelog/ for Jacksonville Jaguars in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/jax/2023/gamelog/ for Jacksonville Jaguars in 2023\n",
      "Processing Kansas City Chiefs for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/kan/2023/gamelog/ for Kansas City Chiefs in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/kan/2023/gamelog/ for Kansas City Chiefs in 2023\n",
      "Processing Los Angeles Chargers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/sdg/2023/gamelog/ for Los Angeles Chargers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/sdg/2023/gamelog/ for Los Angeles Chargers in 2023\n",
      "Processing Los Angeles Rams for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/ram/2023/gamelog/ for Los Angeles Rams in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/ram/2023/gamelog/ for Los Angeles Rams in 2023\n",
      "Processing Las Vegas Raiders for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/rai/2023/gamelog/ for Las Vegas Raiders in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/rai/2023/gamelog/ for Las Vegas Raiders in 2023\n",
      "Processing Miami Dolphins for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/mia/2023/gamelog/ for Miami Dolphins in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/mia/2023/gamelog/ for Miami Dolphins in 2023\n",
      "Processing Minnesota Vikings for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/min/2023/gamelog/ for Minnesota Vikings in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/min/2023/gamelog/ for Minnesota Vikings in 2023\n",
      "Processing New England Patriots for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/nwe/2023/gamelog/ for New England Patriots in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/nwe/2023/gamelog/ for New England Patriots in 2023\n",
      "Processing New Orleans Saints for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/nor/2023/gamelog/ for New Orleans Saints in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/nor/2023/gamelog/ for New Orleans Saints in 2023\n",
      "Processing New York Giants for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/nyg/2023/gamelog/ for New York Giants in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/nyg/2023/gamelog/ for New York Giants in 2023\n",
      "Processing New York Jets for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/nyj/2023/gamelog/ for New York Jets in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/nyj/2023/gamelog/ for New York Jets in 2023\n",
      "Processing Philadelphia Eagles for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/phi/2023/gamelog/ for Philadelphia Eagles in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/phi/2023/gamelog/ for Philadelphia Eagles in 2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Pittsburgh Steelers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/pit/2023/gamelog/ for Pittsburgh Steelers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/pit/2023/gamelog/ for Pittsburgh Steelers in 2023\n",
      "Processing Seattle Seahawks for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/sea/2023/gamelog/ for Seattle Seahawks in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/sea/2023/gamelog/ for Seattle Seahawks in 2023\n",
      "Processing San Francisco 49ers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/sfo/2023/gamelog/ for San Francisco 49ers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/sfo/2023/gamelog/ for San Francisco 49ers in 2023\n",
      "Processing Tampa Bay Buccaneers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/tam/2023/gamelog/ for Tampa Bay Buccaneers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/tam/2023/gamelog/ for Tampa Bay Buccaneers in 2023\n",
      "Processing Tennessee Titans for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/oti/2023/gamelog/ for Tennessee Titans in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/oti/2023/gamelog/ for Tennessee Titans in 2023\n",
      "Processing Washington Commanders for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/was/2023/gamelog/ for Washington Commanders in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/was/2023/gamelog/ for Washington Commanders in 2023\n",
      "Processing Arizona Cardinals for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/crd/2024/gamelog/ for Arizona Cardinals in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/crd/2024/gamelog/ for Arizona Cardinals in 2024\n",
      "Processing Atlanta Falcons for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/atl/2024/gamelog/ for Atlanta Falcons in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/atl/2024/gamelog/ for Atlanta Falcons in 2024\n",
      "Processing Baltimore Ravens for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/rav/2024/gamelog/ for Baltimore Ravens in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/rav/2024/gamelog/ for Baltimore Ravens in 2024\n",
      "Processing Buffalo Bills for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/buf/2024/gamelog/ for Buffalo Bills in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/buf/2024/gamelog/ for Buffalo Bills in 2024\n",
      "Processing Carolina Panthers for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/car/2024/gamelog/ for Carolina Panthers in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/car/2024/gamelog/ for Carolina Panthers in 2024\n",
      "Processing Chicago Bears for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/chi/2024/gamelog/ for Chicago Bears in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/chi/2024/gamelog/ for Chicago Bears in 2024\n",
      "Processing Cincinnati Bengals for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/cin/2024/gamelog/ for Cincinnati Bengals in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/cin/2024/gamelog/ for Cincinnati Bengals in 2024\n",
      "Processing Cleveland Browns for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/cle/2024/gamelog/ for Cleveland Browns in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/cle/2024/gamelog/ for Cleveland Browns in 2024\n",
      "Processing Dallas Cowboys for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/dal/2024/gamelog/ for Dallas Cowboys in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/dal/2024/gamelog/ for Dallas Cowboys in 2024\n",
      "Processing Denver Broncos for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/den/2024/gamelog/ for Denver Broncos in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/den/2024/gamelog/ for Denver Broncos in 2024\n",
      "Processing Detroit Lions for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/det/2024/gamelog/ for Detroit Lions in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/det/2024/gamelog/ for Detroit Lions in 2024\n",
      "Processing Green Bay Packers for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/gnb/2024/gamelog/ for Green Bay Packers in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/gnb/2024/gamelog/ for Green Bay Packers in 2024\n",
      "Processing Houston Texans for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/htx/2024/gamelog/ for Houston Texans in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/htx/2024/gamelog/ for Houston Texans in 2024\n",
      "Processing Indianapolis Colts for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/clt/2024/gamelog/ for Indianapolis Colts in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/clt/2024/gamelog/ for Indianapolis Colts in 2024\n",
      "Processing Jacksonville Jaguars for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/jax/2024/gamelog/ for Jacksonville Jaguars in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/jax/2024/gamelog/ for Jacksonville Jaguars in 2024\n",
      "Processing Kansas City Chiefs for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/kan/2024/gamelog/ for Kansas City Chiefs in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/kan/2024/gamelog/ for Kansas City Chiefs in 2024\n",
      "Processing Los Angeles Chargers for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/sdg/2024/gamelog/ for Los Angeles Chargers in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/sdg/2024/gamelog/ for Los Angeles Chargers in 2024\n",
      "Processing Los Angeles Rams for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/ram/2024/gamelog/ for Los Angeles Rams in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/ram/2024/gamelog/ for Los Angeles Rams in 2024\n",
      "Processing Las Vegas Raiders for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/rai/2024/gamelog/ for Las Vegas Raiders in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/rai/2024/gamelog/ for Las Vegas Raiders in 2024\n",
      "Processing Miami Dolphins for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/mia/2024/gamelog/ for Miami Dolphins in 2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/mia/2024/gamelog/ for Miami Dolphins in 2024\n",
      "Processing Minnesota Vikings for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/min/2024/gamelog/ for Minnesota Vikings in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/min/2024/gamelog/ for Minnesota Vikings in 2024\n",
      "Processing New England Patriots for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/nwe/2024/gamelog/ for New England Patriots in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/nwe/2024/gamelog/ for New England Patriots in 2024\n",
      "Processing New Orleans Saints for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/nor/2024/gamelog/ for New Orleans Saints in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/nor/2024/gamelog/ for New Orleans Saints in 2024\n",
      "Processing New York Giants for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/nyg/2024/gamelog/ for New York Giants in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/nyg/2024/gamelog/ for New York Giants in 2024\n",
      "Processing New York Jets for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/nyj/2024/gamelog/ for New York Jets in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/nyj/2024/gamelog/ for New York Jets in 2024\n",
      "Processing Philadelphia Eagles for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/phi/2024/gamelog/ for Philadelphia Eagles in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/phi/2024/gamelog/ for Philadelphia Eagles in 2024\n",
      "Processing Pittsburgh Steelers for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/pit/2024/gamelog/ for Pittsburgh Steelers in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/pit/2024/gamelog/ for Pittsburgh Steelers in 2024\n",
      "Processing Seattle Seahawks for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/sea/2024/gamelog/ for Seattle Seahawks in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/sea/2024/gamelog/ for Seattle Seahawks in 2024\n",
      "Processing San Francisco 49ers for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/sfo/2024/gamelog/ for San Francisco 49ers in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/sfo/2024/gamelog/ for San Francisco 49ers in 2024\n",
      "Processing Tampa Bay Buccaneers for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/tam/2024/gamelog/ for Tampa Bay Buccaneers in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/tam/2024/gamelog/ for Tampa Bay Buccaneers in 2024\n",
      "Processing Tennessee Titans for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/oti/2024/gamelog/ for Tennessee Titans in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/oti/2024/gamelog/ for Tennessee Titans in 2024\n",
      "Processing Washington Commanders for the year 2024\n",
      "Table with id gamelog2024 not found on page https://www.pro-football-reference.com/teams/was/2024/gamelog/ for Washington Commanders in 2024\n",
      "Table with id gamelog_opp2024 not found on page https://www.pro-football-reference.com/teams/was/2024/gamelog/ for Washington Commanders in 2024\n"
     ]
    }
   ],
   "source": [
    "# UPDATED Team Game Logs with Rate Limiting\n",
    "# Replace the existing Team Game Logs cell with this code\n",
    "\n",
    "# Create directories if they don't exist\n",
    "data_dir = './data/SR-game-logs'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "opponent_data_dir = './data/SR-opponent-game-logs'\n",
    "os.makedirs(opponent_data_dir, exist_ok=True)\n",
    "\n",
    "# List of teams\n",
    "teams = [\n",
    "    ['crd', 'Arizona Cardinals'],\n",
    "    ['atl', 'Atlanta Falcons'],\n",
    "    ['rav', 'Baltimore Ravens'],\n",
    "    ['buf', 'Buffalo Bills'],\n",
    "    ['car', 'Carolina Panthers'],\n",
    "    ['chi', 'Chicago Bears'],\n",
    "    ['cin', 'Cincinnati Bengals'],\n",
    "    ['cle', 'Cleveland Browns'],\n",
    "    ['dal', 'Dallas Cowboys'],\n",
    "    ['den', 'Denver Broncos'],\n",
    "    ['det', 'Detroit Lions'],\n",
    "    ['gnb', 'Green Bay Packers'],\n",
    "    ['htx', 'Houston Texans'],\n",
    "    ['clt', 'Indianapolis Colts'],\n",
    "    ['jax', 'Jacksonville Jaguars'],\n",
    "    ['kan', 'Kansas City Chiefs'],\n",
    "    ['sdg', 'Los Angeles Chargers'],\n",
    "    ['ram', 'Los Angeles Rams'],\n",
    "    ['rai', 'Las Vegas Raiders'],\n",
    "    ['mia', 'Miami Dolphins'],\n",
    "    ['min', 'Minnesota Vikings'],\n",
    "    ['nwe', 'New England Patriots'],\n",
    "    ['nor', 'New Orleans Saints'],\n",
    "    ['nyg', 'New York Giants'],\n",
    "    ['nyj', 'New York Jets'],\n",
    "    ['phi', 'Philadelphia Eagles'],\n",
    "    ['pit', 'Pittsburgh Steelers'],\n",
    "    ['sea', 'Seattle Seahawks'],\n",
    "    ['sfo', 'San Francisco 49ers'],\n",
    "    ['tam', 'Tampa Bay Buccaneers'],\n",
    "    ['oti', 'Tennessee Titans'],\n",
    "    ['was', 'Washington Commanders']\n",
    "]\n",
    "\n",
    "team_game_logs_headers = [\n",
    "    'week_num', 'game_day_of_week', 'game_date', 'boxscore_word', 'game_outcome', 'overtime', \n",
    "    'game_location', 'opp', 'pts_off', 'pts_def', 'pass_cmp', 'pass_att', 'pass_yds', 'pass_td', \n",
    "    'pass_int', 'pass_sacked', 'pass_sacked_yds', 'pass_yds_per_att', 'pass_net_yds_per_att', \n",
    "    'pass_cmp_perc', 'pass_rating', 'rush_att', 'rush_yds', 'rush_yds_per_att', 'rush_td', \n",
    "    'fgm', 'fga', 'xpm', 'xpa', 'punt', 'punt_yds', 'third_down_success', 'third_down_att', \n",
    "    'fourth_down_success', 'fourth_down_att', 'time_of_poss', 'Team_Name'\n",
    "]\n",
    "\n",
    "opponent_game_logs_headers = [\n",
    "    'week_num', 'game_day_of_week', 'game_date', 'boxscore_word', 'game_outcome', 'overtime',\n",
    "    'game_location', 'opp', 'pts_off', 'pts_def', 'pass_cmp', 'pass_att', 'pass_yds', 'pass_td',\n",
    "    'pass_int', 'pass_sacked', 'pass_sacked_yds', 'pass_yds_per_att', 'pass_net_yds_per_att',\n",
    "    'pass_cmp_perc', 'pass_rating', 'rush_att', 'rush_yds', 'rush_yds_per_att', 'rush_td',\n",
    "    'fgm', 'fga', 'xpm', 'xpa', 'punt', 'punt_yds', 'third_down_success', 'third_down_att',\n",
    "    'fourth_down_success', 'fourth_down_att', 'time_of_poss', 'Team_Name'\n",
    "]\n",
    "\n",
    "# Loop through the years\n",
    "for year in range(2023, 2025):\n",
    "    all_team_game_logs = []  # Create empty lists to accumulate team and opponent data for each year\n",
    "    all_opponent_game_logs = []\n",
    "\n",
    "    for team in teams:\n",
    "        abbreviation, name = team\n",
    "        print(f'Processing {name} for the year {year}')\n",
    "        url = f'https://www.pro-football-reference.com/teams/{abbreviation}/{year}/gamelog/'\n",
    "        response = get_with_backoff(url)\n",
    "        \n",
    "        if response is None:\n",
    "            print(f'Failed to retrieve page {url} for {name} in {year}: exhausted retries')\n",
    "            # back off a bit longer before continuing to the next team\n",
    "            sleep(5 + random.uniform(0, 2))\n",
    "            continue\n",
    "        if response.status_code != 200:\n",
    "            print(f'Failed to retrieve page {url} for {name} in {year}: {response.status_code}')\n",
    "            sleep(3 + random.uniform(0, 2))\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        for table_id in [f'gamelog{year}', f'gamelog_opp{year}']:\n",
    "            table = soup.find('table', {'id': table_id})\n",
    "\n",
    "            if table is None:\n",
    "                print(f'Table with id {table_id} not found on page {url} for {name} in {year}')\n",
    "                continue\n",
    "\n",
    "            tbody = table.find('tbody')\n",
    "            game_logs = []\n",
    "            for tr in tbody.find_all('tr'):\n",
    "                row_data = []\n",
    "                for td in tr.find_all(['th', 'td']):\n",
    "                    row_data.append(td.text)\n",
    "                if table_id == f'gamelog{year}':\n",
    "                    row_data.append(name)\n",
    "                    game_logs.append(row_data)\n",
    "                elif table_id == f'gamelog_opp{year}':\n",
    "                    row_data.append(name)\n",
    "                    all_opponent_game_logs.append(row_data)\n",
    "\n",
    "            if table_id == f'gamelog{year}':\n",
    "                all_team_game_logs.extend(game_logs)\n",
    "\n",
    "            # Check if playoff game logs exist for this team and year\n",
    "            playoff_table_id = f'playoff_gamelog{year}'\n",
    "            playoff_table = soup.find('table', {'id': playoff_table_id})\n",
    "\n",
    "            if playoff_table:\n",
    "                playoff_tbody = playoff_table.find('tbody')\n",
    "                playoff_game_logs = []\n",
    "                for tr in playoff_tbody.find_all('tr'):\n",
    "                    row_data = []\n",
    "                    for td in tr.find_all(['th', 'td']):\n",
    "                        row_data.append(td.text)\n",
    "                    row_data.append(name)\n",
    "                    playoff_game_logs.append(row_data)\n",
    "\n",
    "                all_team_game_logs.extend(playoff_game_logs)\n",
    "\n",
    "        # Randomized sleep after processing each team to avoid bursts\n",
    "        sleep(2.5 + random.uniform(0, 1.5))\n",
    "        \n",
    "    # Extra sleep between teams\n",
    "    sleep(1.5 + random.uniform(0, 1.0))\n",
    "\n",
    "    # Save the accumulated team and opponent data to CSV files, named based on the year\n",
    "    with open(f'./data/SR-game-logs/all_teams_game_logs_{year}.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(team_game_logs_headers)\n",
    "        writer.writerows(all_team_game_logs)\n",
    "\n",
    "    with open(f'./data/SR-opponent-game-logs/all_teams_opponent_game_logs_{year}.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(opponent_game_logs_headers)\n",
    "        writer.writerows(all_opponent_game_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1756da-b8ce-48bd-a856-08b062c191b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Arizona Cardinals for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/crd/2023/gamelog/ for Arizona Cardinals in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/crd/2023/gamelog/ for Arizona Cardinals in 2023\n",
      "Processing Atlanta Falcons for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/atl/2023/gamelog/ for Atlanta Falcons in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/atl/2023/gamelog/ for Atlanta Falcons in 2023\n",
      "Processing Baltimore Ravens for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/rav/2023/gamelog/ for Baltimore Ravens in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/rav/2023/gamelog/ for Baltimore Ravens in 2023\n",
      "Processing Buffalo Bills for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/buf/2023/gamelog/ for Buffalo Bills in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/buf/2023/gamelog/ for Buffalo Bills in 2023\n",
      "Processing Carolina Panthers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/car/2023/gamelog/ for Carolina Panthers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/car/2023/gamelog/ for Carolina Panthers in 2023\n",
      "Processing Chicago Bears for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/chi/2023/gamelog/ for Chicago Bears in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/chi/2023/gamelog/ for Chicago Bears in 2023\n",
      "Processing Cincinnati Bengals for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/cin/2023/gamelog/ for Cincinnati Bengals in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/cin/2023/gamelog/ for Cincinnati Bengals in 2023\n",
      "Processing Cleveland Browns for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/cle/2023/gamelog/ for Cleveland Browns in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/cle/2023/gamelog/ for Cleveland Browns in 2023\n",
      "Processing Dallas Cowboys for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/dal/2023/gamelog/ for Dallas Cowboys in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/dal/2023/gamelog/ for Dallas Cowboys in 2023\n",
      "Processing Denver Broncos for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/den/2023/gamelog/ for Denver Broncos in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/den/2023/gamelog/ for Denver Broncos in 2023\n",
      "Processing Detroit Lions for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/det/2023/gamelog/ for Detroit Lions in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/det/2023/gamelog/ for Detroit Lions in 2023\n",
      "Processing Green Bay Packers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/gnb/2023/gamelog/ for Green Bay Packers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/gnb/2023/gamelog/ for Green Bay Packers in 2023\n",
      "Processing Houston Texans for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/htx/2023/gamelog/ for Houston Texans in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/htx/2023/gamelog/ for Houston Texans in 2023\n",
      "Processing Indianapolis Colts for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/clt/2023/gamelog/ for Indianapolis Colts in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/clt/2023/gamelog/ for Indianapolis Colts in 2023\n",
      "Processing Jacksonville Jaguars for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/jax/2023/gamelog/ for Jacksonville Jaguars in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/jax/2023/gamelog/ for Jacksonville Jaguars in 2023\n",
      "Processing Kansas City Chiefs for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/kan/2023/gamelog/ for Kansas City Chiefs in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/kan/2023/gamelog/ for Kansas City Chiefs in 2023\n",
      "Processing Los Angeles Chargers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/sdg/2023/gamelog/ for Los Angeles Chargers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/sdg/2023/gamelog/ for Los Angeles Chargers in 2023\n",
      "Processing Los Angeles Rams for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/ram/2023/gamelog/ for Los Angeles Rams in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/ram/2023/gamelog/ for Los Angeles Rams in 2023\n",
      "Processing Las Vegas Raiders for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/rai/2023/gamelog/ for Las Vegas Raiders in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/rai/2023/gamelog/ for Las Vegas Raiders in 2023\n",
      "Processing Miami Dolphins for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/mia/2023/gamelog/ for Miami Dolphins in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/mia/2023/gamelog/ for Miami Dolphins in 2023\n",
      "Processing Minnesota Vikings for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/min/2023/gamelog/ for Minnesota Vikings in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/min/2023/gamelog/ for Minnesota Vikings in 2023\n",
      "Processing New England Patriots for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/nwe/2023/gamelog/ for New England Patriots in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/nwe/2023/gamelog/ for New England Patriots in 2023\n",
      "Processing New Orleans Saints for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/nor/2023/gamelog/ for New Orleans Saints in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/nor/2023/gamelog/ for New Orleans Saints in 2023\n",
      "Processing New York Giants for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/nyg/2023/gamelog/ for New York Giants in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/nyg/2023/gamelog/ for New York Giants in 2023\n",
      "Processing New York Jets for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/nyj/2023/gamelog/ for New York Jets in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/nyj/2023/gamelog/ for New York Jets in 2023\n",
      "Processing Philadelphia Eagles for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/phi/2023/gamelog/ for Philadelphia Eagles in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/phi/2023/gamelog/ for Philadelphia Eagles in 2023\n",
      "Processing Pittsburgh Steelers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/pit/2023/gamelog/ for Pittsburgh Steelers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/pit/2023/gamelog/ for Pittsburgh Steelers in 2023\n",
      "Processing Seattle Seahawks for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/sea/2023/gamelog/ for Seattle Seahawks in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/sea/2023/gamelog/ for Seattle Seahawks in 2023\n",
      "Processing San Francisco 49ers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/sfo/2023/gamelog/ for San Francisco 49ers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/sfo/2023/gamelog/ for San Francisco 49ers in 2023\n",
      "Processing Tampa Bay Buccaneers for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/tam/2023/gamelog/ for Tampa Bay Buccaneers in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/tam/2023/gamelog/ for Tampa Bay Buccaneers in 2023\n",
      "Processing Tennessee Titans for the year 2023\n",
      "Table with id gamelog2023 not found on page https://www.pro-football-reference.com/teams/oti/2023/gamelog/ for Tennessee Titans in 2023\n",
      "Table with id gamelog_opp2023 not found on page https://www.pro-football-reference.com/teams/oti/2023/gamelog/ for Tennessee Titans in 2023\n",
      "Processing Washington Commanders for the year 2023\n",
      "Failed to retrieve page https://www.pro-football-reference.com/teams/was/2023/gamelog/ for Washington Commanders in 2023: 429\n",
      "Processing Arizona Cardinals for the year 2024\n",
      "Failed to retrieve page https://www.pro-football-reference.com/teams/crd/2024/gamelog/ for Arizona Cardinals in 2024: 429\n",
      "Processing Atlanta Falcons for the year 2024\n",
      "Failed to retrieve page https://www.pro-football-reference.com/teams/atl/2024/gamelog/ for Atlanta Falcons in 2024: 429\n",
      "Processing Baltimore Ravens for the year 2024\n",
      "Failed to retrieve page https://www.pro-football-reference.com/teams/rav/2024/gamelog/ for Baltimore Ravens in 2024: 429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 135\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFailed to retrieve page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.3\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait 1.3 seconds before the next request\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    138\u001b[39m soup = BeautifulSoup(response.content, \u001b[33m'\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # Team Game Logs\n",
    "# # Not in nfl.db currently\n",
    "\n",
    "# # Create directories if they don't exist\n",
    "# data_dir = './data/SR-game-logs'\n",
    "# os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# # Create directories if they don't exist\n",
    "# opponent_data_dir = './data/SR-opponent-game-logs'\n",
    "# os.makedirs(opponent_data_dir, exist_ok=True)\n",
    "\n",
    "# # List of teams\n",
    "# teams = [\n",
    "#     ['crd', 'Arizona Cardinals'],\n",
    "#     ['atl', 'Atlanta Falcons'],\n",
    "#     ['rav', 'Baltimore Ravens'],\n",
    "#     ['buf', 'Buffalo Bills'],\n",
    "#     ['car', 'Carolina Panthers'],\n",
    "#     ['chi', 'Chicago Bears'],\n",
    "#     ['cin', 'Cincinnati Bengals'],\n",
    "#     ['cle', 'Cleveland Browns'],\n",
    "#     ['dal', 'Dallas Cowboys'],\n",
    "#     ['den', 'Denver Broncos'],\n",
    "#     ['det', 'Detroit Lions'],\n",
    "#     ['gnb', 'Green Bay Packers'],\n",
    "#     ['htx', 'Houston Texans'],\n",
    "#     ['clt', 'Indianapolis Colts'],\n",
    "#     ['jax', 'Jacksonville Jaguars'],\n",
    "#     ['kan', 'Kansas City Chiefs'],\n",
    "#     ['sdg', 'Los Angeles Chargers'],\n",
    "#     ['ram', 'Los Angeles Rams'],\n",
    "#     ['rai', 'Las Vegas Raiders'],\n",
    "#     ['mia', 'Miami Dolphins'],\n",
    "#     ['min', 'Minnesota Vikings'],\n",
    "#     ['nwe', 'New England Patriots'],\n",
    "#     ['nor', 'New Orleans Saints'],\n",
    "#     ['nyg', 'New York Giants'],\n",
    "#     ['nyj', 'New York Jets'],\n",
    "#     ['phi', 'Philadelphia Eagles'],\n",
    "#     ['pit', 'Pittsburgh Steelers'],\n",
    "#     ['sea', 'Seattle Seahawks'],\n",
    "#     ['sfo', 'San Francisco 49ers'],\n",
    "#     ['tam', 'Tampa Bay Buccaneers'],\n",
    "#     ['oti', 'Tennessee Titans'],\n",
    "#     ['was', 'Washington Commanders']\n",
    "# ]\n",
    "\n",
    "# # Custom headers for team game logs and opponent game logs\n",
    "# # team_game_logs_headers = [\n",
    "# #     'Week', 'Day', 'Date', '', 'OT', '', 'Opp', 'Tm', 'Opp', 'Cmp', 'Att', 'Yds', 'TD', 'Int', 'Sk', 'Yds', 'Y/A', 'NY/A',\n",
    "# #     'Cmp%', 'Rate', 'Att', 'Yds', 'Y/A', 'TD', 'FGM', 'FGA', 'XPM', 'XPA', 'Pnt', 'Yds', '3DConv', '3DAtt', '4DConv', '4DAtt', 'ToP', 'Team_Name'\n",
    "# # ]\n",
    "# # team_game_logs_headers = [\n",
    "# #     'Week', 'Day', 'Date', '', '', 'OT', '', 'Opp', 'Tm', 'Opp', 'Cmp', 'Att', 'Yds', 'TD', 'Int', 'Sk', 'Yds', 'Y/A', 'NY/A',\n",
    "# #     'Cmp%', 'Rate', 'Att', 'Yds', 'Y/A', 'TD', 'FGM', 'FGA', 'XPM', 'XPA', 'Pnt', 'Yds', '3DConv', '3DAtt', '4DConv', '4DAtt', 'ToP', 'Team_Name'\n",
    "# # ]\n",
    "# team_game_logs_headers = [\n",
    "#     'week_num', 'game_day_of_week', 'game_date', 'boxscore_word', 'game_outcome', 'overtime', \n",
    "#     'game_location', 'opp', 'pts_off', 'pts_def', 'pass_cmp', 'pass_att', 'pass_yds', 'pass_td', \n",
    "#     'pass_int', 'pass_sacked', 'pass_sacked_yds', 'pass_yds_per_att', 'pass_net_yds_per_att', \n",
    "#     'pass_cmp_perc', 'pass_rating', 'rush_att', 'rush_yds', 'rush_yds_per_att', 'rush_td', \n",
    "#     'fgm', 'fga', 'xpm', 'xpa', 'punt', 'punt_yds', 'third_down_success', 'third_down_att', \n",
    "#     'fourth_down_success', 'fourth_down_att', 'time_of_poss', 'Team_Name'\n",
    "# ]\n",
    "# # Need to change to:\n",
    "# # week_num\n",
    "# # game_day_of_week\n",
    "# # game_date\n",
    "# # boxscore_word\n",
    "# # game_outcome\n",
    "# # overtime\n",
    "# # game_location\n",
    "# # opp\n",
    "# # pts_off\n",
    "# # pts_def\n",
    "# # pass_cmp\n",
    "# # pass_att\n",
    "# # pass_yds\n",
    "# # pass_td\n",
    "# # pass_int\n",
    "# # pass_sacked\n",
    "# # pass_sacked_yds\n",
    "# # pass_yds_per_att\n",
    "# # pass_net_yds_per_att\n",
    "# # pass_cmp_perc\n",
    "# # pass_rating\n",
    "# # rush_att\n",
    "# # rush_yds\n",
    "# # rush_yds_per_att\n",
    "# # rush_td\n",
    "# # fgm\n",
    "# # fga\n",
    "# # xpm\n",
    "# # xpa\n",
    "# # punt\n",
    "# # punt_yds\n",
    "# # third_down_success\n",
    "# # third_down_att\n",
    "# # fourth_down_success\n",
    "# # fourth_down_att\n",
    "# # time_of_poss\n",
    "# # team_name\n",
    "\n",
    "# # opponent_game_logs_headers = [\n",
    "# #     'Week', 'Day', 'Date', '', 'OT', '', 'Opp', 'Tm', 'Opp', 'Cmp', 'Att', 'Yds', 'TD', 'Int', 'Sk', 'Yds', 'Y/A', 'NY/A',\n",
    "# #     'Cmp%', 'Rate', 'Att', 'Yds', 'Y/A', 'TD', 'FGM', 'FGA', 'XPM', 'XPA', 'Pnt', 'Yds', '3DConv', '3DAtt', '4DConv', '4DAtt', 'ToP', 'Team_Name'\n",
    "# # ]\n",
    "# # opponent_game_logs_headers = [\n",
    "# #     'Week', 'Day', 'Date', '', '', 'OT', '', 'Opp', 'Tm', 'Opp', 'Cmp', 'Att', 'Yds', 'TD', 'Int', 'Sk', 'Yds', 'Y/A', 'NY/A',\n",
    "# #     'Cmp%', 'Rate', 'Att', 'Yds', 'Y/A', 'TD', 'FGM', 'FGA', 'XPM', 'XPA', 'Pnt', 'Yds', '3DConv', '3DAtt', '4DConv', '4DAtt', 'ToP', 'Team_Name'\n",
    "# # ]\n",
    "# opponent_game_logs_headers = [\n",
    "#     'week_num', 'game_day_of_week', 'game_date', 'boxscore_word', 'game_outcome', 'overtime',\n",
    "#     'game_location', 'opp', 'pts_off', 'pts_def', 'pass_cmp', 'pass_att', 'pass_yds', 'pass_td',\n",
    "#     'pass_int', 'pass_sacked', 'pass_sacked_yds', 'pass_yds_per_att', 'pass_net_yds_per_att',\n",
    "#     'pass_cmp_perc', 'pass_rating', 'rush_att', 'rush_yds', 'rush_yds_per_att', 'rush_td',\n",
    "#     'fgm', 'fga', 'xpm', 'xpa', 'punt', 'punt_yds', 'third_down_success', 'third_down_att',\n",
    "#     'fourth_down_success', 'fourth_down_att', 'time_of_poss', 'Team_Name'\n",
    "# ]\n",
    "\n",
    "# # Loop through the years\n",
    "# # for year in range(2015, 2025):\n",
    "# for year in range(2023, 2025):\n",
    "#     all_team_game_logs = []  # Create empty lists to accumulate team and opponent data for each year\n",
    "#     all_opponent_game_logs = []\n",
    "\n",
    "#     for team in teams:\n",
    "#         abbreviation, name = team\n",
    "#         print(f'Processing {name} for the year {year}')  # Include the year in the print statement\n",
    "#         url = f'https://www.pro-football-reference.com/teams/{abbreviation}/{year}/gamelog/'\n",
    "#         response = requests.get(url)\n",
    "\n",
    "#         if response.status_code != 200:\n",
    "#             print(f'Failed to retrieve page {url} for {name} in {year}: {response.status_code}')\n",
    "#             sleep(1.3)  # Wait 1.3 seconds before the next request\n",
    "#             continue\n",
    "\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#         for table_id in [f'gamelog{year}', f'gamelog_opp{year}']:\n",
    "#             table = soup.find('table', {'id': table_id})\n",
    "\n",
    "#             if table is None:\n",
    "#                 print(f'Table with id {table_id} not found on page {url} for {name} in {year}')\n",
    "#                 continue\n",
    "\n",
    "#             tbody = table.find('tbody')\n",
    "#             game_logs = []\n",
    "#             for tr in tbody.find_all('tr'):\n",
    "#                 row_data = []\n",
    "#                 for td in tr.find_all(['th', 'td']):\n",
    "#                     row_data.append(td.text)\n",
    "#                 if table_id == f'gamelog{year}':\n",
    "#                     row_data.append(name)\n",
    "#                     game_logs.append(row_data)\n",
    "#                 elif table_id == f'gamelog_opp{year}':\n",
    "#                     row_data.append(name)\n",
    "#                     all_opponent_game_logs.append(row_data)\n",
    "\n",
    "#             if table_id == f'gamelog{year}':\n",
    "#                 all_team_game_logs.extend(game_logs)\n",
    "\n",
    "#             playoff_table_id = f'playoff_gamelog{year}'\n",
    "#             playoff_table = soup.find('table', {'id': playoff_table_id})\n",
    "\n",
    "#             if playoff_table:\n",
    "#                 playoff_tbody = playoff_table.find('tbody')\n",
    "#                 playoff_game_logs = []\n",
    "#                 for tr in playoff_tbody.find_all('tr'):\n",
    "#                     row_data = []\n",
    "#                     for td in tr.find_all(['th', 'td']):\n",
    "#                         row_data.append(td.text)\n",
    "#                     row_data.append(name)\n",
    "#                     playoff_game_logs.append(row_data)\n",
    "\n",
    "#                 all_team_game_logs.extend(playoff_game_logs)\n",
    "\n",
    "#     with open(f'./data/SR-game-logs/all_teams_game_logs_{year}.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(team_game_logs_headers)\n",
    "#         writer.writerows(all_team_game_logs)\n",
    "\n",
    "#     with open(f'./data/SR-opponent-game-logs/all_teams_opponent_game_logs_{year}.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(opponent_game_logs_headers)\n",
    "#         writer.writerows(all_opponent_game_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d01833a-1f33-4573-91e8-5d32368866b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[38;5;66;03m# home_team = team_abbreviation_map.get(row['Team_Name'], 'UNKNOWN')\u001b[39;00m\n\u001b[32m     90\u001b[39m         \u001b[38;5;66;03m# away_team = team_abbreviation_map.get(row['opp'], 'UNKNOWN')\u001b[39;00m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.Series([home_team, away_team])\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhome_team_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maway_team_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m = df.apply(determine_home_away, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# df['home_team'] = df.apply(lambda row: team_abbreviation_map[row['Opp']] if row['Unnamed: 6'] == '@' else team_abbreviation_map[row['Team_Name']], axis=1)\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# df['away_team'] = df.apply(lambda row: team_abbreviation_map[row['Team_Name']] if row['Unnamed: 6'] == '@' else team_abbreviation_map[row['Opp']], axis=1)\u001b[39;00m\n\u001b[32m     96\u001b[39m \n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Ensure 'week_num' is a string and pad single digits with a leading zero\u001b[39;00m\n\u001b[32m     98\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mweek_num\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mweek_num\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).str.zfill(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/core/frame.py:4299\u001b[39m, in \u001b[36mDataFrame.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4297\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_frame(key, value)\n\u001b[32m   4298\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np.ndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[32m-> \u001b[39m\u001b[32m4299\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4300\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[32m   4301\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_item_frame_value(key, value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/core/frame.py:4341\u001b[39m, in \u001b[36mDataFrame._setitem_array\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4337\u001b[39m     \u001b[38;5;66;03m# Note: unlike self.iloc[:, indexer] = value, this will\u001b[39;00m\n\u001b[32m   4338\u001b[39m     \u001b[38;5;66;03m#  never try to overwrite values inplace\u001b[39;00m\n\u001b[32m   4340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[32m-> \u001b[39m\u001b[32m4341\u001b[39m         \u001b[43mcheck_key_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4342\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k1, k2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(key, value.columns):\n\u001b[32m   4343\u001b[39m             \u001b[38;5;28mself\u001b[39m[k1] = value[k2]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/core/indexers/utils.py:390\u001b[39m, in \u001b[36mcheck_key_length\u001b[39m\u001b[34m(columns, key, value)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m columns.is_unique:\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value.columns) != \u001b[38;5;28mlen\u001b[39m(key):\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mColumns must be same length as key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    392\u001b[39m     \u001b[38;5;66;03m# Missing keys in columns are represented as -1\u001b[39;00m\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns.get_indexer_non_unique(key)[\u001b[32m0\u001b[39m]) != \u001b[38;5;28mlen\u001b[39m(value.columns):\n",
      "\u001b[31mValueError\u001b[39m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "# Create game_id in game logs\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = 'data/SR-game-logs/'\n",
    "\n",
    "# List to store DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):  # Ensure we are processing only CSV files\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Extract the season from the filename, assuming the format: all_teams_opponent_game_logs_YYYY.csv\n",
    "        # season = filename.split('_')[-1].replace('.csv', '')\n",
    "        season = filename.split('_')[-1].replace('.csv', '')\n",
    "\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add the season column\n",
    "        # df['season'] = season\n",
    "\n",
    "        # Add the raw season column\n",
    "        df['season'] = season\n",
    "        \n",
    "        # Add the cleaned season column (if needed separately for game_id logic)\n",
    "        # df['season'] = raw_season  # You can fu\n",
    "\n",
    "        # Add the cleaned season column (if needed separately for game_id logic)\n",
    "        # df['season'] = raw_season  # You can further clean this if necessary\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        df_list.append(df)\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "team_abbreviation_map = {\n",
    "    'Arizona Cardinals': 'ARI',\n",
    "    'Atlanta Falcons': 'ATL',\n",
    "    'Baltimore Ravens': 'BAL',\n",
    "    'Buffalo Bills': 'BUF',\n",
    "    'Carolina Panthers': 'CAR',\n",
    "    'Chicago Bears': 'CHI',\n",
    "    'Cincinnati Bengals': 'CIN',\n",
    "    'Cleveland Browns': 'CLE',\n",
    "    'Dallas Cowboys': 'DAL',\n",
    "    'Denver Broncos': 'DEN',\n",
    "    'Detroit Lions': 'DET',\n",
    "    'Green Bay Packers': 'GB',\n",
    "    'Houston Texans': 'HOU',\n",
    "    'Indianapolis Colts': 'IND',\n",
    "    'Jacksonville Jaguars': 'JAX',\n",
    "    'Kansas City Chiefs': 'KC',\n",
    "    'Los Angeles Chargers': 'LAC',\n",
    "    'Los Angeles Rams': 'LAR',\n",
    "    'Las Vegas Raiders': 'LVR',\n",
    "    'Oakland Raiders': 'LVR',\n",
    "    'Miami Dolphins': 'MIA',\n",
    "    'Minnesota Vikings': 'MIN',\n",
    "    'New England Patriots': 'NE',\n",
    "    'New Orleans Saints': 'NO',\n",
    "    'New York Giants': 'NYG',\n",
    "    'New York Jets': 'NYJ',\n",
    "    'Philadelphia Eagles': 'PHI',\n",
    "    'Pittsburgh Steelers': 'PIT',\n",
    "    'Seattle Seahawks': 'SEA',\n",
    "    'San Francisco 49ers': 'SF',\n",
    "    'Tampa Bay Buccaneers': 'TB',\n",
    "    'Tennessee Titans': 'TEN',\n",
    "    'Washington Commanders': 'WAS',\n",
    "    'Washington Football Team': 'WAS',\n",
    "    'Washington Redskins': 'WAS',\n",
    "    'St. Louis Rams': 'STL',\n",
    "    'San Diego Chargers': 'LAC'\n",
    "}\n",
    "\n",
    "# Function to determine home and away teams\n",
    "def determine_home_away(row):\n",
    "    if row['game_location'] == '@':\n",
    "        away_team = team_abbreviation_map[row['Team_Name']]\n",
    "        home_team = team_abbreviation_map[row['opp']]\n",
    "    else:\n",
    "        home_team = team_abbreviation_map[row['Team_Name']]\n",
    "        away_team = team_abbreviation_map[row['opp']]\n",
    "        # home_team = team_abbreviation_map.get(row['Team_Name'], 'UNKNOWN')\n",
    "        # away_team = team_abbreviation_map.get(row['opp'], 'UNKNOWN')\n",
    "    return pd.Series([home_team, away_team])\n",
    "    \n",
    "df[['home_team_id', 'away_team_id']] = df.apply(determine_home_away, axis=1)\n",
    "# df['home_team'] = df.apply(lambda row: team_abbreviation_map[row['Opp']] if row['Unnamed: 6'] == '@' else team_abbreviation_map[row['Team_Name']], axis=1)\n",
    "# df['away_team'] = df.apply(lambda row: team_abbreviation_map[row['Team_Name']] if row['Unnamed: 6'] == '@' else team_abbreviation_map[row['Opp']], axis=1)\n",
    "\n",
    "# Ensure 'week_num' is a string and pad single digits with a leading zero\n",
    "df['week_num'] = df['week_num'].astype(str).str.zfill(2)\n",
    "\n",
    "# Create the 'game_id' column by combining 'season', 'week_num', 'away_team', and 'home_team'\n",
    "df['game_id'] = df['season'] + '_' + df['week_num'] + '_' + df['away_team_id'] + '_' + df['home_team_id']\n",
    "\n",
    "# Save the updated combined DataFrame to a new CSV file\n",
    "output_file_path_with_teams = 'data/all_team_game_logs.csv'\n",
    "df.to_csv(output_file_path_with_teams, index=False)\n",
    "print(f\"Updated file with home and away teams saved to: {output_file_path_with_teams}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c6f922-55e3-41fc-a9b3-58d6fd7b68c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/all_team_game_logs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# # Aggregate all_game_logs.csv to single row per game\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# df = pd.read_csv('data/all_team_game_logs.csv')\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m \n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Aggregate all_game_logs.csv to single row per game\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/all_team_game_logs.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Grouping the data by game_id and aggregating the stats separately for home and away teams\u001b[39;00m\n\u001b[32m     47\u001b[39m grouped_df = df.groupby(\u001b[33m'\u001b[39m\u001b[33mgame_id\u001b[39m\u001b[33m'\u001b[39m, group_keys=\u001b[38;5;28;01mFalse\u001b[39;00m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd.Series({\n\u001b[32m     48\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mseason\u001b[39m\u001b[33m'\u001b[39m: x[\u001b[33m'\u001b[39m\u001b[33mseason\u001b[39m\u001b[33m'\u001b[39m].iloc[\u001b[32m0\u001b[39m],  \u001b[38;5;66;03m# Ensure the season is included from the first entry\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhome_pts_off\u001b[39m\u001b[33m'\u001b[39m: x.loc[x[\u001b[33m'\u001b[39m\u001b[33mgame_location\u001b[39m\u001b[33m'\u001b[39m].isnull() | (x[\u001b[33m'\u001b[39m\u001b[33mgame_location\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mpts_off\u001b[39m\u001b[33m'\u001b[39m].sum(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maway_rush_td\u001b[39m\u001b[33m'\u001b[39m: x.loc[x[\u001b[33m'\u001b[39m\u001b[33mgame_location\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33m@\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrush_td\u001b[39m\u001b[33m'\u001b[39m].sum(),\n\u001b[32m     79\u001b[39m }))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/all_team_game_logs.csv'"
     ]
    }
   ],
   "source": [
    "# # Aggregate all_game_logs.csv to single row per game\n",
    "\n",
    "# df = pd.read_csv('data/all_team_game_logs.csv')\n",
    "\n",
    "# # Grouping the data by game_id and aggregating the stats separately for home and away teams\n",
    "# grouped_df = df.groupby('game_id', group_keys=False).apply(lambda x: pd.Series({\n",
    "#     'home_pts_off': x.loc[x['game_location'] == 'N', 'pts_off'].sum() or x.loc[x['game_location'] == '', 'pts_off'].sum(),\n",
    "#     'away_pts_off': x.loc[x['game_location'] == '@', 'pts_off'].sum(),\n",
    "#     'home_pass_cmp': x.loc[x['game_location'] == 'N', 'pass_cmp'].sum() or x.loc[x['game_location'] == '', 'pass_cmp'].sum(),\n",
    "#     'away_pass_cmp': x.loc[x['game_location'] == '@', 'pass_cmp'].sum(),\n",
    "#     'home_pass_att': x.loc[x['game_location'] == 'N', 'pass_att'].sum() or x.loc[x['game_location'] == '', 'pass_att'].sum(),\n",
    "#     'away_pass_att': x.loc[x['game_location'] == '@', 'pass_att'].sum(),\n",
    "#     'home_pass_yds': x.loc[x['game_location'] == 'N', 'pass_yds'].sum() or x.loc[x['game_location'] == '', 'pass_yds'].sum(),\n",
    "#     'away_pass_yds': x.loc[x['game_location'] == '@', 'pass_yds'].sum(),\n",
    "#     'home_pass_td': x.loc[x['game_location'] == 'N', 'pass_td'].sum() or x.loc[x['game_location'] == '', 'pass_td'].sum(),\n",
    "#     'away_pass_td': x.loc[x['game_location'] == '@', 'pass_td'].sum(),\n",
    "#     'home_pass_int': x.loc[x['game_location'] == 'N', 'pass_int'].sum() or x.loc[x['game_location'] == '', 'pass_int'].sum(),\n",
    "#     'away_pass_int': x.loc[x['game_location'] == '@', 'pass_int'].sum(),\n",
    "#     'home_pass_sacked': x.loc[x['game_location'] == 'N', 'pass_sacked'].sum() or x.loc[x['game_location'] == '', 'pass_sacked'].sum(),\n",
    "#     'away_pass_sacked': x.loc[x['game_location'] == '@', 'pass_sacked'].sum(),\n",
    "#     'home_pass_yds_per_att': x.loc[x['game_location'] == 'N', 'pass_yds_per_att'].mean() or x.loc[x['game_location'] == '', 'pass_yds_per_att'].mean(),\n",
    "#     'away_pass_yds_per_att': x.loc[x['game_location'] == '@', 'pass_yds_per_att'].mean(),\n",
    "#     'home_pass_net_yds_per_att': x.loc[x['game_location'] == 'N', 'pass_net_yds_per_att'].mean() or x.loc[x['game_location'] == '', 'pass_net_yds_per_att'].mean(),\n",
    "#     'away_pass_net_yds_per_att': x.loc[x['game_location'] == '@', 'pass_net_yds_per_att'].mean(),\n",
    "#     'home_pass_cmp_perc': x.loc[x['game_location'] == 'N', 'pass_cmp_perc'].mean() or x.loc[x['game_location'] == '', 'pass_cmp_perc'].mean(),\n",
    "#     'away_pass_cmp_perc': x.loc[x['game_location'] == '@', 'pass_cmp_perc'].mean(),\n",
    "#     'home_pass_rating': x.loc[x['game_location'] == 'N', 'pass_rating'].mean() or x.loc[x['game_location'] == '', 'pass_rating'].mean(),\n",
    "#     'away_pass_rating': x.loc[x['game_location'] == '@', 'pass_rating'].mean(),\n",
    "#     'home_rush_att': x.loc[x['game_location'] == 'N', 'rush_att'].sum() or x.loc[x['game_location'] == '', 'rush_att'].sum(),\n",
    "#     'away_rush_att': x.loc[x['game_location'] == '@', 'rush_att'].sum(),\n",
    "#     'home_rush_yds': x.loc[x['game_location'] == 'N', 'rush_yds'].sum() or x.loc[x['game_location'] == '', 'rush_yds'].sum(),\n",
    "#     'away_rush_yds': x.loc[x['game_location'] == '@', 'rush_yds'].sum(),\n",
    "#     'home_rush_yds_per_att': x.loc[x['game_location'] == 'N', 'rush_yds_per_att'].mean() or x.loc[x['game_location'] == '', 'rush_yds_per_att'].mean(),\n",
    "#     'away_rush_yds_per_att': x.loc[x['game_location'] == '@', 'rush_yds_per_att'].mean(),\n",
    "#     'home_rush_td': x.loc[x['game_location'] == 'N', 'rush_td'].sum() or x.loc[x['game_location'] == '', 'rush_td'].sum(),\n",
    "#     'away_rush_td': x.loc[x['game_location'] == '@', 'rush_td'].sum(),\n",
    "# }))\n",
    "\n",
    "# # Save the result to a CSV file if needed\n",
    "# grouped_df.to_csv('data/all_team_game_logs.csv', index=True)\n",
    "\n",
    "# Aggregate all_game_logs.csv to single row per game\n",
    "\n",
    "df = pd.read_csv('data/all_team_game_logs.csv')\n",
    "\n",
    "# Grouping the data by game_id and aggregating the stats separately for home and away teams\n",
    "grouped_df = df.groupby('game_id', group_keys=False).apply(lambda x: pd.Series({\n",
    "    'season': x['season'].iloc[0],  # Ensure the season is included from the first entry\n",
    "    'home_pts_off': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pts_off'].sum(),\n",
    "    'away_pts_off': x.loc[x['game_location'] == '@', 'pts_off'].sum(),\n",
    "    'home_pass_cmp': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pass_cmp'].sum(),\n",
    "    'away_pass_cmp': x.loc[x['game_location'] == '@', 'pass_cmp'].sum(),\n",
    "    'home_pass_att': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pass_att'].sum(),\n",
    "    'away_pass_att': x.loc[x['game_location'] == '@', 'pass_att'].sum(),\n",
    "    'home_pass_yds': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pass_yds'].sum(),\n",
    "    'away_pass_yds': x.loc[x['game_location'] == '@', 'pass_yds'].sum(),\n",
    "    'home_pass_td': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pass_td'].sum(),\n",
    "    'away_pass_td': x.loc[x['game_location'] == '@', 'pass_td'].sum(),\n",
    "    'home_pass_int': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pass_int'].sum(),\n",
    "    'away_pass_int': x.loc[x['game_location'] == '@', 'pass_int'].sum(),\n",
    "    'home_pass_sacked': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pass_sacked'].sum(),\n",
    "    'away_pass_sacked': x.loc[x['game_location'] == '@', 'pass_sacked'].sum(),\n",
    "    'home_pass_yds_per_att': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pass_yds_per_att'].mean(),\n",
    "    'away_pass_yds_per_att': x.loc[x['game_location'] == '@', 'pass_yds_per_att'].mean(),\n",
    "    'home_pass_net_yds_per_att': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pass_net_yds_per_att'].mean(),\n",
    "    'away_pass_net_yds_per_att': x.loc[x['game_location'] == '@', 'pass_net_yds_per_att'].mean(),\n",
    "    'home_pass_cmp_perc': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pass_cmp_perc'].mean(),\n",
    "    'away_pass_cmp_perc': x.loc[x['game_location'] == '@', 'pass_cmp_perc'].mean(),\n",
    "    'home_pass_rating': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'pass_rating'].mean(),\n",
    "    'away_pass_rating': x.loc[x['game_location'] == '@', 'pass_rating'].mean(),\n",
    "    'home_rush_att': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'rush_att'].sum(),\n",
    "    'away_rush_att': x.loc[x['game_location'] == '@', 'rush_att'].sum(),\n",
    "    'home_rush_yds': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'rush_yds'].sum(),\n",
    "    'away_rush_yds': x.loc[x['game_location'] == '@', 'rush_yds'].sum(),\n",
    "    'home_rush_yds_per_att': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'rush_yds_per_att'].mean(),\n",
    "    'away_rush_yds_per_att': x.loc[x['game_location'] == '@', 'rush_yds_per_att'].mean(),\n",
    "    'home_rush_td': x.loc[x['game_location'].isnull() | (x['game_location'] == ''), 'rush_td'].sum(),\n",
    "    'away_rush_td': x.loc[x['game_location'] == '@', 'rush_td'].sum(),\n",
    "}))\n",
    "\n",
    "# Save the result to a CSV file if needed\n",
    "grouped_df.to_csv('data/all_team_game_logs.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20b931-4517-4e1c-995a-6b2e036fc30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!open data/all_team_game_logs.csv\n",
    "# !open data/all_opponent_team_game_logs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368c251-a256-48ec-a18c-af71e70ec9e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Team Stats and Rankings\n",
    "# Not in nfl.db currently\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "# Create directories if they don't exist\n",
    "data_dir = './data/SR-team-stats'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# List of teams and abbreviations\n",
    "teams = [\n",
    "    ['crd', 'Arizona Cardinals'],\n",
    "    ['atl', 'Atlanta Falcons'],\n",
    "    ['rav', 'Baltimore Ravens'],\n",
    "    ['buf', 'Buffalo Bills'],\n",
    "    ['car', 'Carolina Panthers'],\n",
    "    ['chi', 'Chicago Bears'],\n",
    "    ['cin', 'Cincinnati Bengals'],\n",
    "    ['cle', 'Cleveland Browns'],\n",
    "    ['dal', 'Dallas Cowboys'],\n",
    "    ['den', 'Denver Broncos'],\n",
    "    ['det', 'Detroit Lions'],\n",
    "    ['gnb', 'Green Bay Packers'],\n",
    "    ['htx', 'Houston Texans'],\n",
    "    ['clt', 'Indianapolis Colts'],\n",
    "    ['jax', 'Jacksonville Jaguars'],\n",
    "    ['kan', 'Kansas City Chiefs'],\n",
    "    ['sdg', 'Los Angeles Chargers'],\n",
    "    ['ram', 'Los Angeles Rams'],\n",
    "    ['rai', 'Las Vegas Raiders'],\n",
    "    ['mia', 'Miami Dolphins'],\n",
    "    ['min', 'Minnesota Vikings'],\n",
    "    ['nwe', 'New England Patriots'],\n",
    "    ['nor', 'New Orleans Saints'],\n",
    "    ['nyg', 'New York Giants'],\n",
    "    ['nyj', 'New York Jets'],\n",
    "    ['phi', 'Philadelphia Eagles'],\n",
    "    ['pit', 'Pittsburgh Steelers'],\n",
    "    ['sea', 'Seattle Seahawks'],\n",
    "    ['sfo', 'San Francisco 49ers'],\n",
    "    ['tam', 'Tampa Bay Buccaneers'],\n",
    "    ['oti', 'Tennessee Titans'],\n",
    "    ['was', 'Washington Commanders']\n",
    "]\n",
    "\n",
    "# Define headers for team stats CSV\n",
    "team_stats_headers = [\n",
    "    'Player', 'PF', 'Yds', 'Ply', 'Y/P', 'TO', 'FL', '1stD', 'Cmp', 'Att', 'Yds', 'TD', 'Int', 'NY/A',\n",
    "    '1stD', 'Att', 'Yds', 'TD', 'Y/A', '1stD', 'Pen', 'Yds', '1stPy', '#Dr', 'Sc%', 'TO%', 'Start', 'Time', 'Plays', 'Yds', 'Pts', 'Team'\n",
    "]\n",
    "\n",
    "# Loop through the years\n",
    "# for year in range(2015, 2025):\n",
    "for year in range(2023, 2025):\n",
    "    all_team_stats = []  # Create empty list to accumulate team stats data for each year\n",
    "\n",
    "    for team in teams:\n",
    "        abbreviation, name = team\n",
    "        print(f'Processing {name} for the year {year}')  # Include the year in the print statement\n",
    "        url = f'https://www.pro-football-reference.com/teams/{abbreviation}/{year}.htm'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f'Failed to retrieve page {url} for {name} in {year}: {response.status_code}')\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the main table for team stats (e.g., \"team_stats\" table)\n",
    "        table = soup.find('table', {'id': 'team_stats'})\n",
    "        \n",
    "        if table is None:\n",
    "            print(f'Team stats table not found on page {url} for {name} in {year}')\n",
    "            continue\n",
    "\n",
    "        tbody = table.find('tbody')\n",
    "        for tr in tbody.find_all('tr'):\n",
    "            row_data = [tr.find('th').text.strip()]  # Start with the 'Player' column data\n",
    "            row_data.extend([td.text.strip() for td in tr.find_all('td')])  # Add the rest of the row data\n",
    "            row_data.append(abbreviation)  # Append team abbreviation as the last column\n",
    "            all_team_stats.append(row_data)\n",
    "\n",
    "        sleep(2.5)  # Sleep for 2.5 seconds after processing each team\n",
    "\n",
    "    # Save the accumulated team stats data to a CSV file, named based on the year\n",
    "    with open(f'{data_dir}/all_teams_stats_{year}.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(team_stats_headers)\n",
    "        writer.writerows(all_team_stats)\n",
    "\n",
    "    print(f'Saved data for all teams for the year {year}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dff3df-9af8-4f2f-ba18-267917975f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Schedule & Game Results \n",
    "# Not in nfl.db currently\n",
    "\n",
    "# Create directories if they don't exist\n",
    "data_dir = './data/SR-schedule-and-game-results'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# List of teams and abbreviations\n",
    "teams = [\n",
    "    ['crd', 'Arizona Cardinals'],\n",
    "    ['atl', 'Atlanta Falcons'],\n",
    "    ['rav', 'Baltimore Ravens'],\n",
    "    ['buf', 'Buffalo Bills'],\n",
    "    ['car', 'Carolina Panthers'],\n",
    "    ['chi', 'Chicago Bears'],\n",
    "    ['cin', 'Cincinnati Bengals'],\n",
    "    ['cle', 'Cleveland Browns'],\n",
    "    ['dal', 'Dallas Cowboys'],\n",
    "    ['den', 'Denver Broncos'],\n",
    "    ['det', 'Detroit Lions'],\n",
    "    ['gnb', 'Green Bay Packers'],\n",
    "    ['htx', 'Houston Texans'],\n",
    "    ['clt', 'Indianapolis Colts'],\n",
    "    ['jax', 'Jacksonville Jaguars'],\n",
    "    ['kan', 'Kansas City Chiefs'],\n",
    "    ['sdg', 'Los Angeles Chargers'],\n",
    "    ['ram', 'Los Angeles Rams'],\n",
    "    ['rai', 'Las Vegas Raiders'],\n",
    "    ['mia', 'Miami Dolphins'],\n",
    "    ['min', 'Minnesota Vikings'],\n",
    "    ['nwe', 'New England Patriots'],\n",
    "    ['nor', 'New Orleans Saints'],\n",
    "    ['nyg', 'New York Giants'],\n",
    "    ['nyj', 'New York Jets'],\n",
    "    ['phi', 'Philadelphia Eagles'],\n",
    "    ['pit', 'Pittsburgh Steelers'],\n",
    "    ['sea', 'Seattle Seahawks'],\n",
    "    ['sfo', 'San Francisco 49ers'],\n",
    "    ['tam', 'Tampa Bay Buccaneers'],\n",
    "    ['oti', 'Tennessee Titans'],\n",
    "    ['was', 'Washington Commanders']\n",
    "]\n",
    "\n",
    "# Updated headers for the schedule and game results CSV\n",
    "schedule_headers = [\n",
    "    'Week', 'Day', 'Date', 'Time', 'Boxscore', 'Outcome', 'OT', 'Rec', 'Home/Away', 'Opp', \n",
    "    'Tm', 'OppPts', '1stD', 'TotYd', 'PassY', 'RushY', 'TO_lost', \n",
    "    'Opp1stD', 'OppTotYd', 'OppPassY', 'OppRushY', 'TO_won',\n",
    "    'Offense', 'Defense', 'Sp. Tms'\n",
    "]\n",
    "\n",
    "# Loop through the years\n",
    "# for year in range(2015, 2025):\n",
    "for year in range(2023, 2025):\n",
    "    all_games = []  # Create an empty list to accumulate game data for each year\n",
    "\n",
    "    for team in teams:\n",
    "        abbreviation, name = team\n",
    "        print(f'Processing {name} for the year {year}')  # Include the year in the print statement\n",
    "        url = f'https://www.pro-football-reference.com/teams/{abbreviation}/{year}.htm'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f'Failed to retrieve page {url} for {name} in {year}: {response.status_code}')\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the \"Schedule & Game Results\" table\n",
    "        table = soup.find('table', {'id': 'games'})\n",
    "        if table is None:\n",
    "            print(f'Schedule & Game Results table not found on page {url} for {name} in {year}')\n",
    "            continue\n",
    "\n",
    "        tbody = table.find('tbody')\n",
    "        team_games = []  # Store game data for this team\n",
    "\n",
    "        for tr in tbody.find_all('tr'):\n",
    "            # Initialize the row with the Week number\n",
    "            row_data = []\n",
    "            week_th = tr.find('th', {'data-stat': 'week_num'})\n",
    "            week_num = week_th.text.strip() if week_th else ''\n",
    "            row_data.append(week_num)\n",
    "\n",
    "            # Add the rest of the data from 'td' elements\n",
    "            for td in tr.find_all('td'):\n",
    "                row_data.append(td.text.strip())\n",
    "\n",
    "            # Ensure row_data matches the number of headers\n",
    "            if len(row_data) != len(schedule_headers):\n",
    "                row_data += [''] * (len(schedule_headers) - len(row_data))\n",
    "\n",
    "            team_games.append(row_data)\n",
    "            all_games.append(row_data)  # Also add to the all_games list\n",
    "\n",
    "        # Save the team's data to its own CSV file\n",
    "        team_file_path = f'{data_dir}/{abbreviation}_{year}_schedule_and_game_results.csv'\n",
    "        with open(team_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(schedule_headers)\n",
    "            writer.writerows(team_games)\n",
    "\n",
    "        print(f'Saved schedule data for {name} for the year {year}')\n",
    "        sleep(2.5)  # Sleep for 2.5 seconds after processing each team\n",
    "\n",
    "# Merge all years and teams into all_teams_schedule_and_game_results_merged.csv\n",
    "data_dir = './data/SR-schedule-and-game-results'\n",
    "\n",
    "# List to hold all data from the files\n",
    "all_games = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\"_schedule_and_game_results.csv\"):\n",
    "        # Extract the team abbreviation and year from the filename\n",
    "        team_abbr = filename.split('_')[0]\n",
    "        season_year = filename.split('_')[1]\n",
    "        \n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add new columns for the team abbreviation and season year\n",
    "        df['Team'] = team_abbr\n",
    "        df['Season'] = season_year\n",
    "        \n",
    "        # Append the DataFrame to the list of all games\n",
    "        all_games.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(all_games, ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_output_path = os.path.join(data_dir, 'all_teams_schedule_and_game_results_merged.csv')\n",
    "merged_df.to_csv(merged_output_path, index=False)\n",
    "\n",
    "print(f\"Successfully merged all team files into {merged_output_path}\")\n",
    "\n",
    "# # Standardize the Team column abbreviations\n",
    "# team_abbreviation_mapping = {\n",
    "#     'gnb': 'gb',\n",
    "#     'htx': 'hou',\n",
    "#     'clt': 'ind',\n",
    "#     'kan': 'kc',\n",
    "#     'sdg': 'lac',\n",
    "#     'ram': 'lar',\n",
    "#     'rai': 'lvr',\n",
    "#     'nwe': 'ne',\n",
    "#     'nor': 'no',\n",
    "#     'sfo': 'sf',\n",
    "#     'tam': 'tb',\n",
    "#     'oti': 'ten',\n",
    "#     'rav': 'bal',\n",
    "#     'crd': 'ari'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c2fa6-5aa9-4b8f-8860-d4d2a8149dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Team Conversions \n",
    "# Not in nfl.db currently\n",
    "\n",
    "# Create directories if they don't exist\n",
    "data_dir = './data/SR-team-conversions'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# List of teams and abbreviations\n",
    "teams = [\n",
    "    ['crd', 'Arizona Cardinals'],\n",
    "    ['atl', 'Atlanta Falcons'],\n",
    "    ['rav', 'Baltimore Ravens'],\n",
    "    ['buf', 'Buffalo Bills'],\n",
    "    ['car', 'Carolina Panthers'],\n",
    "    ['chi', 'Chicago Bears'],\n",
    "    ['cin', 'Cincinnati Bengals'],\n",
    "    ['cle', 'Cleveland Browns'],\n",
    "    ['dal', 'Dallas Cowboys'],\n",
    "    ['den', 'Denver Broncos'],\n",
    "    ['det', 'Detroit Lions'],\n",
    "    ['gnb', 'Green Bay Packers'],\n",
    "    ['htx', 'Houston Texans'],\n",
    "    ['clt', 'Indianapolis Colts'],\n",
    "    ['jax', 'Jacksonville Jaguars'],\n",
    "    ['kan', 'Kansas City Chiefs'],\n",
    "    ['sdg', 'Los Angeles Chargers'],\n",
    "    ['ram', 'Los Angeles Rams'],\n",
    "    ['rai', 'Las Vegas Raiders'],\n",
    "    ['mia', 'Miami Dolphins'],\n",
    "    ['min', 'Minnesota Vikings'],\n",
    "    ['nwe', 'New England Patriots'],\n",
    "    ['nor', 'New Orleans Saints'],\n",
    "    ['nyg', 'New York Giants'],\n",
    "    ['nyj', 'New York Jets'],\n",
    "    ['phi', 'Philadelphia Eagles'],\n",
    "    ['pit', 'Pittsburgh Steelers'],\n",
    "    ['sea', 'Seattle Seahawks'],\n",
    "    ['sfo', 'San Francisco 49ers'],\n",
    "    ['tam', 'Tampa Bay Buccaneers'],\n",
    "    ['oti', 'Tennessee Titans'],\n",
    "    ['was', 'Washington Commanders']\n",
    "]\n",
    "\n",
    "# Define headers for the team conversions CSV\n",
    "team_conversions_headers = [\n",
    "    'Player', '3DAtt', '3DConv', '3D%', '4DAtt', '4DConv', '4D%', 'RZAtt', 'RZTD', 'RZPct', 'Team'\n",
    "]\n",
    "\n",
    "# Loop through the years\n",
    "# for year in range(2015, 2025):\n",
    "for year in range(2023, 2025):\n",
    "    for team in teams:\n",
    "        abbreviation, name = team\n",
    "        print(f'Processing {name} for the year {year}')  # Include the year in the print statement\n",
    "        url = f'https://www.pro-football-reference.com/teams/{abbreviation}/{year}.htm'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f'Failed to retrieve page {url} for {name} in {year}: {response.status_code}')\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the \"Team Conversions\" table (e.g., \"team_conversions\" table)\n",
    "        table = soup.find('table', {'id': 'team_conversions'})\n",
    "\n",
    "        if table is None:\n",
    "            print(f'Team Conversions table not found on page {url} for {name} in {year}')\n",
    "            continue\n",
    "\n",
    "        all_conversions = []\n",
    "        tbody = table.find('tbody')\n",
    "        for tr in tbody.find_all('tr'):\n",
    "            row_data = [td.text.strip() for td in tr.find_all(['th', 'td'])]  # Extract row data including headers\n",
    "            row_data.append(abbreviation)  # Append team abbreviation at the end\n",
    "            all_conversions.append(row_data)\n",
    "\n",
    "        # Save the conversion data for this team to a separate CSV file\n",
    "        team_file = f'{data_dir}/{abbreviation}_{year}_team_conversions.csv'\n",
    "        with open(team_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(team_conversions_headers)\n",
    "            writer.writerows(all_conversions)\n",
    "\n",
    "        print(f'Saved team conversions data for {name} for the year {year} to {team_file}')\n",
    "        \n",
    "        sleep(2.5)  # Sleep for 2.5 seconds after processing each team\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d556c81-7ba9-4021-867d-e00d12056b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creating home_spread and away_spread columns in nfl.db --- #\n",
    "# Adjusting to also make team_favorite\n",
    "\n",
    "# Function to calculate home_spread, away_spread, team_favorite, and team_covered\n",
    "def calculate_spreads_and_favorite(spread_line, home_team, away_team, home_score, away_score):\n",
    "    if spread_line is None or home_score is None or away_score is None:\n",
    "        # Handle cases where spread_line or scores are missing\n",
    "        return \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "    # Ensure spread_line is a float for arithmetic operations\n",
    "    spread_line = float(spread_line)\n",
    "    abs_spread = abs(spread_line)  # Use absolute value of the spread for comparisons\n",
    "\n",
    "    if spread_line > 0:\n",
    "        # Home team is favored\n",
    "        home_spread = f\"-{spread_line}\"  # Home team is the favorite\n",
    "        away_spread = f\"+{spread_line}\"  # Away team is the underdog\n",
    "        team_favorite = home_team\n",
    "        # Determine which team covered the spread\n",
    "        if home_score > away_score + abs_spread:\n",
    "            team_covered = home_team\n",
    "        elif away_score > home_score - abs_spread:\n",
    "            team_covered = away_team\n",
    "        else:\n",
    "            team_covered = \"Push\"\n",
    "    else:\n",
    "        # Away team is favored\n",
    "        home_spread = f\"+{-spread_line}\"  # Home team is the underdog\n",
    "        away_spread = f\"-{-spread_line}\"  # Away team is the favorite\n",
    "        team_favorite = away_team\n",
    "        # Determine which team covered the spread\n",
    "        if away_score > home_score + abs_spread:\n",
    "            team_covered = away_team\n",
    "        elif home_score > away_score - abs_spread:\n",
    "            team_covered = home_team\n",
    "        else:\n",
    "            team_covered = \"Push\"\n",
    "\n",
    "    return home_spread, away_spread, team_favorite, team_covered\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = 'nfl.db'  # Update the path if needed\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Add new columns to the 'Games' table (if they don't already exist)\n",
    "try:\n",
    "    cursor.execute(\"ALTER TABLE Games ADD COLUMN home_spread TEXT;\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # Column already exists\n",
    "\n",
    "try:\n",
    "    cursor.execute(\"ALTER TABLE Games ADD COLUMN away_spread TEXT;\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # Column already exists\n",
    "\n",
    "try:\n",
    "    cursor.execute(\"ALTER TABLE Games ADD COLUMN team_favorite TEXT;\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # Column already exists\n",
    "\n",
    "try:\n",
    "    cursor.execute(\"ALTER TABLE Games ADD COLUMN team_covered TEXT;\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass  # Column already exists\n",
    "\n",
    "# Update each row in the 'Games' table with home_spread, away_spread, team_favorite, and team_covered\n",
    "cursor.execute(\"SELECT game_id, spread_line, home_team, away_team, home_score, away_score FROM Games;\")\n",
    "games = cursor.fetchall()\n",
    "\n",
    "for game in games:\n",
    "    game_id, spread_line, home_team, away_team, home_score, away_score = game\n",
    "    home_spread, away_spread, team_favorite, team_covered = calculate_spreads_and_favorite(spread_line, home_team, away_team, home_score, away_score)\n",
    "    update_query = \"UPDATE Games SET home_spread = ?, away_spread = ?, team_favorite = ?, team_covered = ? WHERE game_id = ?;\"\n",
    "    cursor.execute(update_query, (home_spread, away_spread, team_favorite, team_covered, game_id))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Columns 'home_spread', 'away_spread', 'team_favorite', and 'team_covered' have been added and updated for all rows in the 'Games' table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942458d6-9211-46d7-a7cc-292427885d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Passing/Rushing/Receiving (from boxscore pages)\n",
    "# For longest reception\n",
    "# Not in nfl.db currently\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import csv\n",
    "# import os\n",
    "# import time\n",
    "\n",
    "# # !mkdir data/passing-rushing-receiving-game-logs/\n",
    "\n",
    "# # Passing, Rushing, and Receiving Game Logs\n",
    "# # for year_to_scrape in range(2015, 2025):\n",
    "# for year_to_scrape in range(2024, 2025):\n",
    "#     # Initialize output CSV file with the year in its name\n",
    "#     output_filename = f'./data/passing-rushing-receiving-game-logs/all_passing_rushing_receiving_{year_to_scrape}.csv'\n",
    "#     with open(output_filename, 'w', newline='') as output_csvfile:\n",
    "#         csvwriter = csv.writer(output_csvfile)\n",
    "#         csvwriter.writerow([\n",
    "#             'player', 'team', 'pass_cmp', 'pass_att', 'pass_yds', 'pass_td', 'pass_int', 'pass_sacked', \n",
    "#             'pass_sacked_yds', 'pass_long', 'pass_rating', 'rush_att', 'rush_yds', 'rush_td', 'rush_long', \n",
    "#             'targets', 'rec', 'rec_yds', 'rec_td', 'rec_long', 'fumbles', 'fumbles_lost', 'game_id'\n",
    "#         ])  # Added 'game_id' to the header row\n",
    "\n",
    "#         # Read the CSV file containing the game data\n",
    "#         with open('./data/games.csv', 'r') as csvfile:\n",
    "#             reader = csv.DictReader(csvfile)\n",
    "#             rows = [row for row in reader if int(row['game_id'].split('_')[0]) == year_to_scrape]  # Filter rows for the year\n",
    "\n",
    "#             for row in rows:\n",
    "#                 pfr_value = row['pfr']\n",
    "#                 game_id = row['game_id']\n",
    "\n",
    "#                 # Form the URL using the 'pfr' value\n",
    "#                 url = f\"https://www.pro-football-reference.com/boxscores/{pfr_value}.htm\"\n",
    "\n",
    "#                 try:\n",
    "#                     # Fetch the webpage\n",
    "#                     response = requests.get(url)\n",
    "\n",
    "#                     # Check if we are being rate limited (status code 429)\n",
    "#                     if response.status_code == 429:\n",
    "#                         print(f\"Rate limit exceeded for URL {url}. Please try again later.\")\n",
    "#                         time.sleep(60)  # Sleep for 60 seconds before retrying\n",
    "#                         continue\n",
    "\n",
    "#                     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#                     # Find the table containing player stats\n",
    "#                     table = soup.find('div', id='div_player_offense')\n",
    "#                     if table:\n",
    "#                         # Loop through the rows to get the player stats\n",
    "#                         for i, tr in enumerate(table.find_all('tr')):\n",
    "#                             if i == 0:  # Skip the first header row\n",
    "#                                 continue\n",
    "#                             player_name = tr.find('th').get_text() if tr.find('th') else ''\n",
    "#                             stats = [td.get_text() for td in tr.find_all('td')]\n",
    "#                             row_data = [player_name] + stats + [game_id]  # Append game_id to the end of the row\n",
    "#                             csvwriter.writerow(row_data)\n",
    "\n",
    "#                     print(f\"Successfully scraped data for game ID: {game_id}, PFR: {pfr_value}\")\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"An error occurred while scraping {url}. Error: {e}\")\n",
    "\n",
    "#                 # Sleep for 2 seconds to avoid overloading the server\n",
    "#                 time.sleep(2)\n",
    "\n",
    "#     print(f\"Scraping completed for {year_to_scrape}. Data saved to {output_filename}.\")\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Directory for saving game logs (ensure it exists)\n",
    "os.makedirs('./data/passing-rushing-receiving-game-logs/', exist_ok=True)\n",
    "\n",
    "# For years in range (2024 in this case)\n",
    "for year_to_scrape in range(2023, 2025):\n",
    "    output_filename = f'./data/passing-rushing-receiving-game-logs/all_passing_rushing_receiving_{year_to_scrape}.csv'\n",
    "    \n",
    "    # Initialize CSV file with a header, including 'player_id'\n",
    "    with open(output_filename, 'w', newline='') as output_csvfile:\n",
    "        csvwriter = csv.writer(output_csvfile)\n",
    "        csvwriter.writerow([\n",
    "            'player', 'player_id', 'team', 'pass_cmp', 'pass_att', 'pass_yds', 'pass_td', 'pass_int', \n",
    "            'pass_sacked', 'pass_sacked_yds', 'pass_long', 'pass_rating', 'rush_att', 'rush_yds', 'rush_td', \n",
    "            'rush_long', 'targets', 'rec', 'rec_yds', 'rec_td', 'rec_long', 'fumbles', 'fumbles_lost', 'game_id'\n",
    "        ])  # Added 'player_id' to the header row\n",
    "\n",
    "        # Open the game data file\n",
    "        with open('./data/games.csv', 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            rows = [row for row in reader if int(row['game_id'].split('_')[0]) == year_to_scrape]\n",
    "\n",
    "            for row in rows:\n",
    "                pfr_value = row['pfr']\n",
    "                game_id = row['game_id']\n",
    "\n",
    "                url = f\"https://www.pro-football-reference.com/boxscores/{pfr_value}.htm\"\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url)\n",
    "\n",
    "                    # Rate limit handling\n",
    "                    if response.status_code == 429:\n",
    "                        print(f\"Rate limit exceeded for URL {url}. Please try again later.\")\n",
    "                        time.sleep(60)\n",
    "                        continue\n",
    "\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    # Find the player offense table\n",
    "                    table = soup.find('div', id='div_player_offense')\n",
    "                    if table:\n",
    "                        for i, tr in enumerate(table.find_all('tr')):\n",
    "                            if i == 0:  # Skip header row\n",
    "                                continue\n",
    "                            player_cell = tr.find('th')\n",
    "                            if player_cell:\n",
    "                                player_name = player_cell.get_text()\n",
    "\n",
    "                                # Extract href from <a> if it exists\n",
    "                                player_link = player_cell.find('a')\n",
    "                                player_id = player_link['href'].split('/')[-1] if player_link else None  # Extract only the 'player_id' part\n",
    "\n",
    "                                # Collect stats\n",
    "                                stats = [td.get_text() for td in tr.find_all('td')]\n",
    "                                row_data = [player_name, player_id] + stats + [game_id]  # Append game_id and player_id\n",
    "                                csvwriter.writerow(row_data)\n",
    "\n",
    "                    print(f\"Successfully scraped data for game ID: {game_id}, PFR: {pfr_value}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while scraping {url}. Error: {e}\")\n",
    "\n",
    "                # Sleep between requests\n",
    "                time.sleep(2)\n",
    "\n",
    "    print(f\"Scraping completed for {year_to_scrape}. Data saved to {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d68020-638d-4440-ba1b-a472810ac8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean weird rows ^\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory path\n",
    "directory = 'data/passing-rushing-receiving-game-logs/'\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Remove rows where the 'player' column is \"Player\" or NaN (missing)\n",
    "        df_cleaned = df[(df['player'] != 'Player') & (df['player'].notna())]\n",
    "\n",
    "        # Save the cleaned DataFrame back to the same CSV file\n",
    "        df_cleaned.to_csv(file_path, index=False)\n",
    "\n",
    "        print(f\"Processed {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aed5c2-7673-496b-b1f9-4cdd61d4cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all ^\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory path\n",
    "directory = 'data/passing-rushing-receiving-game-logs/'\n",
    "merged_file_path = 'data/all_passing_rushing_receiving.csv'  # Path where the merged file will be saved\n",
    "\n",
    "# List to hold all DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "        \n",
    "        print(f\"Added {filename} to the merge list\")\n",
    "\n",
    "# Concatenate all DataFrames in the list into one large DataFrame\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(merged_file_path, index=False)\n",
    "\n",
    "print(f\"All files have been merged into {merged_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7f770-21ef-41de-98ae-17b0ad649b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add opponent_team column ^\n",
    "\n",
    "file_path = 'data/all_passing_rushing_receiving.csv'  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Dictionary to map incorrect team codes to the correct ones\n",
    "team_corrections = {\n",
    "    'NWE': 'NE',\n",
    "    'GNB': 'GB',\n",
    "    'KAN': 'KC',\n",
    "    'STL': 'LAR',\n",
    "    'NOR': 'NO',\n",
    "    'SDG': 'LAC',\n",
    "    'OAK': 'LVR',\n",
    "    'TAM': 'TB',\n",
    "    'SFO': 'SF'\n",
    "}\n",
    "\n",
    "# Apply the corrections to the 'team' column\n",
    "df['team'] = df['team'].replace(team_corrections)\n",
    "\n",
    "# Function to extract the opponent team from the game_id column\n",
    "def get_opponent_team(row):\n",
    "    game_id = row['game_id']\n",
    "    team = row['team']\n",
    "    \n",
    "    # Split the game_id to extract the away and home teams\n",
    "    _, _, away_team, home_team = game_id.split('_')\n",
    "    \n",
    "    # Determine the opponent based on whether the player's team is the home or away team\n",
    "    if team == home_team:\n",
    "        return away_team\n",
    "    elif team == away_team:\n",
    "        return home_team\n",
    "    else:\n",
    "        return None  # In case the team does not match either home or away (shouldn't happen)\n",
    "\n",
    "# Function to determine if the player was at home or away\n",
    "def is_player_home(row):\n",
    "    game_id = row['game_id']\n",
    "    team = row['team']\n",
    "    \n",
    "    # Split the game_id to extract the away and home teams\n",
    "    _, _, away_team, home_team = game_id.split('_')\n",
    "    \n",
    "    # Check if the player's team is the home team\n",
    "    return 'y' if team == home_team else 'n'\n",
    "\n",
    "# Apply the functions to create new columns 'opponent_team' and 'home'\n",
    "df['opponent_team'] = df.apply(get_opponent_team, axis=1)\n",
    "df['home'] = df.apply(is_player_home, axis=1)\n",
    "\n",
    "# Save the updated dataframe to the same CSV file\n",
    "df.to_csv('data/all_passing_rushing_receiving.csv', index=False)  # Save the result\n",
    "\n",
    "# Optionally display the first few rows to verify the changes\n",
    "print(df.head())\n",
    "# !open data/all_passing_rushing_receiving.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae41f5-f787-4eec-9412-693749eacbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add position column ^\n",
    "\n",
    "# Load the CSV files\n",
    "all_passing_file = 'data/all_passing_rushing_receiving.csv'\n",
    "# rosters_file = 'data/Rosters.csv'\n",
    "rosters_file = 'data/rosters.csv'\n",
    "\n",
    "all_passing_df = pd.read_csv(all_passing_file)\n",
    "rosters_df = pd.read_csv(rosters_file)\n",
    "\n",
    "# Merge the two dataframes on player names\n",
    "merged_df = pd.merge(all_passing_df, rosters_df[['full_name', 'position']], \n",
    "                     left_on='player', right_on='full_name', how='left')\n",
    "\n",
    "# Ensure the 'position' column exists even if no matches are found\n",
    "if 'position' not in merged_df.columns:\n",
    "    merged_df['position'] = None\n",
    "\n",
    "# Filter only relevant positions (QB, WR, TE, RB)\n",
    "relevant_positions = ['QB', 'WR', 'TE', 'RB']\n",
    "merged_df['position'] = merged_df['position'].where(merged_df['position'].isin(relevant_positions), None)\n",
    "\n",
    "# Drop the full_name column that was added during the merge\n",
    "merged_df.drop(columns=['full_name'], inplace=True)\n",
    "\n",
    "# Ensure all rows for a player have the same position\n",
    "merged_df['position'] = merged_df.groupby('player')['position'].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "merged_df.to_csv('data/all_passing_rushing_receiving.csv', index=False)\n",
    "\n",
    "# Optionally display the updated dataframe\n",
    "print(merged_df[['player', 'position']].head())\n",
    "\n",
    "# Optional command to open the CSV file (depending on your environment)\n",
    "# !open data/all_passing_rushing_receiving.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2067c-b2bf-4222-9d9e-fd986e4b72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MERGE PLAYER ID'S\n",
    "\n",
    "# # Load the CSV files\n",
    "# roster_df = pd.read_csv('data/rosters.csv')\n",
    "# new_stats_df = pd.read_csv('all_passing_rushing_receiving.csv')\n",
    "\n",
    "# # Merging the 'gsis_id' from the roster dataframe into the 'new_stats_df' (all_passing_rushing_receiving_2024)\n",
    "# # where 'player_id' matches 'pfr_id'\n",
    "# merged_new_stats = new_stats_df.merge(roster_df[['gsis_id', 'pfr_id']], \n",
    "#                                       left_on='player_id', \n",
    "#                                       right_on='pfr_id', \n",
    "#                                       how='left')\n",
    "\n",
    "# # Optional: Save the merged dataframe to a new CSV file if needed\n",
    "# merged_new_stats.to_csv('data/all_passing_rushing_receiving.csv', index=False)\n",
    "\n",
    "# # Display the first few rows of the merged dataframe\n",
    "# print(merged_new_stats.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190f24a-13f6-43a5-af0e-538c51680f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # # # Defense (from boxscore pages)\n",
    "# # # # For sacks/defensive INT\n",
    "# # # # Not in nfl.db currently\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create the directory for defense game logs\n",
    "os.makedirs('data/defense-game-logs', exist_ok=True)\n",
    "\n",
    "# Headers for the defense stats, including 'game_id'\n",
    "headers = [\n",
    "    'player', 'team', 'def_int', 'def_int_yds', 'def_int_td', 'def_int_long', 'pass_defended', 'sacks',\n",
    "    'tackles_combined', 'tackles_solo', 'tackles_assists', 'tackles_loss', 'qb_hits', 'fumbles_rec',\n",
    "    'fumbles_rec_yds', 'fumbles_rec_td', 'fumbles_forced', 'game_id'\n",
    "]\n",
    "\n",
    "for year_to_scrape in range(2023, 2025):\n",
    "    output_filename = f'./data/defense-game-logs/all_defense_{year_to_scrape}.csv'\n",
    "    with open(output_filename, 'w', newline='') as output_csvfile:\n",
    "        csvwriter = csv.writer(output_csvfile)\n",
    "        csvwriter.writerow(headers)\n",
    "\n",
    "        with open('./data/games.csv', 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            rows = [row for row in reader if int(row['game_id'].split('_')[0]) == year_to_scrape]\n",
    "\n",
    "            for row in rows:\n",
    "                # Check if 'away_score' and 'home_score' are both present\n",
    "                if not row['away_score'] or not row['home_score']:\n",
    "                    print(f\"Skipping game {row['game_id']} due to missing scores.\")\n",
    "                    continue  # Skip this game if scores are missing\n",
    "                \n",
    "                pfr_value = row['pfr']\n",
    "                game_id = row['game_id']\n",
    "                url = f\"https://www.pro-football-reference.com/boxscores/{pfr_value}.htm\"\n",
    "\n",
    "                try:\n",
    "                    response = requests.get(url)\n",
    "\n",
    "                    if response.status_code == 429:\n",
    "                        print(f\"Rate limit exceeded for URL {url}. Please try again later.\")\n",
    "                        time.sleep(3)\n",
    "                        continue\n",
    "\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "\n",
    "                    for comment in comments:\n",
    "                        soup_comment = BeautifulSoup(comment, 'html.parser')\n",
    "                        table = soup_comment.find('table', id='player_defense')\n",
    "                        if table:\n",
    "                            for i, tr in enumerate(table.find_all('tr')):\n",
    "                                if i == 0:\n",
    "                                    continue\n",
    "                                player_name = tr.find('th').get_text() if tr.find('th') else ''\n",
    "                                stats = [td.get_text() for td in tr.find_all('td')]\n",
    "                                row_data = [player_name] + stats + [game_id]\n",
    "                                csvwriter.writerow(row_data)\n",
    "                            print(f\"Successfully scraped data for game ID: {game_id}, PFR: {pfr_value}\")\n",
    "                            break\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while scraping {url}. Error: {e}\")\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "    print(f\"Scraping completed for {year_to_scrape}. Data saved to {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0ee52-31e0-4e1b-be1c-42480a4c73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean bad rows ^\n",
    "\n",
    "df = pd.read_csv('./data/defense-game-logs/all_defense_2024.csv')\n",
    "\n",
    "# Drop rows with any missing data\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Write the cleaned DataFrame back to the CSV file\n",
    "df.to_csv('./data/defense-game-logs/all_defense_2024.csv', index=False)\n",
    "\n",
    "# Loop version\n",
    "# directory = './data/defense-game-logs/'\n",
    "\n",
    "# # Loop through all files in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith('.csv'):  # Process only CSV files\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "        \n",
    "#         # Load the CSV file into a DataFrame\n",
    "#         df = pd.read_csv(file_path)\n",
    "        \n",
    "#         # Drop rows with any missing data\n",
    "#         df.dropna(inplace=True)\n",
    "        \n",
    "#         # Write the cleaned DataFrame back to the CSV file\n",
    "#         df.to_csv(file_path, index=False)\n",
    "#         print(f\"Processed file: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa028d-2e5c-40d4-97ff-dc606dfa3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all nfl.db to csv's\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "db_path = 'nfl.db'  # Update this path if needed\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get all table names\n",
    "tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = conn.execute(tables_query).fetchall()\n",
    "\n",
    "# Download each table to a CSV file\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    # Read the table into a pandas DataFrame\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_file_name = f\"{table_name}.csv\"\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    print(f\"Downloaded {table_name} to {csv_file_name}\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2caf80-afb0-4f75-8844-b8182ebb8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "!open PlayerStats.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e41ba5-ddc5-4863-966c-7e4acba4427e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7a59f-54b9-484e-9942-d57e4de2b4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf76b81-d5e9-4a80-82a6-74ebeb9b1cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e5af5-bda1-46c0-8e36-1633165adb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all nfl.db to other files\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Connect to the database\n",
    "db_path = 'nfl.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get the directory where the database is located\n",
    "base_dir = os.path.dirname(db_path)\n",
    "\n",
    "# Function to export a table to JSON format\n",
    "def export_table_to_json(table_name):\n",
    "    output_path = os.path.join(base_dir, f\"{table_name}.json\")\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    \n",
    "    # Fetch all data from the table\n",
    "    cursor = conn.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [description[0] for description in cursor.description]\n",
    "    \n",
    "    # Convert to list of dicts\n",
    "    data = [dict(zip(columns, row)) for row in rows]\n",
    "    \n",
    "    # Write to a JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Export all tables to JSON\n",
    "export_table_to_json('Teams')\n",
    "export_table_to_json('Games')\n",
    "export_table_to_json('PlayerStats')\n",
    "export_table_to_json('Rosters')\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Connect to the database\n",
    "db_path = 'nfl.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get the directory where the database is located\n",
    "base_dir = os.path.dirname(db_path)\n",
    "\n",
    "# Function to export a table to XLSX format\n",
    "def export_table_to_xlsx(table_name):\n",
    "    output_path = os.path.join(base_dir, f\"{table_name}.xlsx\")\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    \n",
    "    # Read data into a pandas DataFrame\n",
    "    df = pd.read_sql(query, conn)\n",
    "    \n",
    "    # Write to Excel file\n",
    "    df.to_excel(output_path, index=False)\n",
    "\n",
    "# Export all tables to XLSX\n",
    "export_table_to_xlsx('Teams')\n",
    "export_table_to_xlsx('Games')\n",
    "export_table_to_xlsx('PlayerStats')\n",
    "export_table_to_xlsx('Rosters')\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Connect to the database\n",
    "db_path = 'nfl.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get the directory where the database is located\n",
    "base_dir = os.path.dirname(db_path)\n",
    "\n",
    "# Function to export a table to XML format\n",
    "def export_table_to_xml(table_name):\n",
    "    output_path = os.path.join(base_dir, f\"{table_name}.xml\")\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    \n",
    "    # Fetch all data from the table\n",
    "    cursor = conn.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [description[0] for description in cursor.description]\n",
    "    \n",
    "    # Create the root element\n",
    "    root = ET.Element(table_name)\n",
    "    \n",
    "    # Iterate over rows and create XML tree structure\n",
    "    for row in rows:\n",
    "        entry = ET.SubElement(root, \"entry\")\n",
    "        for col_name, value in zip(columns, row):\n",
    "            col_element = ET.SubElement(entry, col_name)\n",
    "            col_element.text = str(value)\n",
    "    \n",
    "    # Write to an XML file\n",
    "    tree = ET.ElementTree(root)\n",
    "    tree.write(output_path, encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "# Export all tables to XML\n",
    "export_table_to_xml('Teams')\n",
    "export_table_to_xml('Games')\n",
    "export_table_to_xml('PlayerStats')\n",
    "export_table_to_xml('Rosters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e192f5-da5c-49fc-b9eb-3de866391d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON pieces\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Connect to the database\n",
    "db_path = 'nfl.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get the directory where the database is located\n",
    "base_dir = os.path.dirname(db_path)\n",
    "\n",
    "# Function to split data into chunks of approx 7MB\n",
    "def split_data_into_chunks(data, max_size_mb=7):\n",
    "    # Convert max_size to bytes (7MB = 7 * 1024 * 1024 bytes)\n",
    "    max_size_bytes = max_size_mb * 1024 * 1024\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for record in data:\n",
    "        record_size = len(json.dumps(record).encode('utf-8'))  # Estimate size of the record\n",
    "        if current_size + record_size > max_size_bytes:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        current_chunk.append(record)\n",
    "        current_size += record_size\n",
    "    \n",
    "    # Append the final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Function to export a table to JSON format in chunks\n",
    "def export_table_to_json_in_chunks(table_name, max_size_mb=4):\n",
    "    output_path_base = os.path.join(base_dir, table_name)\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    \n",
    "    # Fetch all data from the table\n",
    "    cursor = conn.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [description[0] for description in cursor.description]\n",
    "    \n",
    "    # Convert to list of dicts\n",
    "    data = [dict(zip(columns, row)) for row in rows]\n",
    "    \n",
    "    # Split data into chunks\n",
    "    data_chunks = split_data_into_chunks(data, max_size_mb=max_size_mb)\n",
    "    \n",
    "    # Write each chunk to a separate JSON file\n",
    "    for i, chunk in enumerate(data_chunks, start=1):\n",
    "        output_path = f\"{output_path_base}{i}.json\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunk, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Export tables with chunking for large datasets\n",
    "export_table_to_json_in_chunks('Teams')  # Assuming Teams and Games are smaller\n",
    "export_table_to_json_in_chunks('Games')\n",
    "export_table_to_json_in_chunks('PlayerStats')  # Split into PlayerStats1, PlayerStats2, ...\n",
    "export_table_to_json_in_chunks('Rosters')  # Split into Rosters1, Rosters2, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe895e9a-3658-4580-91a0-8649a739a7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530275a-5137-4a50-9f13-830d0f66abb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15177cef-138e-4dfe-8f9a-6380d932313f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ff6ac-2d34-4776-a4a1-f6490b1cd547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc744c13-890d-4594-afef-01d69fdb42fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b16c3-1990-4082-b0f3-f478beb23162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26988fc-3910-4784-899b-6fc697f79598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d75f97b-9caa-4b12-a936-e8c6e33aeb71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84428bb4-87f1-4d8e-b7bd-de0f146a537e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f168d-6d3e-44c1-a974-d71f4bf4ba25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348172d5-f75b-40a7-9992-2fc061ba13db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5815f68-6a5c-4e94-8792-417159de1ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5df26e-c683-4bde-ba61-5312046941de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7226ed-f10f-4468-b1ea-badb2d778553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print all tables and columns\n",
    "\n",
    "# # Step 1: Connect to the SQLite database\n",
    "# conn = sqlite3.connect('nfl.db')\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Step 2: Query all table names\n",
    "# cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "# tables = cursor.fetchall()\n",
    "\n",
    "# # Step 3: Print all table and column names\n",
    "# for table_name in tables:\n",
    "#     table_name = table_name[0]\n",
    "#     print(f\"Table: {table_name}\")\n",
    "    \n",
    "#     cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "#     columns = cursor.fetchall()\n",
    "    \n",
    "#     for column in columns:\n",
    "#         print(f\"  Column: {column[1]}\")\n",
    "    \n",
    "#     print()  # Add a newline for better readability\n",
    "\n",
    "# # Close the connection\n",
    "# conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1e2f0-7bf2-41c4-b688-ff82b5b0c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge box scores\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV files\n",
    "# df1 = pd.read_csv('./data/box_scores.csv')\n",
    "# df2 = pd.read_csv('./data/box_scores_2010_2019.csv')\n",
    "\n",
    "# # Concatenate the dataframes\n",
    "# merged_df = pd.concat([df1, df2])\n",
    "\n",
    "# # Reset index if you want a clean index\n",
    "# merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Save the merged dataframe to a new CSV file\n",
    "# merged_df.to_csv('./data/box_scores.csv', index=False)\n",
    "\n",
    "# print('Merged CSV saved as ./data/box_scores.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a455e-1bed-4e33-9ba4-34369ce1be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge scoring tables (touchdown logs)\n",
    "\n",
    "# # Directory containing the CSV files\n",
    "# directory = './data/scoring-tables/'\n",
    "\n",
    "# # Initialize an empty list to hold the dataframes\n",
    "# dataframes = []\n",
    "\n",
    "# # Iterate through each file in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith('.csv'):\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         # Load the CSV file into a DataFrame\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         # Append the DataFrame to the list\n",
    "#         dataframes.append(df)\n",
    "\n",
    "# # Concatenate all the DataFrames into one\n",
    "# merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# # Save the merged DataFrame to a CSV file\n",
    "# merged_df.to_csv('./data/touchdown_logs.csv', index=False)\n",
    "\n",
    "# print(\"Merging completed. The merged data is saved as 'touchdown_logs.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfeb46d-338c-4b78-b806-235b01497eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete table \n",
    "\n",
    "# import sqlite3\n",
    "\n",
    "# # Path to your SQLite database\n",
    "# database_path = 'nfl.db'\n",
    "\n",
    "# # Connect to the database\n",
    "# conn = sqlite3.connect(database_path)\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # SQL command to drop the Rosters table\n",
    "# drop_table_query = \"DROP TABLE IF EXISTS PlayerStats;\"\n",
    "\n",
    "# # Execute the query\n",
    "# cursor.execute(drop_table_query)\n",
    "\n",
    "# # Commit the changes\n",
    "# conn.commit()\n",
    "\n",
    "# # Close the connection\n",
    "# conn.close()\n",
    "\n",
    "# print(\"Rosters table deleted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68faee84-2a74-41f8-bd7e-71dfe19d01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix game log column names\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = 'data/SR-game-logs/'\n",
    "\n",
    "# List of new headers\n",
    "new_headers = [\n",
    "    'week_num', 'game_day_of_week', 'game_date', 'boxscore_word', 'game_outcome', 'overtime',\n",
    "    'game_location', 'opp', 'pts_off', 'pts_def', 'pass_cmp', 'pass_att', 'pass_yds', 'pass_td',\n",
    "    'pass_int', 'pass_sacked', 'pass_sacked_yds', 'pass_yds_per_att', 'pass_net_yds_per_att',\n",
    "    'pass_cmp_perc', 'pass_rating', 'rush_att', 'rush_yds', 'rush_yds_per_att', 'rush_td', 'fgm',\n",
    "    'fga', 'xpm', 'xpa', 'punt', 'punt_yds', 'third_down_success', 'third_down_att', \n",
    "    'fourth_down_success', 'fourth_down_att', 'time_of_poss', 'team_name'\n",
    "]\n",
    "\n",
    "# Loop over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Full path to the CSV file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Change the column headers to the new headers\n",
    "        df.columns = new_headers\n",
    "        \n",
    "        # Save the CSV file back\n",
    "        df.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"Updated headers for {filename}\")\n",
    "\n",
    "print(\"All CSV files have been updated with new headers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65925c-9a65-4abb-b3bf-563c63965e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix opponent game log column names\n",
    "\n",
    "# Fix game log column names\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = 'data/SR-opponent-game-logs/'\n",
    "\n",
    "# List of new headers\n",
    "new_headers = [\n",
    "    'week_num', 'game_day_of_week', 'game_date', 'boxscore_word', 'game_outcome', 'overtime',\n",
    "    'game_location', 'opp', 'pts_off', 'pts_def', 'pass_cmp', 'pass_att', 'pass_yds', 'pass_td',\n",
    "    'pass_int', 'pass_sacked', 'pass_sacked_yds', 'pass_yds_per_att', 'pass_net_yds_per_att',\n",
    "    'pass_cmp_perc', 'pass_rating', 'rush_att', 'rush_yds', 'rush_yds_per_att', 'rush_td', 'fgm',\n",
    "    'fga', 'xpm', 'xpa', 'punt', 'punt_yds', 'third_down_success', 'third_down_att', \n",
    "    'fourth_down_success', 'fourth_down_att', 'time_of_poss', 'team_name'\n",
    "]\n",
    "\n",
    "# Loop over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Full path to the CSV file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Change the column headers to the new headers\n",
    "        df.columns = new_headers\n",
    "        \n",
    "        # Save the CSV file back\n",
    "        df.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"Updated headers for {filename}\")\n",
    "\n",
    "print(\"All CSV files have been updated with new headers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5993b9-006f-4a71-94b8-c49f4d4ca8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
